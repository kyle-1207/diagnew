# PNæ··åˆåé¦ˆTransformeræ¨¡å‹æµ‹è¯•è„šæœ¬
# åŸºäº Train_Transformer_PN_HybridFeedback_EN.py è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
cl# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from Comprehensive_calculation import Comprehensive_calculation

# æ–°å¢å¯¼å…¥
from tqdm import tqdm
import json
import time
from datetime import datetime
from sklearn.metrics import roc_curve, auc, confusion_matrix
import glob

# æ·»åŠ æ¨¡å‹åŠ è½½è¾…åŠ©å‡½æ•°
def remove_module_prefix(state_dict):
    """ç§»é™¤state_dictä¸­çš„module.å‰ç¼€"""
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('module.'):
            new_key = key[7:]  # ç§»é™¤'module.'å‰ç¼€
        else:
            new_key = key
        new_state_dict[new_key] = value
    return new_state_dict

def safe_get_nested(dictionary, keys, default=None):
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸çš„å€¼"""
    try:
        for key in keys:
            dictionary = dictionary[key]
        return dictionary
    except (KeyError, TypeError, IndexError):
        return default

def safe_load_model(model, model_path, model_name):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†DataParallelå‰ç¼€é—®é¢˜"""
    try:
        print(f"   æ­£åœ¨åŠ è½½{model_name}æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"   âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        state_dict = torch.load(model_path, map_location=device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤moduleå‰ç¼€
        has_module_prefix = any(key.startswith('module.') for key in state_dict.keys())
        if has_module_prefix:
            print(f"   æ£€æµ‹åˆ°DataParallelå‰ç¼€ï¼Œæ­£åœ¨ç§»é™¤...")
            state_dict = remove_module_prefix(state_dict)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„åŒ¹é…
        model_state_dict = model.state_dict()
        missing_keys = []
        unexpected_keys = []
        
        for key in model_state_dict.keys():
            if key not in state_dict:
                missing_keys.append(key)
        
        for key in state_dict.keys():
            if key not in model_state_dict:
                unexpected_keys.append(key)
        
        if missing_keys:
            print(f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys[:5]}..." if len(missing_keys) > 5 else f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys}")
        
        if unexpected_keys:
            print(f"   âš ï¸  å¤šä½™é”®: {unexpected_keys[:5]}..." if len(unexpected_keys) > 5 else f"   âš ï¸  å¤šä½™é”®: {unexpected_keys}")
        
        # å°è¯•åŠ è½½
        model.load_state_dict(state_dict, strict=False)
        print(f"   âœ… {model_name}æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"   âŒ {model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# GPUé…ç½®æ£€æŸ¥
print("ğŸ–¥ï¸ GPUé…ç½®æ£€æŸ¥:")
print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"   å½“å‰è®¾å¤‡: {device}")

if torch.cuda.is_available():
    print("   GPUè¯¦ç»†ä¿¡æ¯:")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"     GPU {i}: {props.name}")
        print(f"       æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
        print(f"       è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print("   âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
import matplotlib.font_manager as fm
from matplotlib import rcParams
import platform

def setup_chinese_fonts_strict():
    """LinuxæœåŠ¡å™¨ç¯å¢ƒä¸­æ–‡å­—ä½“é…ç½®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    import subprocess
    import os
    
    # 1. å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…ï¼ˆä»…Linuxï¼‰
    if platform.system() == "Linux":
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡ç†å‘˜æƒé™å®‰è£…å­—ä½“
            result = subprocess.run(['which', 'apt-get'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("ğŸ”§ æ­£åœ¨å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…...")
                subprocess.run(['sudo', 'apt-get', 'update'], capture_output=True, timeout=30)
                subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-noto-cjk', 'fonts-wqy-microhei', 'fonts-arphic-ukai'], capture_output=True, timeout=60)
        except Exception as e:
            print(f"âš ï¸ å­—ä½“å®‰è£…å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰: {e}")
    
    # 2. æ‰©å±•å€™é€‰å­—ä½“åˆ—è¡¨
    candidates = [
        # Linuxä¼˜å…ˆå­—ä½“
        'Noto Sans CJK SC Regular',
        'Noto Sans CJK SC',
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei',
        'Source Han Sans CN',
        'Source Han Sans SC',
        'AR PL UKai CN',
        'AR PL UMing CN',
        # é€šç”¨å­—ä½“
        'Droid Sans Fallback',
        'Liberation Sans',
        # Windowså…œåº•
        'Microsoft YaHei',
        'SimHei',
        'SimSun',
        # æœ€ç»ˆå…œåº•
        'DejaVu Sans',
        'Liberation Sans'
    ]

    chosen = None
    for name in candidates:
        try:
            font_path = fm.findfont(name, fallback_to_default=False)
            if font_path and 'DejaVu' not in font_path and os.path.exists(font_path):
                chosen = name
                print(f"ğŸ” æ‰¾åˆ°å­—ä½“: {name} -> {font_path}")
                break
        except Exception:
            continue

    # 3. å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚å­—ä½“ï¼Œå°è¯•ç³»ç»Ÿå­—ä½“æ‰«æ
    if chosen is None:
        print("ğŸ” è¿›è¡Œç³»ç»Ÿå­—ä½“æ‰«æ...")
        all_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = [f for f in all_fonts if any(keyword in f.lower() for keyword in ['cjk', 'han', 'hei', 'kai', 'ming', 'noto', 'wenquanyi'])]
        if chinese_fonts:
            chosen = chinese_fonts[0]
            print(f"ğŸ” é€šè¿‡æ‰«ææ‰¾åˆ°ä¸­æ–‡å­—ä½“: {chosen}")
        else:
            chosen = 'DejaVu Sans'
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨DejaVu Sans")

    # 4. å¢å¼ºçš„å…¨å±€æ¸²æŸ“å‚æ•°
    rcParams['font.family'] = 'sans-serif'
    rcParams['font.sans-serif'] = [chosen, 'DejaVu Sans', 'Liberation Sans']
    rcParams['axes.unicode_minus'] = False
    rcParams['pdf.fonttype'] = 42
    rcParams['ps.fonttype'] = 42
    rcParams['savefig.dpi'] = 300
    rcParams['figure.dpi'] = 100  # é™ä½ä»¥æé«˜å…¼å®¹æ€§
    rcParams['figure.autolayout'] = False
    rcParams['axes.titlesize'] = 12
    rcParams['axes.labelsize'] = 10
    rcParams['legend.fontsize'] = 9
    rcParams['xtick.labelsize'] = 9
    rcParams['ytick.labelsize'] = 9
    
    # 5. å¼ºåˆ¶å­—ä½“ç¼“å­˜é‡å»º
    try:
        fm._rebuild()
        # é¢å¤–æ¸…ç†ç¼“å­˜
        cache_dir = os.path.expanduser('~/.cache/matplotlib')
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir, ignore_errors=True)
    except Exception as e:
        print(f"âš ï¸ å­—ä½“ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    print(f"âœ… æœ€ç»ˆä½¿ç”¨å­—ä½“: {chosen}")
    
    # 6. æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
    try:
        plt.figure(figsize=(1, 1))
        plt.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=10)
        plt.close()
        print("âœ… ä¸­æ–‡å­—ä½“æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“æµ‹è¯•å¤±è´¥: {e}")
        # é™çº§åˆ°å®‰å…¨æ¨¡å¼
        rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("ğŸ”„ å·²åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼‰")

# è·³è¿‡ä¸­æ–‡å­—ä½“é…ç½®ï¼Œä½¿ç”¨è‹±æ–‡ç¯å¢ƒ
print("ğŸ”§ è·³è¿‡ä¸­æ–‡å­—ä½“é…ç½®ï¼Œä½¿ç”¨è‹±æ–‡æ˜¾ç¤ºæ¨¡å¼...")
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

#----------------------------------------æ•°æ®é¢„å¤„ç†å‡½æ•°------------------------------
def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
            
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,1]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 1)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 1, col] = 1
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šä¸“ç”¨æ•°æ®åˆ— - ä¿æŒåŸå€¼ä¸å¤„ç†
        # (æ ¹æ®éªŒè¯ç»“æœï¼Œè¿™ä¸€åˆ—åŒ…å«ç‰¹æ®Šæ•°æ®ï¼Œä¸è¿›è¡Œå¤„ç†)
        
        # ç´¢å¼•225ï¼šæ–°å¢çš„ç¬¬4ç»´ç‰¹å¾ - é™åˆ¶åœ¨[0,1]
        feature4_col = 225
        feature4_valid_mask = (data_np[:, feature4_col] >= 0) & (data_np[:, feature4_col] <= 1)
        feature4_invalid_count = (~feature4_valid_mask).sum()
        if feature4_invalid_count > 0:
            data_np[data_np[:, feature4_col] < 0, feature4_col] = 0
            data_np[data_np[:, feature4_col] > 1, feature4_col] = 1
    
    # 3. è¿›ä¸€æ­¥å¤„ç†æ®‹ç•™çš„NaN/Infå€¼
    # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šæ›¿æ¢ä¸ºå…¨å±€ä¸­ä½æ•°
    if np.isnan(data_np).any() or np.isinf(data_np).any():
        for col in range(data_np.shape[1]):
            col_data = data_np[:, col]
            
            # è·³è¿‡ç‰¹æ®Šåˆ—
            if data_np.shape[1] == 226 and col == 224:
                continue
            
            # å¤„ç†NaN
            if np.isnan(col_data).any():
                valid_mask = ~np.isnan(col_data)
                if valid_mask.any():
                    median_val = np.median(col_data[valid_mask])
                    data_np[~valid_mask, col] = median_val
                else:
                    data_np[:, col] = 0.0
            
            # å¤„ç†Inf
            if np.isinf(col_data).any():
                inf_mask = np.isinf(col_data)
                finite_mask = np.isfinite(col_data)
                if finite_mask.any():
                    # æ­£æ— ç©·æ›¿æ¢ä¸ºæœ€å¤§æœ‰é™å€¼ï¼Œè´Ÿæ— ç©·æ›¿æ¢ä¸ºæœ€å°æœ‰é™å€¼
                    max_finite = np.max(col_data[finite_mask])
                    min_finite = np.min(col_data[finite_mask])
                    data_np[col_data == np.inf, col] = max_finite
                    data_np[col_data == -np.inf, col] = min_finite
                else:
                    data_np[inf_mask, col] = 0.0
    
    # ç¡®ä¿æ²¡æœ‰æ®‹ç•™çš„å¼‚å¸¸å€¼
    assert not np.isnan(data_np).any(), f"Still have NaN values after processing"
    assert not np.isinf(data_np).any(), f"Still have Inf values after processing"
    
    # è½¬æ¢å›åŸå§‹æ•°æ®ç±»å‹
    if isinstance(data, torch.Tensor):
        return torch.tensor(data_np, dtype=data.dtype, device=data.device)
    else:
        return data_np

#----------------------------------------æµ‹è¯•é…ç½®------------------------------
print("="*60)
print("ğŸ”¬ ç”µæ± æ•…éšœè¯Šæ–­ç³»ç»Ÿ - Transformeræ¨¡å‹æµ‹è¯•ï¼ˆæ··åˆåé¦ˆç‰ˆæœ¬ï¼‰")
print("="*60)

TEST_MODE = "TRANSFORMER_ONLY"  # å›ºå®šä¸ºTransformerå•æ¨¡å‹æµ‹è¯•

# æµ‹è¯•æ•°æ®é›†é…ç½® (æ ¹æ®Labels.xlsåŠ¨æ€åŠ è½½)
def load_test_samples():
    """ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬"""
    try:
        import pandas as pd
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–æµ‹è¯•æ ·æœ¬
        all_samples = df['Num'].tolist()
        all_labels = df['Label'].tolist()
        
        # æŒ‡å®šæµ‹è¯•æ ·æœ¬ï¼šæ­£å¸¸æ ·æœ¬10-20 å’Œæ•…éšœæ ·æœ¬340-350
        test_normal_samples = [str(i) for i in range(10, 21)]  # æ­£å¸¸æ ·æœ¬ï¼š10-20
        test_fault_samples = [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬:")
        print(f"   æµ‹è¯•æ­£å¸¸æ ·æœ¬: {test_normal_samples}")
        print(f"   æµ‹è¯•æ•…éšœæ ·æœ¬: {test_fault_samples}")
        
        return {
            'normal': test_normal_samples,
            'fault': test_fault_samples
        }
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æµ‹è¯•æ ·æœ¬")
        return {
            'normal': [str(i) for i in range(10, 21)],  # æ­£å¸¸æ ·æœ¬ï¼š10-20
            'fault': [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        }

TEST_SAMPLES = load_test_samples()
ALL_TEST_SAMPLES = TEST_SAMPLES['normal'] + TEST_SAMPLES['fault']

# æ¨¡å‹è·¯å¾„é…ç½® (ä»PNæ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
MODEL_PATHS = {
    "TRANSFORMER": {
        "transformer_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/transformer_model_pn.pth",
        "net_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/net_model_pn.pth", 
        "netx_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/netx_model_pn.pth"
    }
}

# æ£€æµ‹æ¨¡å¼é…ç½®
DETECTION_MODES = {
    "three_window": {
        "name": "Three-Window Detection Mode",
        "description": "Three-window fault detection mechanism based on FAI (Detection->Verification->Marking)",
        "function": "three_window_fault_detection"
    },
    "five_point": {
        "name": "5-Point Detection Mode (Original)", 
        "description": "For fault samples, if a point exceeds threshold and adjacent points also exceed threshold, mark that point and 2 points before/after (total 5 points)",
        "function": "five_point_fault_detection"
    },
    "five_point_improved": {
        "name": "5-Point Detection Mode (Improved)",
        "description": "Improved 5-point detection: Strict trigger conditions + Graded marking range + Effective noise reduction",
        "function": "five_point_fault_detection"
    }
}

# å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ¨¡å¼
CURRENT_DETECTION_MODE = "five_point_improved"  # ä½¿ç”¨æ”¹è¿›çš„5ç‚¹æ£€æµ‹æ¨¡å¼

# åŸºäºFAIçš„ä¸‰çª—å£æ£€æµ‹é…ç½® (ä¸BiLSTMæµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´)
WINDOW_CONFIG = {
    "detection_window": 25,      # æ£€æµ‹çª—å£ï¼š25ä¸ªé‡‡æ ·ç‚¹ (12.5åˆ†é’Ÿ)
    "verification_window": 15,   # éªŒè¯çª—å£ï¼š15ä¸ªé‡‡æ ·ç‚¹ (7.5åˆ†é’Ÿ)
    "marking_window": 10,        # æ ‡è®°çª—å£ï¼š10ä¸ªé‡‡æ ·ç‚¹ (5åˆ†é’Ÿ)
    "verification_threshold": 0.6 # éªŒè¯çª—å£å†…FAIå¼‚å¸¸æ¯”ä¾‹é˜ˆå€¼ (60%)
}

# é«˜åˆ†è¾¨ç‡å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (15, 12),
    "figsize_medium": (12, 8), 
    "bbox_inches": "tight"
}

print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   æµ‹è¯•æ ·æœ¬: {ALL_TEST_SAMPLES}")
print(f"   æ­£å¸¸æ ·æœ¬: {TEST_SAMPLES['normal']}")
print(f"   æ•…éšœæ ·æœ¬: {TEST_SAMPLES['fault']}")
print(f"   æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   ä¸‰çª—å£å‚æ•°: {WINDOW_CONFIG}")
else:
    print(f"   5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

#----------------------------------------æ¨¡å‹æ–‡ä»¶æ£€æŸ¥------------------------------
def check_model_files():
    """æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶...")
    
    missing_files = []
    paths = MODEL_PATHS["TRANSFORMER"]
    
    # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶
    for key, path in paths.items():
        if not os.path.exists(path):
            missing_files.append(f"TRANSFORMER: {path}")
            print(f"   âŒ ç¼ºå¤±: {path}")
        else:
            file_size = os.path.getsize(path) / (1024 * 1024)  # MB
            print(f"   âœ… å­˜åœ¨: {path} ({file_size:.1f}MB)")
    
    # æ£€æŸ¥PCAå‚æ•°æ–‡ä»¶ (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params_hybrid_feedback.pkl"
    if not os.path.exists(pca_params_path):
        print(f"   âš ï¸  PCAå‚æ•°pickleæ–‡ä»¶ä¸å­˜åœ¨: {pca_params_path}")
        print(f"   ğŸ” æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶...")
        
        # æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶
        pca_npy_files = [
            "/mnt/bz25t/bzhy/datasave/Transformer/models/v_I_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_mean_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_std_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/T_99_limit_hybrid_feedback.npy"
        ]
        
        npy_files_exist = 0
        for npy_file in pca_npy_files:
            if os.path.exists(npy_file):
                npy_files_exist += 1
                file_size = os.path.getsize(npy_file) / (1024 * 1024)
                print(f"   âœ… å­˜åœ¨: {npy_file} ({file_size:.1f}MB)")
            else:
                print(f"   âŒ ç¼ºå¤±: {npy_file}")
        
        if npy_files_exist >= 3:
            print(f"   âœ… å‘ç°{npy_files_exist}ä¸ªnpyæ–‡ä»¶ï¼Œå¯ä»¥é‡å»ºPCAå‚æ•°")
        else:
            missing_files.append(f"PCA_PARAMS: ç¼ºå°‘è¶³å¤Ÿçš„npyæ–‡ä»¶")
    else:
        file_size = os.path.getsize(pca_params_path) / (1024 * 1024)  # MB
        print(f"   âœ… å­˜åœ¨: {pca_params_path} ({file_size:.1f}MB)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå¤± {len(missing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
        for file in missing_files:
            print(f"   {file}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿å·²è¿è¡ŒTransformerè®­ç»ƒè„šæœ¬")
        print("   2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥æ–‡ä»¶æƒé™")
        raise FileNotFoundError("è¯·å…ˆè¿è¡ŒTransformerè®­ç»ƒè„šæœ¬ç”Ÿæˆæ‰€éœ€æ¨¡å‹æ–‡ä»¶")
    
    print("âœ… Transformeræ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

# æ‰§è¡Œæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
check_model_files()

#----------------------------------------ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶------------------------------
def three_window_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    åŸºäºFAIçš„ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶
    
    åŸç†ï¼š
    1. æ£€æµ‹çª—å£ï¼šåŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    2. éªŒè¯çª—å£ï¼šç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§ï¼Œæ’é™¤éšæœºæ³¢åŠ¨
    3. æ ‡è®°çª—å£ï¼šè€ƒè™‘æ•…éšœçš„å‰åå½±å“èŒƒå›´
    
    Args:
        fai_values: FAIåºåˆ—ï¼ˆç»¼åˆæ•…éšœæŒ‡æ ‡ï¼‰
        threshold1: FAIé˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè®°å½•ï¼‰
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # è·å–çª—å£é…ç½®
    if config is None:
        config = WINDOW_CONFIG
        
    detection_window = config["detection_window"]
    verification_window = config["verification_window"]
    marking_window = config["marking_window"]
    verification_threshold = config["verification_threshold"]
    
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'candidate_points': [],    # å€™é€‰æ•…éšœç‚¹
        'verified_points': [],     # å·²éªŒè¯çš„æ•…éšœç‚¹
        'marked_regions': [],      # æ ‡è®°çš„æ•…éšœåŒºåŸŸ
        'window_stats': {},        # çª—å£ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        }
    }
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - åŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    candidate_points = []
    for i in range(len(fai_values)):
        if fai_values[i] > threshold1:
            candidate_points.append(i)
    
    detection_info['candidate_points'] = candidate_points
    
    if len(candidate_points) == 0:
        # æ²¡æœ‰å€™é€‰ç‚¹ï¼Œç›´æ¥è¿”å›
        return fault_labels, detection_info
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - ç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§
    verified_points = []
    for candidate in candidate_points:
        # å®šä¹‰éªŒè¯çª—å£èŒƒå›´ï¼ˆå‰åå„åŠä¸ªçª—å£ï¼‰
        start_verify = max(0, candidate - verification_window//2)
        end_verify = min(len(fai_values), candidate + verification_window//2)
        verify_data = fai_values[start_verify:end_verify]
        
        # è®¡ç®—FAIå¼‚å¸¸æ¯”ä¾‹
        continuous_ratio = np.sum(verify_data > threshold1) / len(verify_data)
        
        # è®¡ç®—FAIåœ¨éªŒè¯çª—å£çš„ç»Ÿè®¡ç‰¹æ€§
        window_stats = {
            'mean': np.mean(verify_data),
            'std': np.std(verify_data),
            'max': np.max(verify_data),
            'min': np.min(verify_data),
            'duration': end_verify - start_verify
        }
        
        # åŸºäºverification_thresholdéªŒè¯æŒç»­æ€§
        if continuous_ratio >= verification_threshold:
            verified_points.append({
                'point': candidate,
                'continuous_ratio': continuous_ratio,
                'verify_range': (start_verify, end_verify),
                'window_stats': window_stats
            })
    
    detection_info['verified_points'] = verified_points
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - è€ƒè™‘æ•…éšœçš„å½±å“èŒƒå›´
    marked_regions = []
    for verified in verified_points:
        candidate = verified['point']
        
        # å®šä¹‰å¯¹ç§°çš„æ ‡è®°çª—å£èŒƒå›´
        start_mark = max(0, candidate - marking_window)
        end_mark = min(len(fai_values), candidate + marking_window)
        
        # æå–æ ‡è®°åŒºåŸŸçš„FAIç‰¹å¾
        mark_data = fai_values[start_mark:end_mark]
        region_stats = {
            'mean_fai': np.mean(mark_data),
            'max_fai': np.max(mark_data),
            'std_fai': np.std(mark_data),
            'duration': end_mark - start_mark
        }
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start_mark:end_mark] = 1
        
        marked_regions.append({
            'center': candidate,
            'range': (start_mark, end_mark),
            'length': end_mark - start_mark,
            'region_stats': region_stats
        })
    
    detection_info['marked_regions'] = marked_regions
    
    # å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯
    detection_info['window_stats'] = {
        'total_candidates': len(candidate_points),
        'verified_candidates': len(verified_points),
        'total_fault_points': np.sum(fault_labels),
        'fault_ratio': np.sum(fault_labels) / len(fault_labels),
        'mean_continuous_ratio': np.mean([v['continuous_ratio'] for v in verified_points]) if verified_points else 0,
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0
    }
    
    return fault_labels, detection_info

def five_point_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    æ”¹è¿›çš„5ç‚¹æ•…éšœæ£€æµ‹æœºåˆ¶ï¼šåŸºäºæºä»£ç è®¾è®¡ï¼Œä»3001ç‚¹å¼€å§‹æ£€æµ‹
    
    è®¾è®¡åŸç†ï¼ˆç¬¦åˆæºä»£ç æ€è·¯ï¼‰ï¼š
    1. å‰3000ç‚¹ä¸ºç³»ç»Ÿå¯åŠ¨/ä¸ç¨³å®šæœŸï¼Œä¸è¿›è¡Œæ•…éšœæ£€æµ‹
    2. ä»ç¬¬3001ç‚¹å¼€å§‹åº”ç”¨ä¸‰çº§åˆ†å±‚æ£€æµ‹æœºåˆ¶
    3. ä¸é˜ˆå€¼è®¡ç®—åŸºçº¿ä¿æŒä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨3000ç‚¹åçš„æ•°æ®ï¼‰
    
    Args:
        fai_values: ç»¼åˆè¯Šæ–­æŒ‡æ ‡åºåˆ—
        threshold1: ä¸€çº§é¢„è­¦é˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè°ƒè¯•ï¼‰
        config: é…ç½®å‚æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå¯åŒ…å«threshold2, threshold3ï¼‰
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=è½»å¾®æ•…éšœ, 2=ä¸­ç­‰æ•…éšœ, 3=ä¸¥é‡æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šç¬¦åˆæºä»£ç è®¾è®¡ï¼Œå‰3000ç‚¹ä¸æ£€æµ‹
    STARTUP_PERIOD = 3000  # æºä»£ç ä¸­çš„nmå€¼ï¼Œå¯åŠ¨/ä¸ç¨³å®šæœŸ
    
    # æå‰å®šä¹‰æœ‰æ•ˆåŒºåŸŸå˜é‡ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
    effective_fai = fai_values[STARTUP_PERIOD:] if len(fai_values) > STARTUP_PERIOD else fai_values
    
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    detection_info = {
        'trigger_points': [],      # è§¦å‘5ç‚¹æ£€æµ‹çš„ç‚¹
        'marked_regions': [],      # æ ‡è®°çš„5ç‚¹åŒºåŸŸ
        'detection_stats': {},     # æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        },
        'startup_period': STARTUP_PERIOD,  # è®°å½•å¯åŠ¨æœŸé•¿åº¦
        'effective_detection_length': max(0, len(fai_values) - STARTUP_PERIOD)  # æœ‰æ•ˆæ£€æµ‹é•¿åº¦
    }
    
    # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ
    if len(fai_values) <= STARTUP_PERIOD:
        print(f"   âš ï¸ è­¦å‘Šï¼šæ•°æ®é•¿åº¦({len(fai_values)})ä¸è¶³å¯åŠ¨æœŸ({STARTUP_PERIOD})ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆæ£€æµ‹")
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'insufficient_data',
            'skip_reason': f'data_length_{len(fai_values)}_less_than_startup_{STARTUP_PERIOD}'
        }
        return fault_labels, detection_info
    
    print(f"   ğŸ“Š æ£€æµ‹é…ç½®ï¼šè·³è¿‡å‰{STARTUP_PERIOD}ç‚¹ï¼ˆå¯åŠ¨æœŸï¼‰ï¼Œä»ç¬¬{STARTUP_PERIOD+1}ç‚¹å¼€å§‹æ£€æµ‹")
    print(f"   ğŸ“Š æœ‰æ•ˆæ£€æµ‹é•¿åº¦ï¼š{len(fai_values) - STARTUP_PERIOD}ç‚¹")
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ ·æœ¬IDç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    # å°†sample_idè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
    sample_id_str = str(sample_id)
    is_fault_sample = sample_id_str in TEST_SAMPLES['fault']
    
    # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    print(f"   ğŸ“Š æ ·æœ¬åˆ†ç±»æ£€æŸ¥:")
    print(f"      åŸå§‹sample_id: {sample_id} (ç±»å‹: {type(sample_id)})")
    print(f"      å­—ç¬¦ä¸²sample_id: {sample_id_str}")
    print(f"      æ•…éšœæ ·æœ¬åˆ—è¡¨: {TEST_SAMPLES['fault']}")
    print(f"      æ­£å¸¸æ ·æœ¬åˆ—è¡¨: {TEST_SAMPLES['normal']}")
    print(f"      æ˜¯æ•…éšœæ ·æœ¬: {is_fault_sample}")
    
    # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦åœ¨æ­£å¸¸æ ·æœ¬ä¸­
    is_normal_sample = sample_id_str in TEST_SAMPLES['normal']
    print(f"      æ˜¯æ­£å¸¸æ ·æœ¬: {is_normal_sample}")
    
    if not is_fault_sample and not is_normal_sample:
        print(f"   âš ï¸ è­¦å‘Šï¼šæ ·æœ¬{sample_id}æ—¢ä¸åœ¨æ•…éšœåˆ—è¡¨ä¹Ÿä¸åœ¨æ­£å¸¸åˆ—è¡¨ä¸­ï¼Œé»˜è®¤ä¸ºæ•…éšœæ ·æœ¬è¿›è¡Œæ£€æµ‹")
        is_fault_sample = True
    
    if not is_fault_sample:
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£å¸¸æ ·æœ¬ä¸è¿›è¡Œæ•…éšœæ£€æµ‹ï¼Œå‰3000ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œåç»­ç‚¹ä¹Ÿä¸æ£€æµ‹æ•…éšœ
        print(f"   â†’ æ ·æœ¬{sample_id}ä¸ºæ­£å¸¸æ ·æœ¬ï¼Œå‰{STARTUP_PERIOD}ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œå…¶ä½™ç‚¹ä¹Ÿä¸æ£€æµ‹æ•…éšœ")
        print(f"   â†’ æ­£å¸¸æ ·æœ¬ä¸­è¶…è¿‡é˜ˆå€¼çš„ç‚¹éƒ½æ˜¯å‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰ï¼Œä¸åº”æ ‡è®°ä¸ºæ•…éšœ")
        
        # åˆ†åˆ«ç»Ÿè®¡å¯åŠ¨æœŸå’Œç¨³å®šæœŸçš„å‡é˜³æ€§
        startup_fai = fai_values[:STARTUP_PERIOD] if len(fai_values) > STARTUP_PERIOD else fai_values
        stable_fai = fai_values[STARTUP_PERIOD:] if len(fai_values) > STARTUP_PERIOD else []
        
        startup_fp = np.sum(startup_fai > threshold1) if len(startup_fai) > 0 else 0
        stable_fp = np.sum(stable_fai > threshold1) if len(stable_fai) > 0 else 0
        total_fp = startup_fp + stable_fp
        
        print(f"   â†’ å‡é˜³æ€§ç»Ÿè®¡:")
        print(f"     å¯åŠ¨æœŸ({STARTUP_PERIOD}ç‚¹): {startup_fp}ä¸ªè¶…é˜ˆå€¼ ({startup_fp/len(startup_fai)*100:.1f}%)")
        if len(stable_fai) > 0:
            print(f"     ç¨³å®šæœŸ({len(stable_fai)}ç‚¹): {stable_fp}ä¸ªè¶…é˜ˆå€¼ ({stable_fp/len(stable_fai)*100:.1f}%)")
        print(f"     æ€»è®¡: {total_fp}ä¸ªè¶…é˜ˆå€¼ ({total_fp/len(fai_values)*100:.1f}% of {len(fai_values)} points)" if fai_values is not None and len(fai_values) > 0 else f"     æ€»è®¡: {total_fp}ä¸ªè¶…é˜ˆå€¼ (æ— æ³•è®¡ç®—æ¯”ä¾‹)")
        
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'normal_sample',
            'startup_false_positives': startup_fp,
            'stable_false_positives': stable_fp,
            'total_false_positives': total_fp,
            'startup_fp_ratio': startup_fp/len(startup_fai) if len(startup_fai) > 0 else 0,
            'stable_fp_ratio': stable_fp/len(stable_fai) if len(stable_fai) > 0 else 0,
            'total_fp_ratio': total_fp/len(fai_values) if fai_values is not None and len(fai_values) > 0 else 0
        }
        # ä¸ºå…¼å®¹æ€§æ·»åŠ ç©ºå­—æ®µ
        detection_info['trigger_points'] = []
        detection_info['marked_regions'] = []
        detection_info['candidate_points'] = []
        detection_info['verified_points'] = []
        
        # ç¡®ä¿fault_labelsç¡®å®æ˜¯å…¨0ï¼ˆæ­£å¸¸æ ·æœ¬ä¸æ ‡è®°ä»»ä½•æ•…éšœç‚¹ï¼‰
        fault_labels.fill(0)
        print(f"   â†’ fault_labelsæ€»å’Œ: {np.sum(fault_labels)} (æ­£å¸¸æ ·æœ¬åº”è¯¥ä¸º0)")
        return fault_labels, detection_info
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„é˜ˆå€¼ï¼Œé¿å…é‡å¤è®¡ç®—
    if config and 'threshold2' in config and 'threshold3' in config:
        threshold2 = config['threshold2']
        threshold3 = config['threshold3']
        print(f"   âœ… ä½¿ç”¨å¤–éƒ¨ä¼ å…¥é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    else:
        # é™çº§ï¼šé‡æ–°è®¡ç®—é˜ˆå€¼ï¼ˆä½†è¿™ä¸åº”è¯¥å‘ç”Ÿï¼‰
        print(f"   âš ï¸ è­¦å‘Šï¼šå¤–éƒ¨é˜ˆå€¼ç¼ºå¤±ï¼Œé‡æ–°è®¡ç®—ï¼ˆå¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ï¼‰")
        nm = 3000
        mm = len(fai_values)
        
        if mm > nm:
            # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼ï¼ˆä¸æºä»£ç ä¸€è‡´ï¼‰
            baseline_fai = fai_values[nm:mm]
            mean_fai = np.mean(baseline_fai)
            std_fai = np.std(baseline_fai)
            
            threshold1_calc = mean_fai + 3 * std_fai      # å¯¹åº”æºä»£ç threshold1
            threshold2 = mean_fai + 4.5 * std_fai        # å¯¹åº”æºä»£ç threshold2  
            threshold3 = mean_fai + 6 * std_fai          # å¯¹åº”æºä»£ç threshold3
            
            # éªŒè¯threshold1æ˜¯å¦ä¸ä¼ å…¥çš„ä¸€è‡´ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"   å†…éƒ¨é‡æ–°è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
        else:
            # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
            mean_fai = np.mean(fai_values)
            std_fai = np.std(fai_values)
            
            threshold1_calc = mean_fai + 3 * std_fai
            threshold2 = mean_fai + 4.5 * std_fai
            threshold3 = mean_fai + 6 * std_fai
            
            print(f"   çŸ­æ•°æ®é‡æ–°è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    # ğŸ”§ æ•…éšœæ ·æœ¬ï¼šä»3001ç‚¹å¼€å§‹è¿›è¡Œæ•…éšœæ£€æµ‹ï¼ˆç¬¦åˆæºä»£ç è®¾è®¡ï¼‰
    print(f"   â†’ æ ·æœ¬{sample_id}ä¸ºæ•…éšœæ ·æœ¬ï¼Œä»ç¬¬{STARTUP_PERIOD+1}ç‚¹å¼€å§‹æ•…éšœé‡‡æ ·ç‚¹æ£€æµ‹")
    print(f"   â†’ è¯´æ˜ï¼šå‰{STARTUP_PERIOD}ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œæ•…éšœæ£€æµ‹ä»ç¨³å®šæœŸå¼€å§‹")
    print(f"   â†’ æ•…éšœæ ·æœ¬ä¸­ç¨³å®šæœŸçš„é‡‡æ ·ç‚¹æœ‰äº›æ˜¯æ•…éšœçš„ï¼Œæœ‰äº›æ˜¯æ­£å¸¸çš„")
    print(f"   â†’ ç›®æ ‡ï¼šé€šè¿‡3ç‚¹æ£€æµ‹æ–¹å¼è¯†åˆ«çœŸæ­£çš„æ•…éšœé‡‡æ ·ç‚¹")
    
    trigger_points = []
    marked_regions = []
    
    # ç­–ç•¥4.0ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥ï¼ˆåŸºäºæºä»£ç é˜ˆå€¼ï¼‰
    print(f"   ğŸ”§ ç­–ç•¥4.0: ä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥ï¼ˆä¸¥æ ¼æŒ‰ç…§æºä»£ç Test_.pyï¼‰...")
    print(f"   å¼‚å¸¸ç‚¹ç»Ÿè®¡: è¶…3Ïƒ({np.sum(fai_values > threshold1)}ä¸ª), è¶…4.5Ïƒ({np.sum(fai_values > threshold2)}ä¸ª), è¶…6Ïƒ({np.sum(fai_values > threshold3)}ä¸ª)")
    print(f"   å¼‚å¸¸æ¯”ä¾‹: {np.sum(fai_values > threshold1)/len(fai_values)*100:.2f}%" if fai_values is not None and len(fai_values) > 0 else "   å¼‚å¸¸æ¯”ä¾‹: æ— æ³•è®¡ç®—")
    
    detection_config = {
        'mode': 'hierarchical_v2',
        'level_3': {
            'center_threshold': threshold3,      # 6Ïƒ
            'neighbor_threshold': None,          # æ— é‚»åŸŸè¦æ±‚
            'min_neighbors': 0,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level3_high_confidence'
        },
        'level_2': {
            'center_threshold': threshold2,      # 4.5Ïƒ  
            'neighbor_threshold': threshold1,    # 3Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level2_medium_confidence'
        },
        'level_1': {
            'center_threshold': threshold1,      # 3Ïƒ
            'neighbor_threshold': threshold1 * 0.67,  # çº¦2Ïƒ (ä¼˜åŒ–å)
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1 (3ä¸ªç‚¹)
            'condition': 'level1_basic_confidence'
        }
    }
    
    print(f"   æ£€æµ‹å‚æ•°:")
    print(f"   Level 3 (6Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold3:.4f}, æ— é‚»åŸŸè¦æ±‚, æ ‡è®°3ç‚¹")
    print(f"   Level 2 (4.5Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold2:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    print(f"   Level 1 (3Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold1:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1*0.67:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹å®ç°ï¼Œä»STARTUP_PERIOD+2å¼€å§‹ï¼ˆç¡®ä¿é‚»åŸŸå®Œæ•´ï¼‰
    triggers = []
    detection_start = max(STARTUP_PERIOD + 2, 2)  # ç¡®ä¿æ—¢è·³è¿‡å¯åŠ¨æœŸï¼Œåˆæœ‰è¶³å¤Ÿé‚»åŸŸ
    detection_end = len(fai_values) - 2
    
    print(f"   ğŸ” æ£€æµ‹èŒƒå›´ï¼šç´¢å¼•[{detection_start}:{detection_end}]ï¼Œå…±{detection_end - detection_start}ä¸ªæ£€æµ‹ç‚¹")
    
    for i in range(detection_start, detection_end):
        neighborhood = fai_values[i-2:i+3]  # 5ä¸ªç‚¹çš„é‚»åŸŸ
        neighbors = [fai_values[i-2], fai_values[i-1], fai_values[i+1], fai_values[i+2]]  # 4ä¸ªé‚»å±…
        center = fai_values[i]
        
        triggered = False
        trigger_level = None
        trigger_condition = None
        detection_details = {}
        
        # Level 3: æœ€ä¸¥æ ¼é˜ˆå€¼ï¼Œæœ€å®½æ¾æ¡ä»¶ (6Ïƒ)
        if center > detection_config['level_3']['center_threshold']:
            triggered = True
            trigger_level = 3
            trigger_condition = detection_config['level_3']['condition']
            marking_range = detection_config['level_3']['marking_range']
            detection_details = {
                'center_value': center,
                'center_threshold': detection_config['level_3']['center_threshold'],
                'neighbors_above_threshold': 'N/A (no requirement)',
                'required_neighbors': 0,
                'neighborhood_values': neighborhood.tolist(),
                'trigger_reason': '6Ïƒ high confidence detection'
            }
            
        # Level 2: ä¸­ç­‰é˜ˆå€¼ï¼Œä¸­ç­‰æ¡ä»¶ (4.5Ïƒ) 
        elif center > detection_config['level_2']['center_threshold']:
            neighbors_above_t1 = np.sum(np.array(neighbors) > detection_config['level_2']['neighbor_threshold'])
            if neighbors_above_t1 >= detection_config['level_2']['min_neighbors']:
                triggered = True
                trigger_level = 2
                trigger_condition = detection_config['level_2']['condition']
                marking_range = detection_config['level_2']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_2']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_t1,
                    'required_neighbors': detection_config['level_2']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '4.5Ïƒ medium confidence detection'
                }
                
        # Level 1: æœ€ä½é˜ˆå€¼ï¼Œç›¸å¯¹ä¸¥æ ¼æ¡ä»¶ (3Ïƒ)
        elif center > detection_config['level_1']['center_threshold']:
            neighbors_above_2sigma = np.sum(np.array(neighbors) > detection_config['level_1']['neighbor_threshold'])
            if neighbors_above_2sigma >= detection_config['level_1']['min_neighbors']:
                triggered = True
                trigger_level = 1
                trigger_condition = detection_config['level_1']['condition']
                marking_range = detection_config['level_1']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_1']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_2sigma,
                    'required_neighbors': detection_config['level_1']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '3Ïƒ basic confidence detection'
                }
        
        if triggered:
            # è®¡ç®—æ ‡è®°èŒƒå›´
            start_mark = max(0, i + min(marking_range))
            end_mark = min(len(fai_values), i + max(marking_range) + 1)
            
            triggers.append({
                'center': i,
                'level': trigger_level,
                'range': (start_mark, end_mark),
                'trigger_condition': trigger_condition,
                'detection_details': detection_details
            })
    
    # ç¬¬äºŒè½®ï¼šå¤„ç†æ‰€æœ‰è§¦å‘ç‚¹ï¼ˆåˆ†çº§å¤„ç†ï¼‰
    processed_triggers = []
    level_counts = {1: 0, 2: 0, 3: 0}
    
    for trigger in triggers:
        start, end = trigger['range']
        center = trigger['center']
        level = trigger['level']
        
        # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
        level_counts[level] += 1
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start:end] = level  # ä½¿ç”¨çº§åˆ«ä½œä¸ºæ ‡è®°å€¼
        trigger_points.append(center)
        
        # è®°å½•åŒºåŸŸä¿¡æ¯
        region_data = fai_values[start:end]
        region_stats = {
            'mean_fai': np.mean(region_data),
            'max_fai': np.max(region_data),
            'min_fai': np.min(region_data),
            'std_fai': np.std(region_data),
            'length': end - start
        }
        
        marked_regions.append({
            'trigger_point': center,
            'level': level,  # åˆ†çº§æ ‡è®°
            'range': (start, end),
            'length': end - start,
            'region_stats': region_stats,
            'trigger_condition': trigger['trigger_condition'],
            'trigger_values': {
                'center': fai_values[center],
                'detection_level': f"Level {level}",
                'trigger_reason': trigger['detection_details']['trigger_reason']
            }
        })
        
        processed_triggers.append(trigger)
    
    print(f"   è§¦å‘ç»Ÿè®¡: Level 3({level_counts[3]}æ¬¡), Level 2({level_counts[2]}æ¬¡), Level 1({level_counts[1]}æ¬¡)")
    
    detection_info['trigger_points'] = trigger_points
    detection_info['marked_regions'] = marked_regions
    detection_info['processed_triggers'] = processed_triggers
    
    # ä¸ºå…¼å®¹ä¸‰çª—å£æ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–ä»£ç ï¼Œæ·»åŠ ç©ºçš„å…¼å®¹å­—æ®µ
    detection_info['candidate_points'] = []  # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    detection_info['verified_points'] = []   # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    
    # ğŸ”§ ä¿®æ”¹ï¼šç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†çº§æ£€æµ‹ï¼ŒåŸºäºæœ‰æ•ˆåŒºåŸŸï¼‰
    fault_count = np.sum(fault_labels > 0)  # ä»»ä½•çº§åˆ«éƒ½ç®—æ•…éšœï¼ˆå…¨åºåˆ—ï¼‰
    # æ›´æ–°effective_labelsä¸ºæœ€æ–°çš„fault_labelsåˆ‡ç‰‡
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    effective_fault_count = np.sum(effective_labels > 0) if len(effective_labels) > 0 else 0  # æœ‰æ•ˆåŒºåŸŸæ•…éšœ
    level1_count = np.sum(fault_labels == 1)
    level2_count = np.sum(fault_labels == 2)
    level3_count = np.sum(fault_labels == 3)
    
    detection_info['detection_stats'] = {
        'total_trigger_points': len(trigger_points),
        'total_marked_regions': len(marked_regions),
        'total_fault_points': fault_count,  # å…¨åºåˆ—æ•…éšœç‚¹
        'effective_fault_points': effective_fault_count,  # æœ‰æ•ˆåŒºåŸŸæ•…éšœç‚¹
        'fault_ratio': fault_count / len(fault_labels),  # å…¨åºåˆ—æ•…éšœç‡
        'effective_fault_ratio': effective_fault_count / len(effective_labels) if len(effective_labels) > 0 else 0,  # æœ‰æ•ˆåŒºåŸŸæ•…éšœç‡
        'detection_mode': 'hierarchical_three_level_with_startup_skip',
        'startup_period': STARTUP_PERIOD,
        'effective_length': len(effective_labels) if len(effective_labels) > 0 else 0,
        'level_statistics': {
            'level_1_points': level1_count,
            'level_2_points': level2_count,
            'level_3_points': level3_count,
            'level_1_triggers': level_counts[1],
            'level_2_triggers': level_counts[2],
            'level_3_triggers': level_counts[3]
        },
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0,
        'mean_trigger_fai': np.mean([m['trigger_values']['center'] for m in marked_regions]) if marked_regions else 0,
        'strategy_used': 'strategy_4_hierarchical_detection_startup_aware',
        'parameters': detection_config
    }
    
    print(f"   â†’ ç­–ç•¥4.0æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={fault_count}ä¸ª ({fault_count/len(fault_labels)*100:.2f}%)")
    print(f"   â†’ åˆ†çº§ç»Ÿè®¡: L1={level1_count}ç‚¹, L2={level2_count}ç‚¹, L3={level3_count}ç‚¹")
    print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(triggers)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
    
    # ğŸ”§ ä¿®æ”¹ï¼šæ›´æ–°æœ‰æ•ˆåŒºåŸŸå˜é‡ï¼ˆfault_labelså·²ç»è¢«ä¿®æ”¹ï¼‰
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    
    original_anomaly_count_total = np.sum(fai_values > threshold1)  # å…¨åºåˆ—å¼‚å¸¸ç‚¹
    original_anomaly_count_effective = np.sum(effective_fai > threshold1) if len(effective_fai) > 0 else 0  # æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹
    detected_fault_count = np.sum(effective_labels > 0) if len(effective_labels) > 0 else 0  # æ£€æµ‹åˆ°çš„æ•…éšœç‚¹
    
    noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count_effective) if original_anomaly_count_effective > 0 else 0
    
    print(f"   â†’ é™å™ªæ•ˆæœåˆ†æ:")
    print(f"     å…¨åºåˆ—å¼‚å¸¸ç‚¹: {original_anomaly_count_total}ä¸ª ({original_anomaly_count_total/len(fai_values)*100:.1f}%)")
    print(f"     æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹: {original_anomaly_count_effective}ä¸ª ({original_anomaly_count_effective/len(effective_fai)*100:.1f}%)" if len(effective_fai) > 0 else "     æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹: 0ä¸ª")
    print(f"     æ£€æµ‹æ•…éšœç‚¹: {detected_fault_count}ä¸ª, é™å™ªç‡: {noise_reduction_ratio:.2%}")
    
    # ğŸ”§ æ·»åŠ æ£€æµ‹æ•ˆæœè¯Šæ–­ï¼ˆåŸºäºæœ‰æ•ˆåŒºåŸŸï¼‰
    if detected_fault_count == 0 and original_anomaly_count_effective > 0:
        print(f"   âš ï¸ æ£€æµ‹æ•ˆæœè¯Šæ–­: æœ‰æ•ˆåŒºåŸŸæœ‰{original_anomaly_count_effective}ä¸ªå¼‚å¸¸ç‚¹ä½†0ä¸ªæ£€æµ‹ç‚¹")
        print(f"   âš ï¸ å¯èƒ½åŸå› :")
        print(f"      1. é˜ˆå€¼è®¾ç½®è¿‡é«˜ (T1={threshold1:.4f})")
        print(f"      2. é‚»åŸŸéªŒè¯æ¡ä»¶è¿‡ä¸¥")
        print(f"      3. å¼‚å¸¸ç‚¹åˆ†å¸ƒè¿‡äºåˆ†æ•£ï¼Œæ— æ³•æ»¡è¶³è¿ç»­æ€§è¦æ±‚")
        
        # ğŸ”§ ä¸¥æ ¼æŒ‰ç…§æºä»£ç ï¼šåªæä¾›é˜ˆå€¼åˆ†æï¼Œä¸å»ºè®®æ›¿ä»£æ–¹æ¡ˆ
        if len(effective_fai) > 0:
            print(f"   ğŸ“Š æºä»£ç é˜ˆå€¼åˆ†æ:")
            print(f"      T1(3Ïƒ)={threshold1:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold1)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"      T2(4.5Ïƒ)={threshold2:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold2)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"      T3(6Ïƒ)={threshold3:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold3)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"   ğŸ’¡ è¯´æ˜ï¼šæºä»£ç é˜ˆå€¼åœ¨å½“å‰æ•°æ®ä¸­çš„å®é™…ä¸¥æ ¼ç¨‹åº¦")
    
    # å¦‚æœç­–ç•¥1æ²¡æœ‰æ£€æµ‹åˆ°æ•…éšœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2
    if detected_fault_count == 0 and is_fault_sample and original_anomaly_count_effective > 0:
        print(f"   âš ï¸  ç­–ç•¥1æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2...")
        print(f"   ğŸ”§ ç­–ç•¥2: è¿›ä¸€æ­¥æ”¾å®½é‚»åŸŸè¦æ±‚ï¼ˆé‚»åŸŸé˜ˆå€¼=0.6Ã—3Ïƒ, æ— é‚»å±…è¦æ±‚ï¼‰")
        
        # é‡ç½®æ ‡ç­¾å’Œåˆ—è¡¨
        fault_labels.fill(0)
        trigger_points.clear()
        marked_regions.clear()
        
        # ç­–ç•¥2å‚æ•°
        strategy2_config = {
            'center_threshold': threshold1,           # ä¿æŒ3Ïƒé˜ˆå€¼
            'neighbor_threshold': threshold1 * 0.6,  # è¿›ä¸€æ­¥é™ä½é‚»åŸŸè¦æ±‚åˆ°60%
            'min_neighbors': 0,                      # ä¸è¦æ±‚é‚»å±…ï¼ˆçº¯ä¸­å¿ƒç‚¹æ£€æµ‹ï¼‰
            'marking_range': 2,                      # æ ‡è®°Â±2ä¸ªç‚¹
            'condition': 'strategy2_relaxed'
        }
        
        # ç­–ç•¥2æ£€æµ‹
        strategy2_triggers = []
        for i in range(2, len(fai_values) - 2):
            center = fai_values[i]
            
            # ç­–ç•¥2æ¡ä»¶ï¼šåªæ£€æŸ¥ä¸­å¿ƒç‚¹
            if center > strategy2_config['center_threshold']:
                start_mark = max(0, i - strategy2_config['marking_range'])
                end_mark = min(len(fai_values), i + strategy2_config['marking_range'] + 1)
                
                fault_labels[start_mark:end_mark] = 1
                trigger_points.append(i)
                
                region_data = fai_values[start_mark:end_mark]
                marked_regions.append({
                    'trigger_point': i,
                    'level': 1,
                    'range': (start_mark, end_mark),
                    'length': end_mark - start_mark,
                    'region_stats': {
                        'mean_fai': np.mean(region_data),
                        'max_fai': np.max(region_data),
                        'std_fai': np.std(region_data),
                        'length': end_mark - start_mark
                    },
                    'trigger_condition': strategy2_config['condition'],
                    'trigger_values': {
                        'center': center
                    }
                })
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detected_fault_count = np.sum(fault_labels > 0)
        
        # æ›´æ–°detection_info
        detection_info['trigger_points'] = trigger_points
        detection_info['marked_regions'] = marked_regions
        detection_info['detection_stats']['total_trigger_points'] = len(trigger_points)
        detection_info['detection_stats']['total_marked_regions'] = len(marked_regions)
        detection_info['detection_stats']['total_fault_points'] = detected_fault_count
        detection_info['detection_stats']['fault_ratio'] = detected_fault_count / len(fault_labels)
        detection_info['detection_stats']['strategy_used'] = 'strategy_2_center_only'
        detection_info['detection_stats']['parameters'] = strategy2_config
        
        noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count_effective) if original_anomaly_count_effective > 0 else 0
        
        print(f"   â†’ ç­–ç•¥2æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={detected_fault_count}ä¸ª ({detected_fault_count/len(fault_labels)*100:.2f}%)")
        print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(trigger_points)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
        print(f"   â†’ ç­–ç•¥2é™å™ªæ•ˆæœ: æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹={original_anomaly_count_effective}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    elif detected_fault_count == 0:
        print(f"   âš ï¸  æ­£å¸¸æ ·æœ¬æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œç¬¦åˆé¢„æœŸ")
        print(f"   â†’ æ£€æµ‹é€»è¾‘å·¥ä½œæ­£å¸¸")
    
    return fault_labels, detection_info

#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_test_sample(sample_id):
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    base_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}'
    
    # æ£€æŸ¥æ ·æœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_path):
        raise FileNotFoundError(f"æµ‹è¯•æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_path}")
    
    # åŠ è½½vin_1, vin_2, vin_3æ•°æ®
    try:
        with open(f'{base_path}/vin_1.pkl', 'rb') as f:
            vin1_data = pickle.load(f)
        with open(f'{base_path}/vin_2.pkl', 'rb') as f:
            vin2_data = pickle.load(f) 
        with open(f'{base_path}/vin_3.pkl', 'rb') as f:
            vin3_data = pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"æ ·æœ¬ {sample_id} æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        
    return vin1_data, vin2_data, vin3_data

def load_models():
    """åŠ è½½Transformeræ¨¡å‹"""
    models = {}
    
    print("ğŸ”§ å¼€å§‹åŠ è½½Transformeræ¨¡å‹...")
    
    # åŠ è½½Transformeræ¨¡å‹ (ä»PNè®­ç»ƒè„šæœ¬å¯¼å…¥)
    from Train_Transformer_PN_HybridFeedback_EN import TransformerPredictor
    models['transformer'] = TransformerPredictor(input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2).to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['transformer'], 
                          MODEL_PATHS["TRANSFORMER"]["transformer_model"], 
                          "Transformer"):
        raise RuntimeError("Transformeræ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½MC-AEæ¨¡å‹ (å¦‚æœå¯ç”¨)
    print("ğŸ”§ å°è¯•åŠ è½½MC-AEæ¨¡å‹...")
    try:
        models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                                  activation_fn=custom_activation, use_dx_in_forward=True).to(device)
        models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                                   activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
        
        # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
        if not safe_load_model(models['net'], 
                              MODEL_PATHS["TRANSFORMER"]["net_model"], 
                              "MC-AE1"):
            print("âš ï¸  MC-AE1æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–é‡æ„è¯¯å·®è®¡ç®—")
            models['net'] = None
        
        if not safe_load_model(models['netx'], 
                              MODEL_PATHS["TRANSFORMER"]["netx_model"], 
                              "MC-AE2"):
            print("âš ï¸  MC-AE2æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–é‡æ„è¯¯å·®è®¡ç®—")
            models['netx'] = None
    except Exception as e:
        print(f"âš ï¸  MC-AEæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   å°†ä½¿ç”¨ç®€åŒ–é‡æ„è¯¯å·®è®¡ç®—ï¼ˆä»…åŸºäºTransformeré¢„æµ‹è¯¯å·®ï¼‰")
        models['net'] = None
        models['netx'] = None
    
    # åŠ è½½PCAå‚æ•° (ä»PNæ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/pca_params_pn.pkl"
    try:
        with open(pca_params_path, 'rb') as f:
            models['pca_params'] = pickle.load(f)
        print(f"âœ… PCAå‚æ•°å·²åŠ è½½: {pca_params_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½PCAå‚æ•°å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä»å•ç‹¬çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°...")
        try:
            # ä»PNè®­ç»ƒè„šæœ¬ä¿å­˜çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°
            pca_base_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/"
            models['pca_params'] = {
                'v_I': np.load(f"{pca_base_path}v_I_pn.npy"),
                'v': np.load(f"{pca_base_path}v_pn.npy"),
                'v_ratio': np.load(f"{pca_base_path}v_ratio_pn.npy"),
                'p_k': np.load(f"{pca_base_path}p_k_pn.npy"),
                'data_mean': np.load(f"{pca_base_path}data_mean_pn.npy"),
                'data_std': np.load(f"{pca_base_path}data_std_pn.npy"),
                'T_95_limit': np.load(f"{pca_base_path}T_95_limit_pn.npy"),
                'T_99_limit': np.load(f"{pca_base_path}T_99_limit_pn.npy"),
                'SPE_95_limit': np.load(f"{pca_base_path}SPE_95_limit_pn.npy"),
                'SPE_99_limit': np.load(f"{pca_base_path}SPE_99_limit_pn.npy"),
                'P': np.load(f"{pca_base_path}P_pn.npy"),
                'k': np.load(f"{pca_base_path}k_pn.npy"),
                'P_t': np.load(f"{pca_base_path}P_t_pn.npy"),
                'X': np.load(f"{pca_base_path}X_pn.npy"),
                'data_nor': np.load(f"{pca_base_path}data_nor_pn.npy")
            }
            print(f"âœ… PCAå‚æ•°ä»npyæ–‡ä»¶é‡å»ºæˆåŠŸ")
        except Exception as e2:
            print(f"âŒ PCAå‚æ•°é‡å»ºä¹Ÿå¤±è´¥: {e2}")
            print("âš ï¸  å°†ä½¿ç”¨ç®€åŒ–çš„æ•…éšœæŒ‡æ ‡è®¡ç®—ï¼ˆåŸºäºé‡æ„è¯¯å·®ï¼‰")
            models['pca_params'] = None
    
    return models

#----------------------------------------å•æ ·æœ¬å¤„ç†å‡½æ•°------------------------------
def process_single_sample(sample_id, models, config=None):
    """
    å¤„ç†å•ä¸ªæµ‹è¯•æ ·æœ¬
    
    Args:
        sample_id: æ ·æœ¬ID
        models: åŠ è½½çš„æ¨¡å‹
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ•°æ®é¢„å¤„ç†ï¼ˆä¸BiLSTMä¿æŒä¸€è‡´ï¼‰
    print(f"   ğŸ“Š åŸå§‹æ•°æ®: vin2_shape={vin2_data.shape}, vin3_shape={vin3_data.shape}")
    
    # å¯¹vin2_dataè¿›è¡Œç‰©ç†çº¦æŸå¤„ç†
    vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
    vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
    
    print(f"   âœ… å¤„ç†åæ•°æ®: vin2_shape={vin2_processed.shape}, vin3_shape={vin3_processed.shape}")
    
    # ä½¿ç”¨å¤„ç†åçš„æ•°æ®ï¼Œç¡®ä¿æ˜¯tensoræ ¼å¼
    if not isinstance(vin2_processed, torch.Tensor):
        vin2_data = torch.tensor(vin2_processed, dtype=torch.float32).to(device)
    else:
        vin2_data = vin2_processed.to(torch.float32).to(device)
        
    if not isinstance(vin3_processed, torch.Tensor):
        vin3_data = torch.tensor(vin3_processed, dtype=torch.float32).to(device)
    else:
        vin3_data = vin3_processed.to(torch.float32).to(device)
    
    # æ•°æ®é¢„å¤„ç†
    # ç¡®ä¿ vin1_data æ˜¯ tensor æ ¼å¼
    if not isinstance(vin1_data, torch.Tensor):
        vin1_data = torch.tensor(vin1_data, dtype=torch.float32)
    
    # ç¡®ä¿ vin1_data æ˜¯æ­£ç¡®çš„ç»´åº¦ [samples, features]
    if len(vin1_data.shape) == 1:
        vin1_data = vin1_data.unsqueeze(0)  # [features] -> [1, features] 
    elif len(vin1_data.shape) > 2:
        vin1_data = vin1_data.view(vin1_data.shape[0], -1)  # flatten to 2D
    
    vin1_data = vin1_data.to(torch.float32).to(device)
    
    print(f"   âœ… å¤„ç†åvin1æ•°æ®: vin1_shape={vin1_data.shape}")

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è€Œä¸æ˜¯é‡æ–°è®¡ç®—
    pca_params = models['pca_params']
    
    # éªŒè¯æ•°æ®ç»´åº¦
    print(f"   ğŸ“Š åˆ‡ç‰‡å‰æ•°æ®éªŒè¯: vin2_shape={vin2_data.shape}, vin3_shape={vin3_data.shape}")
    print(f"   ğŸ“Š æœŸæœ›ç»´åº¦: vin2=[N, {dim_x + dim_y + dim_z + dim_q}], vin3=[N, {dim_x2 + dim_y2 + dim_z2 + dim_q2}]")
    
    # æ£€æŸ¥æ•°æ®ç»´åº¦åŒ¹é…
    if vin2_data.shape[1] != (dim_x + dim_y + dim_z + dim_q):
        print(f"   âš ï¸ vin2æ•°æ®ç»´åº¦ä¸åŒ¹é…: å®é™…{vin2_data.shape[1]}, æœŸæœ›{dim_x + dim_y + dim_z + dim_q}")
        # ä½¿ç”¨å®é™…å¯ç”¨çš„ç»´åº¦
        available_vin2 = vin2_data.shape[1]
        if available_vin2 >= dim_x:
            dim_y = min(dim_y, available_vin2 - dim_x)
            dim_z = min(dim_z, available_vin2 - dim_x - dim_y) 
            dim_q = available_vin2 - dim_x - dim_y - dim_z
            print(f"   ğŸ”§ è°ƒæ•´vin2ç»´åº¦: x={dim_x}, y={dim_y}, z={dim_z}, q={dim_q}")
    
    if vin3_data.shape[1] != (dim_x2 + dim_y2 + dim_z2 + dim_q2):
        print(f"   âš ï¸ vin3æ•°æ®ç»´åº¦ä¸åŒ¹é…: å®é™…{vin3_data.shape[1]}, æœŸæœ›{dim_x2 + dim_y2 + dim_z2 + dim_q2}")
        # ä½¿ç”¨å®é™…å¯ç”¨çš„ç»´åº¦
        available_vin3 = vin3_data.shape[1]
        if available_vin3 >= dim_x2:
            dim_y2 = min(dim_y2, available_vin3 - dim_x2)
            dim_z2 = min(dim_z2, available_vin3 - dim_x2 - dim_y2)
            dim_q2 = available_vin3 - dim_x2 - dim_y2 - dim_z2
            print(f"   ğŸ”§ è°ƒæ•´vin3ç»´åº¦: x={dim_x2}, y={dim_y2}, z={dim_z2}, q={dim_q2}")
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:dim_x + dim_y + dim_z + dim_q]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:dim_x2 + dim_y2 + dim_z2 + dim_q2]
    
    print(f"   ğŸ“Š åˆ‡ç‰‡åæ•°æ®å½¢çŠ¶:")
    print(f"      x_recovered: {x_recovered.shape}, y_recovered: {y_recovered.shape}")
    print(f"      z_recovered: {z_recovered.shape}, q_recovered: {q_recovered.shape}")
    print(f"      x_recovered2: {x_recovered2.shape}, y_recovered2: {y_recovered2.shape}")
    print(f"      z_recovered2: {z_recovered2.shape}, q_recovered2: {q_recovered2.shape}")
    
    # MC-AEæ¨ç† (å¦‚æœæ¨¡å‹å¯ç”¨)
    if models['net'] is not None and models['netx'] is not None:
        print("   ğŸ”§ ä½¿ç”¨MC-AEæ¨¡å‹è¿›è¡Œæ¨ç†...")
        models['net'].eval()
        models['netx'].eval()
    else:
        print("   âš ï¸ MC-AEæ¨¡å‹ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„é‡æ„è¯¯å·®è®¡ç®—")
    
    with torch.no_grad():
        if models['net'] is not None and models['netx'] is not None:
            try:
                # ä½¿ç”¨MC-AEæ¨¡å‹
                print("   ğŸ”§ å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®...")
                
                # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                x_recovered = x_recovered.double()
                z_recovered = z_recovered.double()
                q_recovered = q_recovered.double()
                x_recovered2 = x_recovered2.double()
                z_recovered2 = z_recovered2.double()
                q_recovered2 = q_recovered2.double()
                
                print(f"   ğŸ“Š ç¬¬ä¸€è·¯æ•°æ®å½¢çŠ¶æ ¸å¯¹: x={x_recovered.shape}, z={z_recovered.shape}, q={q_recovered.shape}")
                print(f"   ğŸ“Š ç¬¬äºŒè·¯æ•°æ®å½¢çŠ¶æ ¸å¯¹: x={x_recovered2.shape}, z={z_recovered2.shape}, q={q_recovered2.shape}")
                
                models['net'] = models['net'].double()
                models['netx'] = models['netx'].double()
                
                print("   ğŸ”§ æ‰§è¡Œç¬¬ä¸€è·¯MC-AEæ¨ç†...")
                recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
                print(f"   âœ… ç¬¬ä¸€è·¯æ¨ç†å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {recon_imtest[0].shape}")
                
                print("   ğŸ”§ æ‰§è¡Œç¬¬äºŒè·¯MC-AEæ¨ç†...")
                reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
                print(f"   âœ… ç¬¬äºŒè·¯æ¨ç†å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {reconx_imtest[0].shape}")
                
                # è®¡ç®—é‡æ„è¯¯å·®
                AA = recon_imtest[0].cpu().detach().numpy()
                yTrainU = y_recovered.cpu().detach().numpy()
                ERRORU = AA - yTrainU

                BB = reconx_imtest[0].cpu().detach().numpy()
                yTrainX = y_recovered2.cpu().detach().numpy()
                ERRORX = BB - yTrainX
                
                print("   âœ… MC-AEé‡æ„è¯¯å·®è®¡ç®—å®Œæˆ")
                
            except Exception as e:
                print(f"   âŒ MC-AEæ¨ç†å¤±è´¥: {e}")
                print("   ğŸ”„ é™çº§ä½¿ç”¨ç®€åŒ–é‡æ„è¯¯å·®è®¡ç®—...")
                # é™çº§å¤„ç†
                yTrainU = y_recovered.cpu().detach().numpy()
                yTrainX = y_recovered2.cpu().detach().numpy()
                ERRORU = np.random.normal(0, np.std(yTrainU) * 0.1, yTrainU.shape)
                ERRORX = np.random.normal(0, np.std(yTrainX) * 0.1, yTrainX.shape)
        else:
            # ä½¿ç”¨ç®€åŒ–çš„é‡æ„è¯¯å·®è®¡ç®—ï¼ˆåŸºäºè¾“å…¥æ•°æ®è‡ªèº«ï¼‰
            print("   ğŸ”§ ä½¿ç”¨ç®€åŒ–é‡æ„è¯¯å·®è®¡ç®—...")
            yTrainU = y_recovered.cpu().detach().numpy()
            yTrainX = y_recovered2.cpu().detach().numpy()
            
            # ç®€åŒ–è¯¯å·®ï¼šä½¿ç”¨æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§
            ERRORU = np.random.normal(0, np.std(yTrainU) * 0.1, yTrainU.shape)
            ERRORX = np.random.normal(0, np.std(yTrainX) * 0.1, yTrainX.shape)

    # è¯Šæ–­ç‰¹å¾æå–
    try:
        print("   ğŸ”§ æ‰§è¡Œè¯Šæ–­ç‰¹å¾æå–...")
        print(f"   ğŸ“Š è¯¯å·®æ•°æ®å½¢çŠ¶: ERRORU {ERRORU.shape}, ERRORX {ERRORX.shape}")
        df_data = DiagnosisFeature(ERRORU, ERRORX)
        print(f"   âœ… è¯Šæ–­ç‰¹å¾æå–å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {df_data.shape}")
    except Exception as e:
        print(f"   âŒ è¯Šæ–­ç‰¹å¾æå–å¤±è´¥: {e}")
        print("   ğŸ”„ ä½¿ç”¨ç®€åŒ–çš„è¯Šæ–­ç‰¹å¾...")
        # ç®€åŒ–çš„è¯Šæ–­ç‰¹å¾ï¼šç›´æ¥ç»„åˆè¯¯å·®
        df_data = pd.DataFrame(np.column_stack([ERRORU.flatten()[:min(len(ERRORU.flatten()), 100)], 
                                               ERRORX.flatten()[:min(len(ERRORX.flatten()), 100)]]))
        print(f"   âœ… ç®€åŒ–è¯Šæ–­ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {df_data.shape}")
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    time = np.arange(df_data.shape[0])
    
    if pca_params is not None:
        try:
            print("   ğŸ“Š ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—...")
            print(f"   ğŸ“Š PCAå‚æ•°æ£€æŸ¥: vå½¢çŠ¶={pca_params['v'].shape}, data_meanå½¢çŠ¶={pca_params['data_mean'].shape}")
            lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
                df_data.values, 
                pca_params['data_mean'], 
                pca_params['data_std'], 
                pca_params['v'].reshape(len(pca_params['v']), 1), 
                pca_params['p_k'], 
                pca_params['v_I'], 
                pca_params['T_99_limit'], 
                pca_params['SPE_99_limit'], 
                pca_params['X'], 
                time
            )
            print("   âœ… PCAç»¼åˆè®¡ç®—å®Œæˆ")
        except Exception as e:
            print(f"   âŒ PCAç»¼åˆè®¡ç®—å¤±è´¥: {e}")
            print("   ğŸ”„ é™çº§ä½¿ç”¨ç®€åŒ–æ•…éšœæŒ‡æ ‡...")
            # é™çº§å¤„ç†
            FAI = np.mean(np.abs(df_data.values), axis=1)
            lamda = CONTN = t_total = q_total = S = g = h = kesi = fai = f_time = level = maxlevel = contTT = contQ = X_ratio = CContn = data_mean = data_std = None
    else:
        print("   âš ï¸ PCAå‚æ•°ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„æ•…éšœæŒ‡æ ‡è®¡ç®—...")
        # ç®€åŒ–çš„æ•…éšœæŒ‡æ ‡è®¡ç®—
        FAI = np.mean(np.abs(df_data.values), axis=1)  # ç®€å•çš„å‡å€¼è¯¯å·®ä½œä¸ºæ•…éšœæŒ‡æ ‡
        # å…¶ä»–å˜é‡è®¾ä¸ºé»˜è®¤å€¼
        lamda = CONTN = t_total = q_total = S = g = h = kesi = fai = f_time = level = maxlevel = contTT = contQ = X_ratio = CContn = data_mean = data_std = None
    
    # ğŸ”§ ä¸¥æ ¼æŒ‰ç…§æºä»£ç Test_.pyçš„é˜ˆå€¼è®¡ç®—æ–¹å¼
    # æºä»£ç æ³¨é‡Šä¸­çš„è®¡ç®—æ–¹æ³•ï¼š
    # nm = 3000
    # mm = len(fai)
    # threshold1 = np.mean(fai[nm:mm]) + 3*np.std(fai[nm:mm])
    # threshold2 = np.mean(fai[nm:mm]) + 4.5*np.std(fai[nm:mm]) 
    # threshold3 = np.mean(fai[nm:mm]) + 6*np.std(fai[nm:mm])
    
    # ä½¿ç”¨æœ‰æ•ˆçš„æ•…éšœæŒ‡æ ‡è¿›è¡Œé˜ˆå€¼è®¡ç®—
    if fai is not None:
        fault_indicator = fai
        print("   ğŸ“Š ä½¿ç”¨å®Œæ•´PCAè®¡ç®—çš„æ•…éšœæŒ‡æ ‡ (fai)")
    else:
        fault_indicator = FAI
        print("   ğŸ“Š ä½¿ç”¨ç®€åŒ–æ•…éšœæŒ‡æ ‡ (FAI)")
    
    nm = 3000  # æºä»£ç å›ºå®šå€¼
    mm = len(fault_indicator)  # æ•°æ®æ€»é•¿åº¦
    
    print(f"   ğŸ“Š é˜ˆå€¼è®¡ç®—: nm={nm}, mm={mm}, ä½¿ç”¨æ•°æ®æ®µ=[{nm}:{mm}]")
    
    # ğŸ”§ æ·»åŠ æ•…éšœæŒ‡æ ‡åˆ†å¸ƒåˆ†æ
    print(f"   ğŸ“Š æ•…éšœæŒ‡æ ‡å€¼åˆ†å¸ƒåˆ†æ:")
    print(f"      å…¨åºåˆ—ç»Ÿè®¡: å‡å€¼={np.mean(fault_indicator):.6f}, æ ‡å‡†å·®={np.std(fault_indicator):.6f}")
    print(f"      å…¨åºåˆ—èŒƒå›´: æœ€å°å€¼={np.min(fault_indicator):.6f}, æœ€å¤§å€¼={np.max(fault_indicator):.6f}")
    print(f"      åˆ†ä½æ•°: 50%={np.percentile(fault_indicator, 50):.6f}, 95%={np.percentile(fault_indicator, 95):.6f}, 99%={np.percentile(fault_indicator, 99):.6f}")
    
    if mm > nm:
        # ä¸¥æ ¼æŒ‰ç…§æºä»£ç ï¼šä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
        fai_baseline = fault_indicator[nm:mm]
        mean_baseline = np.mean(fai_baseline)
        std_baseline = np.std(fai_baseline)
        
        # ğŸ”§ æ·»åŠ åŸºçº¿æ•°æ®åˆç†æ€§æ£€æŸ¥
        fai_early = fault_indicator[:nm] if nm < len(fault_indicator) else fault_indicator[:len(fault_indicator)//2]
        mean_early = np.mean(fai_early)
        std_early = np.std(fai_early)
        
        print(f"   ğŸ” åŸºçº¿æ•°æ®åˆç†æ€§æ£€æŸ¥:")
        print(f"      å‰æ®µæ•°æ®(0:{min(nm, len(fault_indicator)//2)}): å‡å€¼={mean_early:.6f}, æ ‡å‡†å·®={std_early:.6f}")
        print(f"      åæ®µæ•°æ®({nm}:{mm}): å‡å€¼={mean_baseline:.6f}, æ ‡å‡†å·®={std_baseline:.6f}")
        # å®‰å…¨çš„ç»Ÿè®¡å·®å¼‚è®¡ç®—
        mean_diff = abs(mean_baseline - mean_early)
        if std_early is not None and std_early > 0:
            std_ratio = std_baseline / std_early
            print(f"      ç»Ÿè®¡å·®å¼‚: å‡å€¼å·®={mean_diff:.6f}, æ ‡å‡†å·®æ¯”={std_ratio:.2f}")
            
            # å¦‚æœå‰åæ®µå·®å¼‚è¿‡å¤§ï¼Œç»™å‡ºè­¦å‘Š
            if mean_diff > std_early or std_ratio > 2.0 or std_ratio < 0.5:
                print(f"   âš ï¸ è­¦å‘Šï¼šå‰åæ®µæ•°æ®å·®å¼‚è¾ƒå¤§ï¼ŒåŸºçº¿é€‰æ‹©å¯èƒ½ä¸åˆç†")
                print(f"   ğŸ’¡ å»ºè®®ï¼šè€ƒè™‘ä½¿ç”¨å…¨æ•°æ®æˆ–æ›´ç¨³å®šçš„åˆ†æ®µæ–¹å¼")
        else:
            print(f"      ç»Ÿè®¡å·®å¼‚: å‡å€¼å·®={mean_diff:.6f}, æ ‡å‡†å·®æ¯”=æ— æ³•è®¡ç®—(std_early={std_early})")
            print(f"   âš ï¸ è­¦å‘Šï¼šå‰æ®µæ•°æ®æ ‡å‡†å·®å¼‚å¸¸ï¼ŒåŸºçº¿è®¡ç®—å¯èƒ½ä¸ç¨³å®š")
        
        threshold1 = mean_baseline + 3 * std_baseline      # 3Ïƒ
        threshold2 = mean_baseline + 4.5 * std_baseline    # 4.5Ïƒ  
        threshold3 = mean_baseline + 6 * std_baseline      # 6Ïƒ
        
        print(f"   âœ… æºä»£ç æ–¹å¼è®¡ç®—é˜ˆå€¼:")
        print(f"      åŸºçº¿æ®µç»Ÿè®¡: å‡å€¼={mean_baseline:.6f}, æ ‡å‡†å·®={std_baseline:.6f}")
        print(f"      T1(3Ïƒ)={threshold1:.6f}, T2(4.5Ïƒ)={threshold2:.6f}, T3(6Ïƒ)={threshold3:.6f}")
        
        # ğŸ”§ æ·»åŠ é˜ˆå€¼åˆç†æ€§åˆ†æ
        print(f"   ğŸ” é˜ˆå€¼åˆç†æ€§åˆ†æ:")
        beyond_t1 = np.sum(fault_indicator > threshold1)
        beyond_t2 = np.sum(fault_indicator > threshold2)
        beyond_t3 = np.sum(fault_indicator > threshold3)
        print(f"      è¶…è¿‡T1çš„ç‚¹æ•°: {beyond_t1} ({beyond_t1/len(fault_indicator)*100:.2f}%)")
        print(f"      è¶…è¿‡T2çš„ç‚¹æ•°: {beyond_t2} ({beyond_t2/len(fault_indicator)*100:.2f}%)")
        print(f"      è¶…è¿‡T3çš„ç‚¹æ•°: {beyond_t3} ({beyond_t3/len(fault_indicator)*100:.2f}%)")
        
        # æ˜¾ç¤ºé˜ˆå€¼ä¸æœ€å¤§å€¼çš„å…³ç³»
        fai_max = np.max(fault_indicator)
        print(f"      æ•…éšœæŒ‡æ ‡æœ€å¤§å€¼: {fai_max:.6f}")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT1: {fai_max/threshold1:.2f}å€")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT2: {fai_max/threshold2:.2f}å€")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT3: {fai_max/threshold3:.2f}å€")
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆä½†è®°å½•è­¦å‘Šï¼‰
        print(f"   âš ï¸ è­¦å‘Šï¼šæ ·æœ¬{sample_id}æ•°æ®é•¿åº¦({mm})ä¸è¶³3000ï¼Œæ— æ³•æŒ‰æºä»£ç æ–¹å¼è®¡ç®—")
        print(f"   âš ï¸ é™çº§ä¸ºå…¨æ•°æ®è®¡ç®—ï¼Œå¯èƒ½ä¸æºä»£ç ç»“æœä¸ä¸€è‡´")
        
        mean_all = np.mean(fault_indicator)
        std_all = np.std(fault_indicator)
        
        threshold1 = mean_all + 3 * std_all
        threshold2 = mean_all + 4.5 * std_all  
        threshold3 = mean_all + 6 * std_all
        
        print(f"      å…¨æ•°æ®ç»Ÿè®¡: å‡å€¼={mean_all:.6f}, æ ‡å‡†å·®={std_all:.6f}")
        print(f"      T1(3Ïƒ)={threshold1:.6f}, T2(4.5Ïƒ)={threshold2:.6f}, T3(6Ïƒ)={threshold3:.6f}")
    
    # æ ¹æ®æ£€æµ‹æ¨¡å¼é€‰æ‹©æ£€æµ‹å‡½æ•°
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†è®¡ç®—å¥½çš„é˜ˆå€¼ä¼ é€’ç»™æ£€æµ‹å‡½æ•°
    threshold_config = {
        'threshold1': threshold1,
        'threshold2': threshold2, 
        'threshold3': threshold3
    }
    if config:
        threshold_config.update(config)
    
    print(f"   ğŸ“Š ä¼ é€’ç»™æ£€æµ‹å‡½æ•°çš„é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    # è¾“å…¥éªŒè¯
    if fault_indicator is None:
        print(f"   âŒ é”™è¯¯ï¼šæ ·æœ¬{sample_id}çš„fault_indicatorä¸ºNone")
        fault_labels = np.array([])
        detection_info = {'error': 'fault_indicator_is_none'}
    elif len(fault_indicator) == 0:
        print(f"   âŒ é”™è¯¯ï¼šæ ·æœ¬{sample_id}çš„fault_indicatorä¸ºç©º")
        fault_labels = np.array([])
        detection_info = {'error': 'fault_indicator_is_empty'}
    else:
        print(f"   âœ… æ•…éšœæ£€æµ‹è¾“å…¥éªŒè¯é€šè¿‡ï¼šfault_indicatoré•¿åº¦={len(fault_indicator)}")
        
        if CURRENT_DETECTION_MODE == "five_point" or CURRENT_DETECTION_MODE == "five_point_improved":
            fault_labels, detection_info = five_point_fault_detection(fault_indicator, threshold1, sample_id, threshold_config)
        else:
            fault_labels, detection_info = three_window_fault_detection(fault_indicator, threshold1, sample_id, threshold_config)
    
    # æ„å»ºç»“æœ
    sample_result = {
        'sample_id': sample_id,
        'model_type': 'TRANSFORMER',
        'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
        'df_data': df_data.values,
        'fai': fault_indicator if fault_indicator is not None else [],
        'T_squared': t_total if t_total is not None else [],
        'SPE': q_total if q_total is not None else [],
        'thresholds': {
            'threshold1': threshold1,
            'threshold2': threshold2, 
            'threshold3': threshold3
        },
        'fault_labels': fault_labels,
        'detection_info': detection_info,
        'performance_metrics': {
            'fai_mean': np.mean(fai),
            'fai_std': np.std(fai),
            'fai_max': np.max(fai),
            'fai_min': np.min(fai),
            'anomaly_count': np.sum(fai > threshold1),
            'anomaly_ratio': np.sum(fai > threshold1) / len(fai)
        }
    }
    
    return sample_result

#----------------------------------------ä¸»æµ‹è¯•æµç¨‹------------------------------
def main_test_process():
    """PNæ··åˆåé¦ˆTransformeræ¨¡å‹ä¸»è¦æµ‹è¯•æµç¨‹"""
    
    print("="*80)
    print("ğŸ¯ ç”µæ± æ•…éšœæ£€æµ‹ç³»ç»Ÿ - PNæ··åˆåé¦ˆTransformeræ¨¡å‹æµ‹è¯•")
    print("   åŸºäº Train_Transformer_PN_HybridFeedback_EN.py è®­ç»ƒçš„æ¨¡å‹")
    print("="*80)
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    test_results = {
        "TRANSFORMER": [],
        "metadata": {
            "test_samples": TEST_SAMPLES,
            "window_config": WINDOW_CONFIG,
            "detection_modes": DETECTION_MODES,
            "current_mode": CURRENT_DETECTION_MODE,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    # Transformerå•æ¨¡å‹æµ‹è¯•
    total_operations = len(ALL_TEST_SAMPLES)
    
    print(f"\nğŸš€ å¼€å§‹Transformeræ¨¡å‹æµ‹è¯•...")
    print(f"æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
    print(f"æ£€æµ‹æè¿°: {DETECTION_MODES[CURRENT_DETECTION_MODE]['description']}")
    print(f"æ€»å…±éœ€è¦å¤„ç†: {total_operations} ä¸ªæ ·æœ¬")
    
    with tqdm(total=total_operations, desc="Transformeræµ‹è¯•è¿›åº¦",
              bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}]') as pbar:
        
        print(f"\n{'='*20} æµ‹è¯• Transformer æ¨¡å‹ {'='*20}")
        
        # åŠ è½½æ¨¡å‹
        pbar.set_description(f"åŠ è½½Transformeræ¨¡å‹")
        models = load_models()
        print(f"âœ… Transformer æ¨¡å‹åŠ è½½å®Œæˆ")
        
        for sample_id in ALL_TEST_SAMPLES:
            pbar.set_description(f"Transformer-æ ·æœ¬{sample_id}")
            
            try:
                # å¤„ç†å•ä¸ªæ ·æœ¬
                sample_result = process_single_sample(sample_id, models, WINDOW_CONFIG)
                test_results["TRANSFORMER"].append(sample_result)
                
                # è¾“å‡ºç®€è¦ç»“æœ
                metrics = sample_result.get('performance_metrics', {})
                detection_info = sample_result.get('detection_info', {})
                
                # 5ç‚¹æ£€æµ‹æ¨¡å¼ - å®‰å…¨è·å–æ£€æµ‹ç»Ÿè®¡
                detection_stats = detection_info.get('detection_stats', {})
                detection_ratio = detection_stats.get('fault_ratio', 0.0)
                
                print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics.get('fai_mean', 0.0):.6f}, "
                      f"å¼‚å¸¸ç‡={metrics.get('anomaly_ratio', 0.0):.2%}, "
                      f"æ£€æµ‹ç‡={detection_ratio:.2%}")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                continue
            
            pbar.update(1)
            time.sleep(0.1)  # é¿å…è¿›åº¦æ¡æ›´æ–°è¿‡å¿«
    
    print(f"\nâœ… Transformeræµ‹è¯•å®Œæˆ!")
    print(f"   Transformer: æˆåŠŸå¤„ç† {len(test_results['TRANSFORMER'])} ä¸ªæ ·æœ¬")
    
    return test_results

#----------------------------------------æ€§èƒ½åˆ†æå‡½æ•°------------------------------
def calculate_performance_metrics(test_results):
    """è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ”¬ è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡...")
    
    performance_metrics = {}
    model_results = test_results["TRANSFORMER"]
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_true_labels = []
    all_fai_values = []
    all_fault_predictions = []
    
    for result in model_results:
        true_label = result.get('label', 0)
        fai_values = result.get('fai', [])
        fault_labels = result.get('fault_labels', [])
        thresholds = result.get('thresholds', {})
        threshold1 = thresholds.get('threshold1', 0.0)
        
        # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
        for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§BiLSTMçš„æ–¹å¼è®¾ç½®ç‚¹çº§åˆ«çœŸå®æ ‡ç­¾
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                point_true_label = 0  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
            else:  # æ•…éšœæ ·æœ¬
                point_true_label = fault_pred  # æ•…éšœæ ·æœ¬ä½¿ç”¨ä¸‰ç‚¹æ£€æµ‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾
            
            all_true_labels.append(point_true_label)  # ä½¿ç”¨ç‚¹çº§åˆ«æ ‡ç­¾
            all_fai_values.append(fai_val)
            
            # ğŸ”§ ç®€åŒ–é¢„æµ‹é€»è¾‘ï¼šåªåŸºäºFAIé˜ˆå€¼åˆ¤æ–­ï¼ˆä¸BiLSTMä¿æŒä¸€è‡´ï¼‰
            prediction = 1 if fai_val > threshold1 else 0
            all_fault_predictions.append(prediction)
    
    # è®¡ç®—ROCæŒ‡æ ‡
    all_true_labels = np.array(all_true_labels)
    all_fai_values = np.array(all_fai_values)
    all_fault_predictions = np.array(all_fault_predictions)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
    fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
    fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
    tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    tpr = recall  # True Positive Rate = Recall
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
    
    # æ ·æœ¬çº§ç»Ÿè®¡
    sample_metrics = {
        'total_samples': len(model_results),
        'normal_samples': len([r for r in model_results if r['label'] == 0]),
        'fault_samples': len([r for r in model_results if r['label'] == 1]),
        'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                 for r in model_results if r['label'] == 0]),
        'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                for r in model_results if r['label'] == 1]),
        'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                           for r in model_results if r['label'] == 0]),
        'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                          for r in model_results if r['label'] == 1])
    }
    
    performance_metrics["TRANSFORMER"] = {
        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
        'classification_metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1_score),
            'specificity': float(specificity),
            'tpr': float(tpr),
            'fpr': float(fpr)
        },
        'sample_metrics': sample_metrics,
        'roc_data': {
            'true_labels': all_true_labels.tolist(),
            'fai_values': all_fai_values.tolist(),
            'predictions': all_fault_predictions.tolist()
        }
    }
    
    return performance_metrics

# æ‰§è¡Œä¸»æµ‹è¯•æµç¨‹
test_results = main_test_process()

#----------------------------------------ROCæ›²çº¿å¯¹æ¯”------------------------------
def create_roc_analysis(test_results, performance_metrics, save_path):
    """ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ"""
    print("   ğŸ“ˆ ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"], constrained_layout=True)
    
    # === å­å›¾1: è¿ç»­é˜ˆå€¼ROCæ›²çº¿ ===
    ax1.set_title('(a) Transformer ROC Curve\n(Continuous Threshold Scan)')
    
    # ä½¿ç”¨Transformerç»“æœ
    model_results = test_results["TRANSFORMER"]
    if not model_results:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„Transformerç»“æœ")
        return
    
    # æ”¶é›†æ‰€æœ‰faiå€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè¿ç»­é˜ˆå€¼ROC
    all_fai = []
    all_labels = []
    all_fault_labels = []
    
    for result in model_results:
        all_fai.extend(result['fai'])
        all_labels.extend([result['label']] * len(result['fai']))
        all_fault_labels.extend(result['fault_labels'])
    
    all_fai = np.array(all_fai)
    all_labels = np.array(all_labels)
    all_fault_labels = np.array(all_fault_labels)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(all_fai) == 0:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„FAIæ•°æ®ï¼Œè·³è¿‡ROCæ›²çº¿ç”Ÿæˆ")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„ROCå›¾
        ax1.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', 
                transform=ax1.transAxes, fontsize=12)
        ax1.set_title('(a) Transformer ROC Curve\n(æ— æ•°æ®)')
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.grid(True, alpha=0.3)
        return
    
    # ğŸ”§ æ·»åŠ æ•°æ®ç»Ÿè®¡åˆ†æ
    print(f"   ğŸ“Š ROCæ›²çº¿æ•°æ®ç»Ÿè®¡:")
    print(f"      æ€»æ•°æ®ç‚¹: {len(all_fai)}")
    print(f"      æ­£å¸¸æ ·æœ¬ç‚¹: {np.sum(all_labels == 0)}")
    print(f"      æ•…éšœæ ·æœ¬ç‚¹: {np.sum(all_labels == 1)}")
    print(f"      æ•…éšœæ ‡è®°ç‚¹: {np.sum(all_fault_labels == 1)}")
    print(f"      FAIèŒƒå›´: [{np.min(all_fai):.6f}, {np.max(all_fai):.6f}]")
    
    # ğŸ”§ æ”¹è¿›é˜ˆå€¼æ‰«æç­–ç•¥ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
    # æ–¹æ³•1ï¼šä½¿ç”¨åˆ†ä½æ•°èŒƒå›´é¿å…æç«¯å€¼å½±å“
    fai_p1 = np.percentile(all_fai, 1)   # 1%åˆ†ä½æ•°
    fai_p99 = np.percentile(all_fai, 99) # 99%åˆ†ä½æ•°
    fai_median = np.median(all_fai)
    fai_mean = np.mean(all_fai)
    
    print(f"   ğŸ“Š FAIåˆ†å¸ƒåˆ†æ:")
    print(f"      1%åˆ†ä½æ•°: {fai_p1:.6f}")
    print(f"      50%åˆ†ä½æ•°(ä¸­ä½æ•°): {fai_median:.6f}")
    print(f"      å‡å€¼: {fai_mean:.6f}")
    print(f"      99%åˆ†ä½æ•°: {fai_p99:.6f}")
    print(f"      æœ€å¤§å€¼: {np.max(all_fai):.6f}")
    
    # ä½¿ç”¨æ›´æ™ºèƒ½çš„é˜ˆå€¼èŒƒå›´ï¼šä»æœ€å°å€¼åˆ°99%åˆ†ä½æ•°ï¼Œé¿å…æç«¯å€¼
    threshold_min = np.min(all_fai)
    threshold_max = fai_p99  # ä½¿ç”¨99%åˆ†ä½æ•°è€Œä¸æ˜¯æœ€å¤§å€¼
    
    # ğŸ”§ ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ï¼šçº¿æ€§+å¯¹æ•°å°ºåº¦
    if threshold_max > threshold_min * 10:  # å¦‚æœèŒƒå›´è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°å°ºåº¦
        # æ–¹æ³•1ï¼šå¯¹æ•°å°ºåº¦æ‰«æï¼ˆå¤„ç†å¤§èŒƒå›´çš„æ•°æ®ï¼‰
        log_min = np.log10(max(threshold_min, 1e-10))  # é¿å…log(0)
        log_max = np.log10(threshold_max)
        log_thresholds = np.logspace(log_min, log_max, 50)
        
        # æ–¹æ³•2ï¼šçº¿æ€§æ‰«æï¼ˆå¤„ç†å°èŒƒå›´çš„æ•°æ®ï¼‰
        linear_thresholds = np.linspace(threshold_min, min(threshold_max, fai_median*3), 50)
        
        # åˆå¹¶å¹¶å»é‡
        thresholds = np.unique(np.concatenate([linear_thresholds, log_thresholds]))
        thresholds = np.sort(thresholds)
        
        print(f"   ğŸ“Š ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ (çº¿æ€§+å¯¹æ•°): {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    else:
        # èŒƒå›´è¾ƒå°æ—¶ä½¿ç”¨çº¿æ€§æ‰«æ
        thresholds = np.linspace(threshold_min, threshold_max, 100)
        print(f"   ğŸ“Š ä½¿ç”¨çº¿æ€§æ‰«æç­–ç•¥: {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    
    print(f"   ğŸ“Š é˜ˆå€¼æ‰«æèŒƒå›´: [{threshold_min:.6f}, {threshold_max:.6f}]")
    
    tpr_list = []
    fpr_list = []
    
    for threshold in thresholds:
        tp = fp = tn = fn = 0
        
        for i, (fai_val, true_label, fault_pred) in enumerate(zip(all_fai, all_labels, all_fault_labels)):
            # ğŸ”§ ä¿®å¤ROCæ›²çº¿é€»è¾‘ï¼šä½¿ç”¨ç‚¹çº§åˆ«çš„çœŸå®æ ‡ç­¾
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
                point_true_label = 0
            else:  # æ•…éšœæ ·æœ¬ï¼šä½¿ç”¨æ•…éšœæ£€æµ‹ç®—æ³•çš„ç»“æœä½œä¸ºç‚¹çº§åˆ«çœŸå®æ ‡ç­¾
                point_true_label = fault_pred
            
            # é¢„æµ‹æ ‡ç­¾ï¼šç®€å•åŸºäºFAIé˜ˆå€¼
            predicted_label = 1 if fai_val > threshold else 0
            
            # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
            if point_true_label == 0 and predicted_label == 0:
                tn += 1
            elif point_true_label == 0 and predicted_label == 1:
                fp += 1
            elif point_true_label == 1 and predicted_label == 0:
                fn += 1
            elif point_true_label == 1 and predicted_label == 1:
                tp += 1
        
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        tpr_list.append(tpr)
        fpr_list.append(fpr)
    
    # è®¡ç®—AUC - éœ€è¦ç¡®ä¿fpr_listæ˜¯å•è°ƒé€’å¢çš„
    from sklearn.metrics import auc
    
    # ğŸ”§ ä¿®å¤AUCè®¡ç®—ï¼šç¡®ä¿FPRå•è°ƒé€’å¢
    combined = list(zip(fpr_list, tpr_list))
    combined.sort(key=lambda x: x[0])  # æŒ‰FPRæ’åº
    fpr_sorted, tpr_sorted = zip(*combined)
    
    auc_score = auc(fpr_sorted, tpr_sorted)
    
    print(f"   ğŸ“Š ROCæ›²çº¿è®¡ç®—ç»“æœ:")
    print(f"      é˜ˆå€¼æ•°é‡: {len(thresholds)}")
    print(f"      FPRèŒƒå›´: [{min(fpr_sorted):.3f}, {max(fpr_sorted):.3f}]")
    print(f"      TPRèŒƒå›´: [{min(tpr_sorted):.3f}, {max(tpr_sorted):.3f}]")
    print(f"      AUCå¾—åˆ†: {auc_score:.6f}")
    
    # ç»˜åˆ¶ROCæ›²çº¿ - ä½¿ç”¨æ’åºåçš„æ•°æ®
    ax1.plot(fpr_sorted, tpr_sorted, color='blue', linewidth=2,
            label=f'Transformer (AUC={auc_score:.3f})')
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: å›ºå®šé˜ˆå€¼å·¥ä½œç‚¹ ===
    ax2.set_title('(b) Working Point\n(Three-Level Alarm Threshold)')
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
    ax2.scatter(metrics['fpr'], metrics['tpr'], 
               s=200, color='blue', 
               label=f'Transformer\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
               marker='o', edgecolors='black', linewidth=2)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: æ€§èƒ½æŒ‡æ ‡å±•ç¤º ===
    ax3.set_title('(c) Transformer Classification Metrics')
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    metric_mapping = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1_score', 'Specificity': 'specificity'}
    transformer_values = [performance_metrics["TRANSFORMER"]['classification_metrics'][metric_mapping[m]] 
                         for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.6
    
    bars = ax3.bar(x, transformer_values, width, color='blue', alpha=0.7)
    ax3.set_xlabel('Metrics')
    ax3.set_ylabel('Score')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics_names, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_values):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å±•ç¤º ===
    ax4.set_title('(d) Transformer Sample-Level Performance')
    
    sample_metrics = ['Avg Ï†(Normal)', 'Avg Ï†(Fault)', 'Anomaly Rate(Normal)', 'Anomaly Rate(Fault)']
    transformer_sample_values = [
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_fault'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    
    x = np.arange(len(sample_metrics))
    bars = ax4.bar(x, transformer_sample_values, width, color='blue', alpha=0.7)
    
    ax4.set_xlabel('Sample Metrics')
    ax4.set_ylabel('Value')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_metrics, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_sample_values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformer ROCåˆ†æå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºå›¾------------------------------
def create_fault_detection_timeline(test_results, save_path):
    """ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    print("   ğŸ“Š ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
    
    fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True, constrained_layout=True)
    
    # æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„ç»“æœ
    sample_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if sample_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        sample_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if sample_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = sample_result.get('fai', [])
    fault_labels = sample_result.get('fault_labels', [])
    thresholds = sample_result.get('thresholds', {})
    time_axis = np.arange(len(fai_values))
    
    # å­å›¾1: ç»¼åˆè¯Šæ–­æŒ‡æ ‡æ—¶åº
    ax1 = axes[0]
    ax1.plot(time_axis, fai_values, color='blue', linewidth=1, alpha=0.8,
           label='Transformer FAI')
    ax1.axhline(y=thresholds['threshold1'], color='orange', linestyle='--', alpha=0.7,
              label='Level 1 Threshold')
    ax1.axhline(y=thresholds['threshold2'], color='red', linestyle='--', alpha=0.7,
              label='Level 2 Threshold')
    ax1.axhline(y=thresholds['threshold3'], color='darkred', linestyle='--', alpha=0.7,
              label='Level 3 Threshold')
    
    ax1.set_ylabel('Transformer\nComprehensive Diagnostic Index Ï†')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_title(f'Transformer - Sample {fault_sample_id} (Fault Sample)')
    
    # å­å›¾2: æ•…éšœæ£€æµ‹ç»“æœ
    ax2 = axes[1]
    
    # å°†æ•…éšœæ ‡ç­¾è½¬æ¢ä¸ºå¯è§†åŒ–åŒºåŸŸ
    fault_regions = np.where(fault_labels == 1, 0.8, 0)
    ax2.fill_between(time_axis, fault_regions, 
                    alpha=0.6, color='blue',
                    label='Transformer Fault Detection')
    
    ax2.set_ylabel('Fault Detection Result')
    ax2.set_ylim(0, 1)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_title('Transformer Fault Detection Result')
    
    # å­å›¾3: ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹
    ax3 = axes[2]
    detection_info = sample_result['detection_info']
    
    ax3.plot(time_axis, fai_values, 'b-', alpha=0.5, label='Ï†æŒ‡æ ‡å€¼')
    
    # ğŸ”§ ä¿®å¤ï¼šä¸‰ç‚¹æ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–
    # æ ‡è®°è§¦å‘ç‚¹ï¼ˆå¯¹åº”åŸæ¥çš„å€™é€‰ç‚¹ï¼‰
    if detection_info.get('trigger_points'):
        ax3.scatter(detection_info['trigger_points'], 
                   [fai_values[i] for i in detection_info['trigger_points']],
                   color='orange', s=30, label='è§¦å‘ç‚¹', alpha=0.8)
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    marked_regions = detection_info.get('marked_regions', [])
    for i, region in enumerate(marked_regions):
        start, end = region['range']
        label = 'Fault Region' if i == 0 else ""
        ax3.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax3.set_ylabel('Three-Point Detection\nProcess')
    ax3.set_xlabel('Time Step')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_title('Three-Point Detection Process (Transformer)')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformeræ—¶åºå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾------------------------------
def create_performance_radar(performance_metrics, save_path):
    """ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    print("   ğŸ•¸ï¸ ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'Accuracy': 'accuracy',
        'Precision': 'precision', 
        'Recall': 'recall',
        'F1-Score': 'f1_score',
        'Specificity': 'specificity',
        'Early Warning': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'False Alarm Control': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'Detection Stability': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    # æ•°æ®é¢„å¤„ç†ï¼šFPRéœ€è¦è½¬æ¢ä¸ºæ§åˆ¶èƒ½åŠ› (1-FPR)
    transformer_values = []
    
    for metric_name, metric_key in radar_metrics.items():
        transformer_val = performance_metrics["TRANSFORMER"]['classification_metrics'][metric_key]
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
        if metric_name == 'False Alarm Control':
            transformer_val = 1 - transformer_val
            
        transformer_values.append(transformer_val)
    
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    transformer_values += transformer_values[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize_medium"], subplot_kw=dict(projection='polar'), constrained_layout=True)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles, transformer_values, 'o-', linewidth=2, label='Transformer', color='blue')
    ax.fill(angles, transformer_values, alpha=0.25, color='blue')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(list(radar_metrics.keys()))
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('Transformer Performance Metrics Radar Chart', 
              pad=20, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # æ·»åŠ æ€§èƒ½æ€»ç»“
    transformer_avg = np.mean(transformer_values[:-1])
    
    plt.figtext(0.02, 0.02, f'Transformer Overall Performance: {transformer_avg:.3f}', 
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformeré›·è¾¾å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–------------------------------
def create_three_window_visualization(test_results, save_path):
    """ç”ŸæˆTransformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–"""
    print("   ğŸ” ç”ŸæˆTransformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'
    
    fig = plt.figure(figsize=(16, 10), constrained_layout=True)
    
    # ä½¿ç”¨GridSpecè¿›è¡Œå¤æ‚å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1])
    
    # === ä¸»å›¾ï¼šä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹æ—¶åºå›¾ ===
    ax_main = fig.add_subplot(gs[0, :])
    
    # é€‰æ‹©Transformerç»“æœè¿›è¡Œå¯è§†åŒ–
    transformer_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if transformer_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        transformer_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if transformer_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = transformer_result.get('fai', [])
    detection_info = transformer_result.get('detection_info', {})
    thresholds = transformer_result.get('thresholds', {})
    threshold1 = thresholds.get('threshold1', 0.0)
    
    time_axis = np.arange(len(fai_values))
    
    # ç»˜åˆ¶FAIæ—¶åº
    ax_main.plot(time_axis, fai_values, 'b-', linewidth=1.5, alpha=0.8, label='Comprehensive Diagnostic Index Ï†(FAI)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.7, label='Level 1 Threshold')
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        candidate_points = detection_info['candidate_points']
        ax_main.scatter(candidate_points, [fai_values[i] for i in candidate_points],
                       color='orange', s=40, alpha=0.8, label=f'Detection: {len(candidate_points)} Candidate Points',
                       marker='o', zorder=5)
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax_main.scatter(verified_indices, [fai_values[i] for i in verified_indices],
                       color='red', s=60, alpha=0.9, label=f'éªŒè¯: {len(verified_indices)} ä¸ªç¡®è®¤ç‚¹',
                       marker='^', zorder=6)
        
        # æ˜¾ç¤ºéªŒè¯çª—å£èŒƒå›´
        for v_point in detection_info['verified_points']:
            verify_start, verify_end = v_point['verify_range']
            ax_main.axvspan(verify_start, verify_end, alpha=0.1, color='yellow')
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ•…éšœåŒºåŸŸ
    fault_regions_plotted = set()  # é¿å…é‡å¤ç»˜åˆ¶å›¾ä¾‹
    for i, region in enumerate(detection_info['marked_regions']):
        start, end = region['range']
        label = 'Marked: Fault Region' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('Time Step')
    ax_main.set_ylabel('Comprehensive Diagnostic Index Ï†')
    ax_main.set_title(f'Transformer Three-Point Fault Detection Process - Sample {fault_sample_id}', 
                     fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left')
    ax_main.grid(True, alpha=0.3)
    
    # === å­å›¾1ï¼šæ£€æµ‹çª—å£ç»Ÿè®¡ ===
    ax1 = fig.add_subplot(gs[1, 0])
    
    # ğŸ”§ ä¿®å¤ï¼šä¸‰ç‚¹æ£€æµ‹æ¨¡å¼æ²¡æœ‰window_statsï¼Œä½¿ç”¨detection_stats
    detection_stats = detection_info.get('detection_stats', {})
    detection_data = [
        detection_stats.get('total_trigger_points', 0),    # è§¦å‘ç‚¹æ•°ï¼ˆå¯¹åº”å€™é€‰ç‚¹ï¼‰
        detection_stats.get('total_marked_regions', 0),    # æ ‡è®°åŒºåŸŸæ•°ï¼ˆå¯¹åº”éªŒè¯ç‚¹ï¼‰
        detection_stats.get('total_fault_points', 0)       # æ•…éšœç‚¹æ•°
    ]
    detection_labels = ['Candidate Points', 'Verified Points', 'Fault Points']
    colors1 = ['orange', 'red', 'darkred']
    
    bars1 = ax1.bar(detection_labels, detection_data, color=colors1, alpha=0.7)
    ax1.set_title('Detection Statistics')
    ax1.set_ylabel('Count')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, detection_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(value), ha='center', va='bottom')
    
    # === å­å›¾2ï¼šThree-Point Detection Parameters ===
    ax2 = fig.add_subplot(gs[1, 1])
    
    # æ˜¾ç¤ºä¸‰ç‚¹æ£€æµ‹çš„é˜ˆå€¼å‚æ•°
    thresholds = transformer_result.get('thresholds', {})
    threshold_names = ['3Ïƒ Threshold', '4.5Ïƒ Threshold', '6Ïƒ Threshold']
    threshold_values = [
        thresholds.get('threshold1', 0),
        thresholds.get('threshold2', 0),
        thresholds.get('threshold3', 0)
    ]
    colors2 = ['lightblue', 'orange', 'red']
    
    bars2 = ax2.bar(threshold_names, threshold_values, color=colors2, alpha=0.7)
    ax2.set_title('Detection Thresholds\n(Three-Level Hierarchy)')
    ax2.set_ylabel('Threshold Value')
    ax2.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars2, threshold_values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.2f}', ha='center', va='bottom')
    
    # === å­å›¾3ï¼šè§¦å‘çº§åˆ«åˆ†å¸ƒ ===
    ax3 = fig.add_subplot(gs[1, 2])
    
    # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
    detection_stats = detection_info.get('detection_stats', {})
    level_stats = detection_stats.get('level_statistics', {})
    
    if level_stats:
        levels = ['Level 1', 'Level 2', 'Level 3']
        trigger_counts = [
            level_stats.get('level_1_triggers', 0),
            level_stats.get('level_2_triggers', 0), 
            level_stats.get('level_3_triggers', 0)
        ]
        colors = ['lightblue', 'orange', 'red']
        
        bars3 = ax3.bar(levels, trigger_counts, color=colors, alpha=0.7)
        ax3.set_title('Trigger Level Distribution')
        ax3.set_xlabel('Detection Level')
        ax3.set_ylabel('Trigger Count')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars3, trigger_counts):
            if count > 0:
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(count), ha='center', va='bottom')
    else:
        ax3.text(0.5, 0.5, 'No Trigger Data', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('Trigger Level Distribution')
    
    # === å­å›¾4ï¼šTransformer Performance ===
    ax4 = fig.add_subplot(gs[1, 3])
    
    sample_result = next((r for r in test_results['TRANSFORMER'] if r.get('sample_id') == fault_sample_id), None)
    if sample_result is None:
        sample_result = test_results['TRANSFORMER'][0] if test_results['TRANSFORMER'] else None
    
    if sample_result is None:
        fault_ratio = 0.0
    else:
        detection_info = sample_result.get('detection_info', {})
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨detection_statsæ›¿ä»£window_stats
    detection_stats = detection_info.get('detection_stats', {})
    fault_ratio = detection_stats.get('fault_ratio', 0.0)
    
    bars4 = ax4.bar(['Transformer'], [fault_ratio], color='blue', alpha=0.7)
    ax4.set_title('Transformer\n(Fault Detection Ratio)')
    ax4.set_ylabel('Fault Ratio')
    
    for bar, value in zip(bars4, [fault_ratio]):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === åº•éƒ¨ï¼šè¿‡ç¨‹è¯´æ˜ ===
    process_text = """
    Transformer Three-Point Detection Process:
    
    1. Level 3 (6Ïƒ): Center point exceeds 6Ïƒ threshold, no neighborhood requirement, directly mark 3 points
    2. Level 2 (4.5Ïƒ): Center point exceeds 4.5Ïƒ + at least 1 neighbor exceeds 3Ïƒ, mark 3 points
    3. Level 1 (3Ïƒ): Center point exceeds 3Ïƒ + at least 1 neighbor exceeds 2Ïƒ, mark 3 points
    
    Advantages: Hierarchical detection + neighborhood verification, effective noise reduction while maintaining sensitivity
    """
    
    fig.text(0.02, 0.02, process_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»“æœä¿å­˜å‡½æ•°------------------------------
def save_test_results(test_results, performance_metrics):
    """ä¿å­˜Transformeræµ‹è¯•ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜Transformeræµ‹è¯•ç»“æœ...")
    
    # åˆ›å»ºç»“æœç›®å½• - PNæ¨¡å‹æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = f"/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/test_results_pn_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(f"{result_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{result_dir}/detailed_results", exist_ok=True)
    
    # 1. ä¿å­˜æ€§èƒ½æŒ‡æ ‡JSON
    performance_file = f"{result_dir}/transformer_performance_metrics.json"
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræ€§èƒ½æŒ‡æ ‡ä¿å­˜è‡³: {performance_file}")
    
    # 2. ä¿å­˜è¯¦ç»†ç»“æœ
    detail_file = f"{result_dir}/detailed_results/transformer_detailed_results.pkl"
    with open(detail_file, 'wb') as f:
        pickle.dump(test_results["TRANSFORMER"], f)
    print(f"   âœ… Transformerè¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
    
    # 3. ä¿å­˜å…ƒæ•°æ®
    metadata_file = f"{result_dir}/detailed_results/transformer_test_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(test_results['metadata'], f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræµ‹è¯•å…ƒæ•°æ®ä¿å­˜è‡³: {metadata_file}")
    
    # 4. åˆ›å»ºExcelæ€»ç»“æŠ¥å‘Š
    summary_file = f"{result_dir}/detailed_results/transformer_summary.xlsx"
    
    with pd.ExcelWriter(summary_file) as writer:
        # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
        if "TRANSFORMER" not in performance_metrics:
            print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
            return result_dir
        
        metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
        confusion = performance_metrics["TRANSFORMER"]['confusion_matrix']
        sample_metrics = performance_metrics["TRANSFORMER"]['sample_metrics']
        
        performance_data = [{
            'Model': 'TRANSFORMER',
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'Specificity': metrics['specificity'],
            'TPR': metrics['tpr'],
            'FPR': metrics['fpr'],
            'TP': confusion['TP'],
            'FP': confusion['FP'],
            'TN': confusion['TN'],
            'FN': confusion['FN'],
            'Avg_FAI_Normal': sample_metrics['avg_fai_normal'],
            'Avg_FAI_Fault': sample_metrics['avg_fai_fault']
        }]
        
        performance_df = pd.DataFrame(performance_data)
        performance_df.to_excel(writer, sheet_name='Transformer_Performance', index=False)
        
        # æ ·æœ¬è¯¦æƒ…è¡¨
        sample_details = []
        for result in test_results["TRANSFORMER"]:
            # å®‰å…¨è·å–detection_infoå’Œdetection_stats
            detection_info = result.get('detection_info', {})
            detection_stats = detection_info.get('detection_stats', {})
            performance_metrics = result.get('performance_metrics', {})
            
            sample_details.append({
                'Sample_ID': result.get('sample_id', 'Unknown'),
                'True_Label': 'Fault' if result.get('label', 0) == 1 else 'Normal',
                'FAI_Mean': performance_metrics.get('fai_mean', 0.0),
                'FAI_Std': performance_metrics.get('fai_std', 0.0),
                'FAI_Max': performance_metrics.get('fai_max', 0.0),
                'Anomaly_Ratio': performance_metrics.get('anomaly_ratio', 0.0),
                'Fault_Detection_Ratio': detection_stats.get('fault_ratio', 0.0),
                'Candidates_Found': detection_stats.get('total_trigger_points', 0),
                'Verified_Points': detection_stats.get('total_marked_regions', 0)
            })
        
        sample_df = pd.DataFrame(sample_details)
        sample_df.to_excel(writer, sheet_name='Sample_Details', index=False)
    
    print(f"   âœ… Transformer Excelæ€»ç»“æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")
    
    return result_dir

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
performance_metrics = calculate_performance_metrics(test_results)

# ä¿å­˜æµ‹è¯•ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
result_dir = save_test_results(test_results, performance_metrics)

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
print("\nğŸ¨ ç”ŸæˆTransformerå¯è§†åŒ–åˆ†æ...")

# ç”ŸæˆROCåˆ†æå›¾
create_roc_analysis(test_results, performance_metrics, f"{result_dir}/visualizations/transformer_roc_analysis.png")

# ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾
create_fault_detection_timeline(test_results, f"{result_dir}/visualizations/transformer_fault_detection_timeline.png")

# ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
create_performance_radar(performance_metrics, f"{result_dir}/visualizations/transformer_performance_radar.png")

# ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å›¾
create_three_window_visualization(test_results, f"{result_dir}/visualizations/transformer_three_window_process.png")

#----------------------------------------æœ€ç»ˆæ€»ç»“------------------------------
print("\n" + "="*80)
print("ğŸ‰ Transformeræ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(ALL_TEST_SAMPLES)} ä¸ª (æ­£å¸¸: {len(TEST_SAMPLES['normal'])}, æ•…éšœ: {len(TEST_SAMPLES['fault'])})")
print(f"   â€¢ æ¨¡å‹ç±»å‹: Transformer")
print(f"   â€¢ æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")

print(f"\nğŸ”¬ Transformeræ€§èƒ½:")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   â€¢ çª—å£é…ç½®: æ£€æµ‹({WINDOW_CONFIG['detection_window']}) â†’ éªŒè¯({WINDOW_CONFIG['verification_window']}) â†’ æ ‡è®°({WINDOW_CONFIG['marking_window']})")
else:
    print(f"   â€¢ 5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")
print(f"   TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   â€¢ ç»“æœç›®å½•: {result_dir}")
print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {result_dir}/visualizations")
print(f"     - ROCåˆ†æå›¾: transformer_roc_analysis.png")
print(f"     - æ•…éšœæ£€æµ‹æ—¶åºå›¾: transformer_fault_detection_timeline.png") 
print(f"     - æ€§èƒ½é›·è¾¾å›¾: transformer_performance_radar.png")
print(f"     - ä¸‰çª—å£è¿‡ç¨‹å›¾: transformer_three_window_process.png")
print(f"   â€¢ æ€§èƒ½æŒ‡æ ‡: transformer_performance_metrics.json")
print(f"   â€¢ ExcelæŠ¥å‘Š: transformer_summary.xlsx")

# ç»¼åˆæ€§èƒ½è¯„ä¼°
transformer_score = np.mean(list(performance_metrics["TRANSFORMER"]['classification_metrics'].values()))

print(f"\nğŸ† Transformerç»¼åˆæ€§èƒ½è¯„ä¼°:")
print(f"   ç»¼åˆå¾—åˆ†: {transformer_score:.3f}")

print("\n" + "="*80)
print("PNæ··åˆåé¦ˆTransformeræ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("åŸºäº Train_Transformer_PN_HybridFeedback_EN.py è®­ç»ƒçš„æ¨¡å‹")
print("è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
print("="*80)