# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from Comprehensive_calculation import Comprehensive_calculation

# æ–°å¢å¯¼å…¥
from tqdm import tqdm
import json
import time
from datetime import datetime
from sklearn.metrics import roc_curve, auc, confusion_matrix
import glob

# æ·»åŠ æ¨¡å‹åŠ è½½è¾…åŠ©å‡½æ•°
def remove_module_prefix(state_dict):
    """ç§»é™¤state_dictä¸­çš„module.å‰ç¼€"""
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('module.'):
            new_key = key[7:]  # ç§»é™¤'module.'å‰ç¼€
        else:
            new_key = key
        new_state_dict[new_key] = value
    return new_state_dict

def safe_get_nested(dictionary, keys, default=None):
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸çš„å€¼"""
    try:
        for key in keys:
            dictionary = dictionary[key]
        return dictionary
    except (KeyError, TypeError, IndexError):
        return default

def safe_load_model(model, model_path, model_name):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†DataParallelå‰ç¼€é—®é¢˜"""
    try:
        print(f"   æ­£åœ¨åŠ è½½{model_name}æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"   âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        state_dict = torch.load(model_path, map_location=device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤moduleå‰ç¼€
        has_module_prefix = any(key.startswith('module.') for key in state_dict.keys())
        if has_module_prefix:
            print(f"   æ£€æµ‹åˆ°DataParallelå‰ç¼€ï¼Œæ­£åœ¨ç§»é™¤...")
            state_dict = remove_module_prefix(state_dict)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„åŒ¹é…
        model_state_dict = model.state_dict()
        missing_keys = []
        unexpected_keys = []
        
        for key in model_state_dict.keys():
            if key not in state_dict:
                missing_keys.append(key)
        
        for key in state_dict.keys():
            if key not in model_state_dict:
                unexpected_keys.append(key)
        
        if missing_keys:
            print(f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys[:5]}..." if len(missing_keys) > 5 else f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys}")
        
        if unexpected_keys:
            print(f"   âš ï¸  å¤šä½™é”®: {unexpected_keys[:5]}..." if len(unexpected_keys) > 5 else f"   âš ï¸  å¤šä½™é”®: {unexpected_keys}")
        
        # å°è¯•åŠ è½½
        model.load_state_dict(state_dict, strict=False)
        print(f"   âœ… {model_name}æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"   âŒ {model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# GPUé…ç½®æ£€æŸ¥
print("ğŸ–¥ï¸ GPUé…ç½®æ£€æŸ¥:")
print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"   å½“å‰è®¾å¤‡: {device}")

if torch.cuda.is_available():
    print("   GPUè¯¦ç»†ä¿¡æ¯:")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"     GPU {i}: {props.name}")
        print(f"       æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
        print(f"       è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print("   âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
import matplotlib.font_manager as fm
import platform

def setup_chinese_fonts():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    system = platform.system()
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©å­—ä½“
    if system == "Windows":
        chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi']
    elif system == "Linux":
        chinese_fonts = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'Source Han Sans CN', 'DejaVu Sans']
    elif system == "Darwin":  # macOS
        chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
    else:
        chinese_fonts = ['DejaVu Sans', 'Arial Unicode MS']
    
    # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_font = None
    for font in chinese_fonts:
        try:
            # æ£€æŸ¥å­—ä½“æ˜¯å¦å­˜åœ¨
            font_path = fm.findfont(font)
            if font_path != fm.rcParams['font.sans-serif'][0]:
                available_font = font
                break
        except:
            continue
    
    if available_font:
        plt.rcParams['font.sans-serif'] = [available_font] + plt.rcParams['font.sans-serif']
        print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {available_font}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®")
        # ä½¿ç”¨æ›´é€šç”¨çš„å­—ä½“é…ç½®
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
    
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# æ‰§è¡Œå­—ä½“é…ç½®
setup_chinese_fonts()

# æ¸…ç†å­—ä½“ç¼“å­˜å¹¶å¼ºåˆ¶åˆ·æ–°
try:
    fm._rebuild()
    print("âœ… å­—ä½“ç¼“å­˜å·²æ¸…ç†å¹¶é‡å»º")
except:
    print("âš ï¸ å­—ä½“ç¼“å­˜æ¸…ç†å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰é…ç½®")

#----------------------------------------æµ‹è¯•é…ç½®------------------------------
print("="*60)
print("ğŸ”¬ ç”µæ± æ•…éšœè¯Šæ–­ç³»ç»Ÿ - Transformeræ¨¡å‹æµ‹è¯•ï¼ˆæ··åˆåé¦ˆç‰ˆæœ¬ï¼‰")
print("="*60)

TEST_MODE = "TRANSFORMER_ONLY"  # å›ºå®šä¸ºTransformerå•æ¨¡å‹æµ‹è¯•

# æµ‹è¯•æ•°æ®é›†é…ç½® (æ ¹æ®Labels.xlsåŠ¨æ€åŠ è½½)
def load_test_samples():
    """ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬"""
    try:
        import pandas as pd
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–æµ‹è¯•æ ·æœ¬
        all_samples = df['Num'].tolist()
        all_labels = df['Label'].tolist()
        
        # æŒ‡å®šæµ‹è¯•æ ·æœ¬ï¼šæ­£å¸¸æ ·æœ¬10,11 å’Œæ•…éšœæ ·æœ¬335,336
        test_normal_samples = ['10', '11']  # æ­£å¸¸æ ·æœ¬
        test_fault_samples = ['335', '336']  # æ•…éšœæ ·æœ¬
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬:")
        print(f"   æµ‹è¯•æ­£å¸¸æ ·æœ¬: {test_normal_samples}")
        print(f"   æµ‹è¯•æ•…éšœæ ·æœ¬: {test_fault_samples}")
        
        return {
            'normal': test_normal_samples,
            'fault': test_fault_samples
        }
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æµ‹è¯•æ ·æœ¬")
        return {
            'normal': ['10', '11'],
            'fault': ['335', '336']
        }

TEST_SAMPLES = load_test_samples()
ALL_TEST_SAMPLES = TEST_SAMPLES['normal'] + TEST_SAMPLES['fault']

# æ¨¡å‹è·¯å¾„é…ç½® (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
MODEL_PATHS = {
    "TRANSFORMER": {
        "transformer_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/transformer_model_hybrid_feedback.pth",
        "net_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/net_model_hybrid_feedback.pth", 
        "netx_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/netx_model_hybrid_feedback.pth"
    }
}

# æ£€æµ‹æ¨¡å¼é…ç½®
DETECTION_MODES = {
    "three_window": {
        "name": "ä¸‰çª—å£æ£€æµ‹æ¨¡å¼",
        "description": "åŸºäºFAIçš„ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶ï¼ˆæ£€æµ‹->éªŒè¯->æ ‡è®°ï¼‰",
        "function": "three_window_fault_detection"
    },
    "five_point": {
        "name": "5ç‚¹æ£€æµ‹æ¨¡å¼ï¼ˆåŸç‰ˆï¼‰", 
        "description": "å¯¹äºæ•…éšœæ ·æœ¬ï¼Œå¦‚æœæŸç‚¹é«˜äºé˜ˆå€¼ä¸”å‰åç›¸é‚»ç‚¹ä¹Ÿé«˜äºé˜ˆå€¼ï¼Œåˆ™æ ‡è®°è¯¥ç‚¹åŠå‰å2ä¸ªç‚¹ï¼ˆå…±5ä¸ªç‚¹ï¼‰",
        "function": "five_point_fault_detection"
    },
    "five_point_improved": {
        "name": "5ç‚¹æ£€æµ‹æ¨¡å¼ï¼ˆæ”¹è¿›ç‰ˆï¼‰",
        "description": "æ”¹è¿›çš„5ç‚¹æ£€æµ‹ï¼šä¸¥æ ¼çš„è§¦å‘æ¡ä»¶ + åˆ†çº§æ ‡è®°èŒƒå›´ + æœ‰æ•ˆé™å™ªæœºåˆ¶",
        "function": "five_point_fault_detection"
    }
}

# å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ¨¡å¼
CURRENT_DETECTION_MODE = "five_point_improved"  # ä½¿ç”¨æ”¹è¿›çš„5ç‚¹æ£€æµ‹æ¨¡å¼

# åŸºäºFAIçš„ä¸‰çª—å£æ£€æµ‹é…ç½® (ä¸BiLSTMæµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´)
WINDOW_CONFIG = {
    "detection_window": 25,      # æ£€æµ‹çª—å£ï¼š25ä¸ªé‡‡æ ·ç‚¹ (12.5åˆ†é’Ÿ)
    "verification_window": 15,   # éªŒè¯çª—å£ï¼š15ä¸ªé‡‡æ ·ç‚¹ (7.5åˆ†é’Ÿ)
    "marking_window": 10,        # æ ‡è®°çª—å£ï¼š10ä¸ªé‡‡æ ·ç‚¹ (5åˆ†é’Ÿ)
    "verification_threshold": 0.6 # éªŒè¯çª—å£å†…FAIå¼‚å¸¸æ¯”ä¾‹é˜ˆå€¼ (60%)
}

# é«˜åˆ†è¾¨ç‡å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (15, 12),
    "figsize_medium": (12, 8), 
    "bbox_inches": "tight"
}

print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   æµ‹è¯•æ ·æœ¬: {ALL_TEST_SAMPLES}")
print(f"   æ­£å¸¸æ ·æœ¬: {TEST_SAMPLES['normal']}")
print(f"   æ•…éšœæ ·æœ¬: {TEST_SAMPLES['fault']}")
print(f"   æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   ä¸‰çª—å£å‚æ•°: {WINDOW_CONFIG}")
else:
    print(f"   5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

#----------------------------------------æ¨¡å‹æ–‡ä»¶æ£€æŸ¥------------------------------
def check_model_files():
    """æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶...")
    
    missing_files = []
    paths = MODEL_PATHS["TRANSFORMER"]
    
    # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶
    for key, path in paths.items():
        if not os.path.exists(path):
            missing_files.append(f"TRANSFORMER: {path}")
            print(f"   âŒ ç¼ºå¤±: {path}")
        else:
            file_size = os.path.getsize(path) / (1024 * 1024)  # MB
            print(f"   âœ… å­˜åœ¨: {path} ({file_size:.1f}MB)")
    
    # æ£€æŸ¥PCAå‚æ•°æ–‡ä»¶ (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params_hybrid_feedback.pkl"
    if not os.path.exists(pca_params_path):
        print(f"   âš ï¸  PCAå‚æ•°pickleæ–‡ä»¶ä¸å­˜åœ¨: {pca_params_path}")
        print(f"   ğŸ” æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶...")
        
        # æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶
        pca_npy_files = [
            "/mnt/bz25t/bzhy/datasave/Transformer/models/v_I_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_mean_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_std_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/T_99_limit_hybrid_feedback.npy"
        ]
        
        npy_files_exist = 0
        for npy_file in pca_npy_files:
            if os.path.exists(npy_file):
                npy_files_exist += 1
                file_size = os.path.getsize(npy_file) / (1024 * 1024)
                print(f"   âœ… å­˜åœ¨: {npy_file} ({file_size:.1f}MB)")
            else:
                print(f"   âŒ ç¼ºå¤±: {npy_file}")
        
        if npy_files_exist >= 3:
            print(f"   âœ… å‘ç°{npy_files_exist}ä¸ªnpyæ–‡ä»¶ï¼Œå¯ä»¥é‡å»ºPCAå‚æ•°")
        else:
            missing_files.append(f"PCA_PARAMS: ç¼ºå°‘è¶³å¤Ÿçš„npyæ–‡ä»¶")
    else:
        file_size = os.path.getsize(pca_params_path) / (1024 * 1024)  # MB
        print(f"   âœ… å­˜åœ¨: {pca_params_path} ({file_size:.1f}MB)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå¤± {len(missing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
        for file in missing_files:
            print(f"   {file}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿å·²è¿è¡ŒTransformerè®­ç»ƒè„šæœ¬")
        print("   2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥æ–‡ä»¶æƒé™")
        raise FileNotFoundError("è¯·å…ˆè¿è¡ŒTransformerè®­ç»ƒè„šæœ¬ç”Ÿæˆæ‰€éœ€æ¨¡å‹æ–‡ä»¶")
    
    print("âœ… Transformeræ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

# æ‰§è¡Œæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
check_model_files()

#----------------------------------------ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶------------------------------
def three_window_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    åŸºäºFAIçš„ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶
    
    åŸç†ï¼š
    1. æ£€æµ‹çª—å£ï¼šåŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    2. éªŒè¯çª—å£ï¼šç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§ï¼Œæ’é™¤éšæœºæ³¢åŠ¨
    3. æ ‡è®°çª—å£ï¼šè€ƒè™‘æ•…éšœçš„å‰åå½±å“èŒƒå›´
    
    Args:
        fai_values: FAIåºåˆ—ï¼ˆç»¼åˆæ•…éšœæŒ‡æ ‡ï¼‰
        threshold1: FAIé˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè®°å½•ï¼‰
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # è·å–çª—å£é…ç½®
    if config is None:
        config = WINDOW_CONFIG
        
    detection_window = config["detection_window"]
    verification_window = config["verification_window"]
    marking_window = config["marking_window"]
    verification_threshold = config["verification_threshold"]
    
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'candidate_points': [],    # å€™é€‰æ•…éšœç‚¹
        'verified_points': [],     # å·²éªŒè¯çš„æ•…éšœç‚¹
        'marked_regions': [],      # æ ‡è®°çš„æ•…éšœåŒºåŸŸ
        'window_stats': {},        # çª—å£ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        }
    }
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - åŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    candidate_points = []
    for i in range(len(fai_values)):
        if fai_values[i] > threshold1:
            candidate_points.append(i)
    
    detection_info['candidate_points'] = candidate_points
    
    if len(candidate_points) == 0:
        # æ²¡æœ‰å€™é€‰ç‚¹ï¼Œç›´æ¥è¿”å›
        return fault_labels, detection_info
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - ç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§
    verified_points = []
    for candidate in candidate_points:
        # å®šä¹‰éªŒè¯çª—å£èŒƒå›´ï¼ˆå‰åå„åŠä¸ªçª—å£ï¼‰
        start_verify = max(0, candidate - verification_window//2)
        end_verify = min(len(fai_values), candidate + verification_window//2)
        verify_data = fai_values[start_verify:end_verify]
        
        # è®¡ç®—FAIå¼‚å¸¸æ¯”ä¾‹
        continuous_ratio = np.sum(verify_data > threshold1) / len(verify_data)
        
        # è®¡ç®—FAIåœ¨éªŒè¯çª—å£çš„ç»Ÿè®¡ç‰¹æ€§
        window_stats = {
            'mean': np.mean(verify_data),
            'std': np.std(verify_data),
            'max': np.max(verify_data),
            'min': np.min(verify_data),
            'duration': end_verify - start_verify
        }
        
        # åŸºäºverification_thresholdéªŒè¯æŒç»­æ€§
        if continuous_ratio >= verification_threshold:
            verified_points.append({
                'point': candidate,
                'continuous_ratio': continuous_ratio,
                'verify_range': (start_verify, end_verify),
                'window_stats': window_stats
            })
    
    detection_info['verified_points'] = verified_points
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - è€ƒè™‘æ•…éšœçš„å½±å“èŒƒå›´
    marked_regions = []
    for verified in verified_points:
        candidate = verified['point']
        
        # å®šä¹‰å¯¹ç§°çš„æ ‡è®°çª—å£èŒƒå›´
        start_mark = max(0, candidate - marking_window)
        end_mark = min(len(fai_values), candidate + marking_window)
        
        # æå–æ ‡è®°åŒºåŸŸçš„FAIç‰¹å¾
        mark_data = fai_values[start_mark:end_mark]
        region_stats = {
            'mean_fai': np.mean(mark_data),
            'max_fai': np.max(mark_data),
            'std_fai': np.std(mark_data),
            'duration': end_mark - start_mark
        }
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start_mark:end_mark] = 1
        
        marked_regions.append({
            'center': candidate,
            'range': (start_mark, end_mark),
            'length': end_mark - start_mark,
            'region_stats': region_stats
        })
    
    detection_info['marked_regions'] = marked_regions
    
    # å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯
    detection_info['window_stats'] = {
        'total_candidates': len(candidate_points),
        'verified_candidates': len(verified_points),
        'total_fault_points': np.sum(fault_labels),
        'fault_ratio': np.sum(fault_labels) / len(fault_labels),
        'mean_continuous_ratio': np.mean([v['continuous_ratio'] for v in verified_points]) if verified_points else 0,
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0
    }
    
    return fault_labels, detection_info

def five_point_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    æ”¹è¿›çš„5ç‚¹æ•…éšœæ£€æµ‹æœºåˆ¶ï¼šå¢å¼ºè¿ç»­æ€§æ£€æµ‹å’Œé™å™ªèƒ½åŠ›
    
    è®¾è®¡åŸç†ï¼š
    1. ä¸¥æ ¼çš„è§¦å‘æ¡ä»¶ï¼šè¦æ±‚ä¸­å¿ƒç‚¹åŠå…¶é‚»åŸŸæ»¡è¶³æ›´ä¸¥æ ¼çš„ä¸€è‡´æ€§
    2. åˆç†çš„æ ‡è®°èŒƒå›´ï¼šæ ¹æ®æ•…éšœçº§åˆ«æ ‡è®°ä¸åŒå¤§å°çš„åŒºåŸŸ
    3. æœ‰æ•ˆçš„é™å™ªæœºåˆ¶ï¼šè¿‡æ»¤å­¤ç«‹å¼‚å¸¸ç‚¹ï¼Œå…³æ³¨æŒç»­æ€§æ•…éšœ
    
    Args:
        fai_values: ç»¼åˆè¯Šæ–­æŒ‡æ ‡åºåˆ—
        threshold1: ä¸€çº§é¢„è­¦é˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè°ƒè¯•ï¼‰
        config: é…ç½®å‚æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå¯åŒ…å«threshold2, threshold3ï¼‰
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=è½»å¾®æ•…éšœ, 2=ä¸­ç­‰æ•…éšœ, 3=ä¸¥é‡æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'trigger_points': [],      # è§¦å‘5ç‚¹æ£€æµ‹çš„ç‚¹
        'marked_regions': [],      # æ ‡è®°çš„5ç‚¹åŒºåŸŸ
        'detection_stats': {},     # æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        }
    }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•…éšœæ ·æœ¬ï¼ˆåŸºäºæ ·æœ¬IDï¼‰
    is_fault_sample = sample_id in TEST_SAMPLES['fault']
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"   æ ·æœ¬{sample_id}: ç±»å‹={type(sample_id)}, æ•…éšœæ ·æœ¬åˆ—è¡¨={TEST_SAMPLES['fault']}, æ˜¯æ•…éšœæ ·æœ¬={is_fault_sample}")
    
    if not is_fault_sample:
        # æ­£å¸¸æ ·æœ¬ç›´æ¥è¿”å›å…¨0æ ‡ç­¾
        print(f"   â†’ æ ·æœ¬{sample_id}ä¸ºæ­£å¸¸æ ·æœ¬ï¼Œè¿”å›å…¨0æ ‡ç­¾")
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'normal_sample'
        }
        # ä¸ºå…¼å®¹æ€§æ·»åŠ ç©ºå­—æ®µ
        detection_info['trigger_points'] = []
        detection_info['marked_regions'] = []
        detection_info['candidate_points'] = []
        detection_info['verified_points'] = []
        
        # ç¡®ä¿fault_labelsç¡®å®æ˜¯å…¨0
        fault_labels.fill(0)
        print(f"   â†’ fault_labelsæ€»å’Œ: {np.sum(fault_labels)} (åº”è¯¥ä¸º0)")
        return fault_labels, detection_info
    
    # è·å–å¤šçº§é˜ˆå€¼ï¼ˆä¸¥æ ¼æŒ‰ç…§æºä»£ç Test_.pyçš„æ–¹å¼ï¼‰
    if config and 'threshold2' in config and 'threshold3' in config:
        threshold2 = config['threshold2']
        threshold3 = config['threshold3']
    else:
        # æŒ‰ç…§æºä»£ç Test_.pyçš„é˜ˆå€¼è®¡ç®—æ–¹å¼
        nm = 3000
        mm = len(fai_values)
        
        if mm > nm:
            # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼ï¼ˆä¸æºä»£ç ä¸€è‡´ï¼‰
            baseline_fai = fai_values[nm:mm]
            mean_fai = np.mean(baseline_fai)
            std_fai = np.std(baseline_fai)
            
            threshold1_calc = mean_fai + 3 * std_fai      # å¯¹åº”æºä»£ç threshold1
            threshold2 = mean_fai + 4.5 * std_fai        # å¯¹åº”æºä»£ç threshold2  
            threshold3 = mean_fai + 6 * std_fai          # å¯¹åº”æºä»£ç threshold3
            
            # éªŒè¯threshold1æ˜¯å¦ä¸ä¼ å…¥çš„ä¸€è‡´ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"   æºä»£ç é˜ˆå€¼è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
        else:
            # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
            mean_fai = np.mean(fai_values)
            std_fai = np.std(fai_values)
            
            threshold1_calc = mean_fai + 3 * std_fai
            threshold2 = mean_fai + 4.5 * std_fai
            threshold3 = mean_fai + 6 * std_fai
            
            print(f"   çŸ­æ•°æ®é˜ˆå€¼è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    # æ•…éšœæ ·æœ¬ï¼šå®æ–½æ”¹è¿›çš„å¤šçº§5ç‚¹æ£€æµ‹
    trigger_points = []
    marked_regions = []
    
    # ç­–ç•¥4.0ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥
    print(f"   ğŸ”§ ç­–ç•¥4.0: ä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥...")
    print(f"   åˆ†æç»“æœ: æ•…éšœæ ·æœ¬æœ‰{np.sum(fai_values > threshold1)}ä¸ªå¼‚å¸¸ç‚¹({np.sum(fai_values > threshold1)/len(fai_values)*100:.2f}%)")
    print(f"   é˜ˆå€¼åˆ†å¸ƒ: >6Ïƒ({np.sum(fai_values > threshold3)}ä¸ª), >4.5Ïƒ({np.sum(fai_values > threshold2)}ä¸ª), >3Ïƒ({np.sum(fai_values > threshold1)}ä¸ª)")
    
    detection_config = {
        'mode': 'hierarchical_v2',
        'level_3': {
            'center_threshold': threshold3,      # 6Ïƒ
            'neighbor_threshold': None,          # æ— é‚»åŸŸè¦æ±‚
            'min_neighbors': 0,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level3_high_confidence'
        },
        'level_2': {
            'center_threshold': threshold2,      # 4.5Ïƒ  
            'neighbor_threshold': threshold1,    # 3Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level2_medium_confidence'
        },
        'level_1': {
            'center_threshold': threshold1,      # 3Ïƒ
            'neighbor_threshold': threshold1 * 0.67,  # 2Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1 (3ä¸ªç‚¹)
            'condition': 'level1_basic_confidence'
        }
    }
    
    print(f"   æ£€æµ‹å‚æ•°:")
    print(f"   Level 3 (6Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold3:.4f}, æ— é‚»åŸŸè¦æ±‚, æ ‡è®°3ç‚¹")
    print(f"   Level 2 (4.5Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold2:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    print(f"   Level 1 (3Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold1:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1*0.67:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    
    # ä¸‰çº§åˆ†çº§æ£€æµ‹å®ç°
    triggers = []
    for i in range(2, len(fai_values) - 2):
        neighborhood = fai_values[i-2:i+3]  # 5ä¸ªç‚¹çš„é‚»åŸŸ
        neighbors = [fai_values[i-2], fai_values[i-1], fai_values[i+1], fai_values[i+2]]  # 4ä¸ªé‚»å±…
        center = fai_values[i]
        
        triggered = False
        trigger_level = None
        trigger_condition = None
        detection_details = {}
        
        # Level 3: æœ€ä¸¥æ ¼é˜ˆå€¼ï¼Œæœ€å®½æ¾æ¡ä»¶ (6Ïƒ)
        if center > detection_config['level_3']['center_threshold']:
            triggered = True
            trigger_level = 3
            trigger_condition = detection_config['level_3']['condition']
            marking_range = detection_config['level_3']['marking_range']
            detection_details = {
                'center_value': center,
                'center_threshold': detection_config['level_3']['center_threshold'],
                'neighbors_above_threshold': 'N/A (no requirement)',
                'required_neighbors': 0,
                'neighborhood_values': neighborhood.tolist(),
                'trigger_reason': '6Ïƒ high confidence detection'
            }
            
        # Level 2: ä¸­ç­‰é˜ˆå€¼ï¼Œä¸­ç­‰æ¡ä»¶ (4.5Ïƒ) 
        elif center > detection_config['level_2']['center_threshold']:
            neighbors_above_t1 = np.sum(np.array(neighbors) > detection_config['level_2']['neighbor_threshold'])
            if neighbors_above_t1 >= detection_config['level_2']['min_neighbors']:
                triggered = True
                trigger_level = 2
                trigger_condition = detection_config['level_2']['condition']
                marking_range = detection_config['level_2']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_2']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_t1,
                    'required_neighbors': detection_config['level_2']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '4.5Ïƒ medium confidence detection'
                }
                
        # Level 1: æœ€ä½é˜ˆå€¼ï¼Œç›¸å¯¹ä¸¥æ ¼æ¡ä»¶ (3Ïƒ)
        elif center > detection_config['level_1']['center_threshold']:
            neighbors_above_2sigma = np.sum(np.array(neighbors) > detection_config['level_1']['neighbor_threshold'])
            if neighbors_above_2sigma >= detection_config['level_1']['min_neighbors']:
                triggered = True
                trigger_level = 1
                trigger_condition = detection_config['level_1']['condition']
                marking_range = detection_config['level_1']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_1']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_2sigma,
                    'required_neighbors': detection_config['level_1']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '3Ïƒ basic confidence detection'
                }
        
        if triggered:
            # è®¡ç®—æ ‡è®°èŒƒå›´
            start_mark = max(0, i + min(marking_range))
            end_mark = min(len(fai_values), i + max(marking_range) + 1)
            
            triggers.append({
                'center': i,
                'level': trigger_level,
                'range': (start_mark, end_mark),
                'trigger_condition': trigger_condition,
                'detection_details': detection_details
            })
    
    # ç¬¬äºŒè½®ï¼šå¤„ç†æ‰€æœ‰è§¦å‘ç‚¹ï¼ˆåˆ†çº§å¤„ç†ï¼‰
    processed_triggers = []
    level_counts = {1: 0, 2: 0, 3: 0}
    
    for trigger in triggers:
        start, end = trigger['range']
        center = trigger['center']
        level = trigger['level']
        
        # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
        level_counts[level] += 1
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start:end] = level  # ä½¿ç”¨çº§åˆ«ä½œä¸ºæ ‡è®°å€¼
        trigger_points.append(center)
        
        # è®°å½•åŒºåŸŸä¿¡æ¯
        region_data = fai_values[start:end]
        region_stats = {
            'mean_fai': np.mean(region_data),
            'max_fai': np.max(region_data),
            'min_fai': np.min(region_data),
            'std_fai': np.std(region_data),
            'length': end - start
        }
        
        marked_regions.append({
            'trigger_point': center,
            'level': level,  # åˆ†çº§æ ‡è®°
            'range': (start, end),
            'length': end - start,
            'region_stats': region_stats,
            'trigger_condition': trigger['trigger_condition'],
            'trigger_values': {
                'center': fai_values[center],
                'detection_level': f"Level {level}",
                'trigger_reason': trigger['detection_details']['trigger_reason']
            }
        })
        
        processed_triggers.append(trigger)
    
    print(f"   è§¦å‘ç»Ÿè®¡: Level 3({level_counts[3]}æ¬¡), Level 2({level_counts[2]}æ¬¡), Level 1({level_counts[1]}æ¬¡)")
    
    detection_info['trigger_points'] = trigger_points
    detection_info['marked_regions'] = marked_regions
    detection_info['processed_triggers'] = processed_triggers
    
    # ä¸ºå…¼å®¹ä¸‰çª—å£æ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–ä»£ç ï¼Œæ·»åŠ ç©ºçš„å…¼å®¹å­—æ®µ
    detection_info['candidate_points'] = []  # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    detection_info['verified_points'] = []   # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†çº§æ£€æµ‹ï¼‰
    fault_count = np.sum(fault_labels > 0)  # ä»»ä½•çº§åˆ«éƒ½ç®—æ•…éšœ
    level1_count = np.sum(fault_labels == 1)
    level2_count = np.sum(fault_labels == 2)
    level3_count = np.sum(fault_labels == 3)
    
    detection_info['detection_stats'] = {
        'total_trigger_points': len(trigger_points),
        'total_marked_regions': len(marked_regions),
        'total_fault_points': fault_count,
        'fault_ratio': fault_count / len(fault_labels),
        'detection_mode': 'hierarchical_three_level',
        'level_statistics': {
            'level_1_points': level1_count,
            'level_2_points': level2_count,
            'level_3_points': level3_count,
            'level_1_triggers': level_counts[1],
            'level_2_triggers': level_counts[2],
            'level_3_triggers': level_counts[3]
        },
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0,
        'mean_trigger_fai': np.mean([m['trigger_values']['center'] for m in marked_regions]) if marked_regions else 0,
        'strategy_used': 'strategy_4_hierarchical_detection',
        'parameters': detection_config
    }
    
    print(f"   â†’ ç­–ç•¥4.0æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={fault_count}ä¸ª ({fault_count/len(fault_labels)*100:.2f}%)")
    print(f"   â†’ åˆ†çº§ç»Ÿè®¡: L1={level1_count}ç‚¹, L2={level2_count}ç‚¹, L3={level3_count}ç‚¹")
    print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(triggers)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
    
    # æ·»åŠ æ”¹è¿›æ•ˆæœå¯¹æ¯”
    original_anomaly_count = np.sum(fai_values > threshold1)
    detected_fault_count = np.sum(fault_labels > 0)
    noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count) if original_anomaly_count > 0 else 0
    
    print(f"   â†’ é™å™ªæ•ˆæœ: åŸå§‹å¼‚å¸¸ç‚¹={original_anomaly_count}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    # å¦‚æœç­–ç•¥1æ²¡æœ‰æ£€æµ‹åˆ°æ•…éšœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2
    if detected_fault_count == 0 and is_fault_sample:
        print(f"   âš ï¸  ç­–ç•¥1æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2...")
        print(f"   ğŸ”§ ç­–ç•¥2: è¿›ä¸€æ­¥æ”¾å®½é‚»åŸŸè¦æ±‚ï¼ˆé‚»åŸŸé˜ˆå€¼=0.6Ã—3Ïƒ, æ— é‚»å±…è¦æ±‚ï¼‰")
        
        # é‡ç½®æ ‡ç­¾å’Œåˆ—è¡¨
        fault_labels.fill(0)
        trigger_points.clear()
        marked_regions.clear()
        
        # ç­–ç•¥2å‚æ•°
        strategy2_config = {
            'center_threshold': threshold1,           # ä¿æŒ3Ïƒé˜ˆå€¼
            'neighbor_threshold': threshold1 * 0.6,  # è¿›ä¸€æ­¥é™ä½é‚»åŸŸè¦æ±‚åˆ°60%
            'min_neighbors': 0,                      # ä¸è¦æ±‚é‚»å±…ï¼ˆçº¯ä¸­å¿ƒç‚¹æ£€æµ‹ï¼‰
            'marking_range': 2,                      # æ ‡è®°Â±2ä¸ªç‚¹
            'condition': 'strategy2_relaxed'
        }
        
        # ç­–ç•¥2æ£€æµ‹
        strategy2_triggers = []
        for i in range(2, len(fai_values) - 2):
            center = fai_values[i]
            
            # ç­–ç•¥2æ¡ä»¶ï¼šåªæ£€æŸ¥ä¸­å¿ƒç‚¹
            if center > strategy2_config['center_threshold']:
                start_mark = max(0, i - strategy2_config['marking_range'])
                end_mark = min(len(fai_values), i + strategy2_config['marking_range'] + 1)
                
                fault_labels[start_mark:end_mark] = 1
                trigger_points.append(i)
                
                region_data = fai_values[start_mark:end_mark]
                marked_regions.append({
                    'trigger_point': i,
                    'level': 1,
                    'range': (start_mark, end_mark),
                    'length': end_mark - start_mark,
                    'region_stats': {
                        'mean_fai': np.mean(region_data),
                        'max_fai': np.max(region_data),
                        'std_fai': np.std(region_data),
                        'length': end_mark - start_mark
                    },
                    'trigger_condition': strategy2_config['condition'],
                    'trigger_values': {
                        'center': center
                    }
                })
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detected_fault_count = np.sum(fault_labels > 0)
        
        # æ›´æ–°detection_info
        detection_info['trigger_points'] = trigger_points
        detection_info['marked_regions'] = marked_regions
        detection_info['detection_stats']['total_trigger_points'] = len(trigger_points)
        detection_info['detection_stats']['total_marked_regions'] = len(marked_regions)
        detection_info['detection_stats']['total_fault_points'] = detected_fault_count
        detection_info['detection_stats']['fault_ratio'] = detected_fault_count / len(fault_labels)
        detection_info['detection_stats']['strategy_used'] = 'strategy_2_center_only'
        detection_info['detection_stats']['parameters'] = strategy2_config
        
        noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count) if original_anomaly_count > 0 else 0
        
        print(f"   â†’ ç­–ç•¥2æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={detected_fault_count}ä¸ª ({detected_fault_count/len(fault_labels)*100:.2f}%)")
        print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(trigger_points)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
        print(f"   â†’ ç­–ç•¥2é™å™ªæ•ˆæœ: åŸå§‹å¼‚å¸¸ç‚¹={original_anomaly_count}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    elif detected_fault_count == 0:
        print(f"   âš ï¸  æ­£å¸¸æ ·æœ¬æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œç¬¦åˆé¢„æœŸ")
        print(f"   â†’ æ£€æµ‹é€»è¾‘å·¥ä½œæ­£å¸¸")
    
    return fault_labels, detection_info

#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_test_sample(sample_id):
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    base_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}'
    
    # æ£€æŸ¥æ ·æœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_path):
        raise FileNotFoundError(f"æµ‹è¯•æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_path}")
    
    # åŠ è½½vin_1, vin_2, vin_3æ•°æ®
    try:
        with open(f'{base_path}/vin_1.pkl', 'rb') as f:
            vin1_data = pickle.load(f)
        with open(f'{base_path}/vin_2.pkl', 'rb') as f:
            vin2_data = pickle.load(f) 
        with open(f'{base_path}/vin_3.pkl', 'rb') as f:
            vin3_data = pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"æ ·æœ¬ {sample_id} æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        
    return vin1_data, vin2_data, vin3_data

def load_models():
    """åŠ è½½Transformeræ¨¡å‹"""
    models = {}
    
    print("ğŸ”§ å¼€å§‹åŠ è½½Transformeræ¨¡å‹...")
    
    # åŠ è½½Transformeræ¨¡å‹
    from Train_Transformer_HybridFeedback import TransformerPredictor
    models['transformer'] = TransformerPredictor().to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['transformer'], 
                          MODEL_PATHS["TRANSFORMER"]["transformer_model"], 
                          "Transformer"):
        raise RuntimeError("Transformeræ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½MC-AEæ¨¡å‹
    models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                              activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                               activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['net'], 
                          MODEL_PATHS["TRANSFORMER"]["net_model"], 
                          "MC-AE1"):
        raise RuntimeError("MC-AE1æ¨¡å‹åŠ è½½å¤±è´¥")
    
    if not safe_load_model(models['netx'], 
                          MODEL_PATHS["TRANSFORMER"]["netx_model"], 
                          "MC-AE2"):
        raise RuntimeError("MC-AE2æ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½PCAå‚æ•° (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params_hybrid_feedback.pkl"
    try:
        with open(pca_params_path, 'rb') as f:
            models['pca_params'] = pickle.load(f)
        print(f"âœ… PCAå‚æ•°å·²åŠ è½½: {pca_params_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½PCAå‚æ•°å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä»å•ç‹¬çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°...")
        try:
            # ä»è®­ç»ƒè„šæœ¬ä¿å­˜çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°
            pca_base_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/"
            models['pca_params'] = {
                'v_I': np.load(f"{pca_base_path}v_I_hybrid_feedback.npy"),
                'v': np.load(f"{pca_base_path}v_hybrid_feedback.npy"),
                'v_ratio': np.load(f"{pca_base_path}v_ratio_hybrid_feedback.npy"),
                'p_k': np.load(f"{pca_base_path}p_k_hybrid_feedback.npy"),
                'data_mean': np.load(f"{pca_base_path}data_mean_hybrid_feedback.npy"),
                'data_std': np.load(f"{pca_base_path}data_std_hybrid_feedback.npy"),
                'T_95_limit': np.load(f"{pca_base_path}T_95_limit_hybrid_feedback.npy"),
                'T_99_limit': np.load(f"{pca_base_path}T_99_limit_hybrid_feedback.npy"),
                'SPE_95_limit': np.load(f"{pca_base_path}SPE_95_limit_hybrid_feedback.npy"),
                'SPE_99_limit': np.load(f"{pca_base_path}SPE_99_limit_hybrid_feedback.npy"),
                'P': np.load(f"{pca_base_path}P_hybrid_feedback.npy"),
                'k': np.load(f"{pca_base_path}k_hybrid_feedback.npy"),
                'P_t': np.load(f"{pca_base_path}P_t_hybrid_feedback.npy"),
                'X': np.load(f"{pca_base_path}X_hybrid_feedback.npy"),
                'data_nor': np.load(f"{pca_base_path}data_nor_hybrid_feedback.npy")
            }
            print(f"âœ… PCAå‚æ•°ä»npyæ–‡ä»¶é‡å»ºæˆåŠŸ")
        except Exception as e2:
            print(f"âŒ PCAå‚æ•°é‡å»ºä¹Ÿå¤±è´¥: {e2}")
        raise RuntimeError("PCAå‚æ•°åŠ è½½å¤±è´¥")
    
    return models

#----------------------------------------å•æ ·æœ¬å¤„ç†å‡½æ•°------------------------------
def process_single_sample(sample_id, models, config=None):
    """
    å¤„ç†å•ä¸ªæµ‹è¯•æ ·æœ¬
    
    Args:
        sample_id: æ ·æœ¬ID
        models: åŠ è½½çš„æ¨¡å‹
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
    
    # æ•°æ®é¢„å¤„ç†
    if len(vin1_data.shape) == 2:
        vin1_data = vin1_data.unsqueeze(1)
    vin1_data = vin1_data.to(torch.float32).to(device)

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è€Œä¸æ˜¯é‡æ–°è®¡ç®—
    pca_params = models['pca_params']
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
    
    # MC-AEæ¨ç†
    models['net'].eval()
    models['netx'].eval()
    
    with torch.no_grad():
        models['net'] = models['net'].double()
        models['netx'] = models['netx'].double()
        
        recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
        reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    AA = recon_imtest[0].cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU

    BB = reconx_imtest[0].cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX

    # è¯Šæ–­ç‰¹å¾æå–
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—
    time = np.arange(df_data.shape[0])
    
    lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
        df_data.values, 
        pca_params['data_mean'], 
        pca_params['data_std'], 
        pca_params['v'].reshape(len(pca_params['v']), 1), 
        pca_params['p_k'], 
        pca_params['v_I'], 
        pca_params['T_99_limit'], 
        pca_params['SPE_99_limit'], 
        pca_params['X'], 
        time
    )
    
    # è®¡ç®—é˜ˆå€¼ - æŒ‰æºä»£ç æ–¹å¼ï¼Œæ¯ä¸ªæ ·æœ¬ä½¿ç”¨è‡ªå·±çš„æ•°æ®
    nm = 3000  # å›ºå®šå€¼ï¼Œä¸æºä»£ç ä¸€è‡´
    mm = len(fai)  # æ•°æ®æ€»é•¿åº¦
    
    # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
    if mm > nm:
        # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼ï¼ˆæºä»£ç é€»è¾‘ï¼‰
        threshold1 = np.mean(fai[nm:mm]) + 3*np.std(fai[nm:mm])
        threshold2 = np.mean(fai[nm:mm]) + 4.5*np.std(fai[nm:mm])
        threshold3 = np.mean(fai[nm:mm]) + 6*np.std(fai[nm:mm])
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        print(f"   âš ï¸ æ ·æœ¬{sample_id}æ•°æ®é•¿åº¦({mm})ä¸è¶³3000ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—é˜ˆå€¼")
        threshold1 = np.mean(fai) + 3*np.std(fai)
        threshold2 = np.mean(fai) + 4.5*np.std(fai)
        threshold3 = np.mean(fai) + 6*np.std(fai)
    
    # æ ¹æ®æ£€æµ‹æ¨¡å¼é€‰æ‹©æ£€æµ‹å‡½æ•°
    if CURRENT_DETECTION_MODE == "five_point":
        fault_labels, detection_info = five_point_fault_detection(fai, threshold1, sample_id, config)
    else:
        fault_labels, detection_info = three_window_fault_detection(fai, threshold1, sample_id, config)
    
    # æ„å»ºç»“æœ
    sample_result = {
        'sample_id': sample_id,
        'model_type': 'TRANSFORMER',
        'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
        'df_data': df_data.values,
        'fai': fai,
        'T_squared': t_total,
        'SPE': q_total,
        'thresholds': {
            'threshold1': threshold1,
            'threshold2': threshold2, 
            'threshold3': threshold3
        },
        'fault_labels': fault_labels,
        'detection_info': detection_info,
        'performance_metrics': {
            'fai_mean': np.mean(fai),
            'fai_std': np.std(fai),
            'fai_max': np.max(fai),
            'fai_min': np.min(fai),
            'anomaly_count': np.sum(fai > threshold1),
            'anomaly_ratio': np.sum(fai > threshold1) / len(fai)
        }
    }
    
    return sample_result

#----------------------------------------ä¸»æµ‹è¯•æµç¨‹------------------------------
def main_test_process():
    """ä¸»è¦æµ‹è¯•æµç¨‹"""
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    test_results = {
        "TRANSFORMER": [],
        "metadata": {
            "test_samples": TEST_SAMPLES,
            "window_config": WINDOW_CONFIG,
            "detection_modes": DETECTION_MODES,
            "current_mode": CURRENT_DETECTION_MODE,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    print(f"\nğŸš€ å¼€å§‹Transformeræ¨¡å‹æµ‹è¯•...")
    print(f"æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
    print(f"æ£€æµ‹æè¿°: {DETECTION_MODES[CURRENT_DETECTION_MODE]['description']}")
    print(f"æ€»å…±éœ€è¦å¤„ç†: {len(ALL_TEST_SAMPLES)} ä¸ªæ ·æœ¬")
    
    # åŠ è½½æ¨¡å‹
    print(f"\n{'='*20} åŠ è½½æ¨¡å‹ {'='*20}")
    models = load_models()
    print(f"âœ… Transformer æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # å•é…ç½®æµ‹è¯• (ä¸BiLSTMä¿æŒä¸€è‡´)
    print(f"\n{'='*20} æµ‹è¯• Transformer æ¨¡å‹ {'='*20}")
        if CURRENT_DETECTION_MODE == "three_window":
        print(f"   æ£€æµ‹çª—å£: {WINDOW_CONFIG['detection_window']}")
        print(f"   éªŒè¯çª—å£: {WINDOW_CONFIG['verification_window']}")
        print(f"   æ ‡è®°çª—å£: {WINDOW_CONFIG['marking_window']}")
        print(f"   éªŒè¯é˜ˆå€¼: {WINDOW_CONFIG['verification_threshold']}")
        else:
            print(f"   5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")
        
    with tqdm(total=len(ALL_TEST_SAMPLES), desc="Transformeræµ‹è¯•è¿›åº¦",
                  bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}]') as pbar:
            
            for sample_id in ALL_TEST_SAMPLES:
            pbar.set_description(f"Transformer-æ ·æœ¬{sample_id}")
            
            try:
                # ä½¿ç”¨çª—å£é…ç½®å¤„ç†æ ·æœ¬
                sample_result = process_single_sample(sample_id, models, WINDOW_CONFIG)
                test_results["TRANSFORMER"].append(sample_result)
                    
                    # è¾“å‡ºç®€è¦ç»“æœ
                    metrics = sample_result.get('performance_metrics', {})
                    detection_info = sample_result.get('detection_info', {})
                    
                    if CURRENT_DETECTION_MODE == "three_window":
                        window_stats = detection_info.get('window_stats', {})
                        detection_ratio = window_stats.get('fault_ratio', 0.0)
                    else:
                        detection_stats = detection_info.get('detection_stats', {})
                        detection_ratio = detection_stats.get('fault_ratio', 0.0)
                    
                    print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics.get('fai_mean', 0.0):.6f}, "
                          f"å¼‚å¸¸ç‡={metrics.get('anomaly_ratio', 0.0):.2%}, "
                          f"æ£€æµ‹ç‡={detection_ratio:.2%}")
                    
                except Exception as e:
                    print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                    continue
                
                pbar.update(1)
                time.sleep(0.1)  # é¿å…è¿›åº¦æ¡æ›´æ–°è¿‡å¿«
    
    print(f"\nâœ… Transformeræµ‹è¯•å®Œæˆ!")
    print(f"   Transformer: æˆåŠŸå¤„ç† {len(test_results['TRANSFORMER'])} ä¸ªæ ·æœ¬")
    
    return test_results

#----------------------------------------æ€§èƒ½åˆ†æå‡½æ•°------------------------------
def calculate_performance_metrics(test_results):
    """è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ”¬ è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡...")
    
    performance_metrics = {}
    model_results = test_results["TRANSFORMER"]
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_true_labels = []
    all_fai_values = []
    all_fault_predictions = []
    
    for result in model_results:
        true_label = result.get('label', 0)
        fai_values = result.get('fai', [])
        fault_labels = result.get('fault_labels', [])
        thresholds = result.get('thresholds', {})
        threshold1 = thresholds.get('threshold1', 0.0)
        
        # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
        for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
            all_true_labels.append(true_label)
            all_fai_values.append(fai_val)
            
            # ä¿®æ­£åçš„ROCé€»è¾‘ï¼š
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                # æ­£å¸¸æ ·æœ¬ä¸­ï¼šç»¼åˆè¯Šæ–­å€¼ > é˜ˆå€¼1 å°±æ˜¯FPï¼Œå¦åˆ™å°±æ˜¯TN
                prediction = 1 if fai_val > threshold1 else 0
            else:  # æ•…éšœæ ·æœ¬
                # æ•…éšœæ ·æœ¬ä¸­ï¼šéœ€è¦ç»¼åˆè¯Šæ–­å€¼ > é˜ˆå€¼1 ä¸” ä¸‰çª—å£ç¡®è®¤ä¸ºæ•…éšœ æ‰æ˜¯TP
                if fai_val > threshold1 and fault_pred == 1:
                    prediction = 1  # TP
                else:
                    prediction = 0  # FN (åŒ…æ‹¬ï¼šfai_val <= threshold1 æˆ– fault_pred == 0)
            
            all_fault_predictions.append(prediction)
    
    # è®¡ç®—ROCæŒ‡æ ‡
    all_true_labels = np.array(all_true_labels)
    all_fai_values = np.array(all_fai_values)
    all_fault_predictions = np.array(all_fault_predictions)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
    fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
    fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
    tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    tpr = recall  # True Positive Rate = Recall
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
    
    # æ ·æœ¬çº§ç»Ÿè®¡
    sample_metrics = {
        'total_samples': len(model_results),
        'normal_samples': len([r for r in model_results if r['label'] == 0]),
        'fault_samples': len([r for r in model_results if r['label'] == 1]),
        'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                 for r in model_results if r['label'] == 0]),
        'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                for r in model_results if r['label'] == 1]),
        'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                           for r in model_results if r['label'] == 0]),
        'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                          for r in model_results if r['label'] == 1])
    }
    
    performance_metrics["TRANSFORMER"] = {
        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
        'classification_metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1_score),
            'specificity': float(specificity),
            'tpr': float(tpr),
            'fpr': float(fpr)
        },
        'sample_metrics': sample_metrics,
        'roc_data': {
            'true_labels': all_true_labels.tolist(),
            'fai_values': all_fai_values.tolist(),
            'predictions': all_fault_predictions.tolist()
        }
    }
    
    return performance_metrics

# æ‰§è¡Œä¸»æµ‹è¯•æµç¨‹
test_results = main_test_process()

#----------------------------------------ROCæ›²çº¿å¯¹æ¯”------------------------------
def create_roc_analysis(test_results, performance_metrics, save_path):
    """ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ"""
    print("   ğŸ“ˆ ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"])
    
    # === å­å›¾1: è¿ç»­é˜ˆå€¼ROCæ›²çº¿ ===
    ax1.set_title('(a) Transformer ROC Curve\n(Continuous Threshold Scan)')
    
    # ä½¿ç”¨Transformerç»“æœ
    model_results = test_results["TRANSFORMER"]
    if not model_results:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„Transformerç»“æœ")
        return
    
    # æ”¶é›†æ‰€æœ‰faiå€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè¿ç»­é˜ˆå€¼ROC
    all_fai = []
    all_labels = []
    all_fault_labels = []
    
    for result in model_results:
        all_fai.extend(result['fai'])
        all_labels.extend([result['label']] * len(result['fai']))
        all_fault_labels.extend(result['fault_labels'])
    
    all_fai = np.array(all_fai)
    all_labels = np.array(all_labels)
    all_fault_labels = np.array(all_fault_labels)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(all_fai) == 0:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„FAIæ•°æ®ï¼Œè·³è¿‡ROCæ›²çº¿ç”Ÿæˆ")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„ROCå›¾
        ax1.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', 
                transform=ax1.transAxes, fontsize=12)
        ax1.set_title('(a) Transformer ROC Curve\n(æ— æ•°æ®)')
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.grid(True, alpha=0.3)
        return
    
    # ç”Ÿæˆè¿ç»­é˜ˆå€¼èŒƒå›´
    thresholds = np.linspace(np.min(all_fai), np.max(all_fai), 100)
    
    tpr_list = []
    fpr_list = []
    
    for threshold in thresholds:
        tp = fp = tn = fn = 0
        
        for i, (fai_val, true_label, fault_pred) in enumerate(zip(all_fai, all_labels, all_fault_labels)):
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                if fai_val > threshold:
                    fp += 1
                else:
                    tn += 1
            else:  # æ•…éšœæ ·æœ¬
                # æ•…éšœæ ·æœ¬ï¼šéœ€è¦fai > threshold ä¸” ä¸‰çª—å£ç¡®è®¤ä¸ºæ•…éšœ æ‰æ˜¯TP
                if fai_val > threshold and fault_pred == 1:
                    tp += 1
                else:
                    fn += 1
        
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        tpr_list.append(tpr)
        fpr_list.append(fpr)
    
    # è®¡ç®—AUC
    from sklearn.metrics import auc
    auc_score = auc(fpr_list, tpr_list)
    
    # ç»˜åˆ¶ROCæ›²çº¿
    ax1.plot(fpr_list, tpr_list, color='blue', linewidth=2,
            label=f'Transformer (AUC={auc_score:.3f})')
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: å›ºå®šé˜ˆå€¼å·¥ä½œç‚¹ ===
    ax2.set_title('(b) Working Point\n(Three-Level Alarm Threshold)')
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
    ax2.scatter(metrics['fpr'], metrics['tpr'], 
               s=200, color='blue', 
               label=f'Transformer\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
               marker='o', edgecolors='black', linewidth=2)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: æ€§èƒ½æŒ‡æ ‡å±•ç¤º ===
    ax3.set_title('(c) Transformer Classification Metrics')
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    metric_mapping = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1_score', 'Specificity': 'specificity'}
    transformer_values = [performance_metrics["TRANSFORMER"]['classification_metrics'][metric_mapping[m]] 
                         for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.6
    
    bars = ax3.bar(x, transformer_values, width, color='blue', alpha=0.7)
    ax3.set_xlabel('Metrics')
    ax3.set_ylabel('Score')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics_names, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_values):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å±•ç¤º ===
    ax4.set_title('(d) Transformer Sample-Level Performance')
    
    sample_metrics = ['Avg Ï†(Normal)', 'Avg Ï†(Fault)', 'Anomaly Rate(Normal)', 'Anomaly Rate(Fault)']
    transformer_sample_values = [
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_fault'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    
    x = np.arange(len(sample_metrics))
    bars = ax4.bar(x, transformer_sample_values, width, color='blue', alpha=0.7)
    
    ax4.set_xlabel('Sample Metrics')
    ax4.set_ylabel('Value')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_metrics, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_sample_values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… Transformer ROCåˆ†æå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºå›¾------------------------------
def create_fault_detection_timeline(test_results, save_path):
    """ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    print("   ğŸ“Š ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
    
    fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)
    
    # æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„ç»“æœ
    sample_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if sample_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        sample_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if sample_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = sample_result.get('fai', [])
    fault_labels = sample_result.get('fault_labels', [])
    thresholds = sample_result.get('thresholds', {})
    time_axis = np.arange(len(fai_values))
    
    # å­å›¾1: ç»¼åˆè¯Šæ–­æŒ‡æ ‡æ—¶åº
    ax1 = axes[0]
    ax1.plot(time_axis, fai_values, color='blue', linewidth=1, alpha=0.8,
           label='Transformer FAI')
    ax1.axhline(y=thresholds['threshold1'], color='orange', linestyle='--', alpha=0.7,
              label='ä¸€çº§é˜ˆå€¼')
    ax1.axhline(y=thresholds['threshold2'], color='red', linestyle='--', alpha=0.7,
              label='äºŒçº§é˜ˆå€¼')
    ax1.axhline(y=thresholds['threshold3'], color='darkred', linestyle='--', alpha=0.7,
              label='ä¸‰çº§é˜ˆå€¼')
    
    ax1.set_ylabel('Transformer\nç»¼åˆè¯Šæ–­æŒ‡æ ‡Ï†')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_title(f'Transformer - æ ·æœ¬ {fault_sample_id} (æ•…éšœæ ·æœ¬)')
    
    # å­å›¾2: æ•…éšœæ£€æµ‹ç»“æœ
    ax2 = axes[1]
    
    # å°†æ•…éšœæ ‡ç­¾è½¬æ¢ä¸ºå¯è§†åŒ–åŒºåŸŸ
    fault_regions = np.where(fault_labels == 1, 0.8, 0)
    ax2.fill_between(time_axis, fault_regions, 
                    alpha=0.6, color='blue',
                    label='Transformer Fault Detection')
    
    ax2.set_ylabel('æ•…éšœæ£€æµ‹ç»“æœ')
    ax2.set_ylim(0, 1)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_title('Transformeræ•…éšœæ£€æµ‹ç»“æœ')
    
    # å­å›¾3: ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹
    ax3 = axes[2]
    detection_info = sample_result['detection_info']
    
    ax3.plot(time_axis, fai_values, 'b-', alpha=0.5, label='Ï†æŒ‡æ ‡å€¼')
    
    # æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        ax3.scatter(detection_info['candidate_points'], 
                   [fai_values[i] for i in detection_info['candidate_points']],
                   color='orange', s=30, label='å€™é€‰ç‚¹', alpha=0.8)
    
    # æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax3.scatter(verified_indices,
                   [fai_values[i] for i in verified_indices],
                   color='red', s=50, label='éªŒè¯ç‚¹', marker='^')
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    for region in detection_info['marked_regions']:
        start, end = region['range']
        ax3.axvspan(start, end, alpha=0.2, color='red', label='æ ‡è®°æ•…éšœåŒºåŸŸ')
    
    ax3.set_ylabel('ä¸‰çª—å£\næ£€æµ‹è¿‡ç¨‹')
    ax3.set_xlabel('æ—¶é—´æ­¥é•¿')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_title('ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹ (Transformer)')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… Transformeræ—¶åºå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾------------------------------
def create_performance_radar(performance_metrics, save_path):
    """ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    print("   ğŸ•¸ï¸ ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'å‡†ç¡®ç‡': 'accuracy',
        'ç²¾ç¡®ç‡': 'precision', 
        'å¬å›ç‡': 'recall',
        'F1åˆ†æ•°': 'f1_score',
        'ç‰¹å¼‚æ€§': 'specificity',
        'æ—©æœŸé¢„è­¦': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'è¯¯æŠ¥æ§åˆ¶': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'æ£€æµ‹ç¨³å®šæ€§': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    # æ•°æ®é¢„å¤„ç†ï¼šFPRéœ€è¦è½¬æ¢ä¸ºæ§åˆ¶èƒ½åŠ› (1-FPR)
    transformer_values = []
    
    for metric_name, metric_key in radar_metrics.items():
        transformer_val = performance_metrics["TRANSFORMER"]['classification_metrics'][metric_key]
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
        if metric_name == 'è¯¯æŠ¥æ§åˆ¶':
            transformer_val = 1 - transformer_val
            
        transformer_values.append(transformer_val)
    
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    transformer_values += transformer_values[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize_medium"], subplot_kw=dict(projection='polar'))
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles, transformer_values, 'o-', linewidth=2, label='Transformer', color='blue')
    ax.fill(angles, transformer_values, alpha=0.25, color='blue')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(list(radar_metrics.keys()))
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('Transformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾', 
              pad=20, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # æ·»åŠ æ€§èƒ½æ€»ç»“
    transformer_avg = np.mean(transformer_values[:-1])
    
    plt.figtext(0.02, 0.02, f'Transformerç»¼åˆæ€§èƒ½: {transformer_avg:.3f}', 
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… Transformeré›·è¾¾å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–------------------------------
def create_three_window_visualization(test_results, save_path):
    """ç”ŸæˆTransformerä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–"""
    print("   ğŸ” ç”ŸæˆTransformerä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'
    
    fig = plt.figure(figsize=(16, 10))
    
    # ä½¿ç”¨GridSpecè¿›è¡Œå¤æ‚å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1])
    
    # === ä¸»å›¾ï¼šä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹æ—¶åºå›¾ ===
    ax_main = fig.add_subplot(gs[0, :])
    
    # é€‰æ‹©Transformerç»“æœè¿›è¡Œå¯è§†åŒ–
    transformer_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if transformer_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        transformer_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if transformer_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = transformer_result.get('fai', [])
    detection_info = transformer_result.get('detection_info', {})
    thresholds = transformer_result.get('thresholds', {})
    threshold1 = thresholds.get('threshold1', 0.0)
    
    time_axis = np.arange(len(fai_values))
    
    # ç»˜åˆ¶FAIæ—¶åº
    ax_main.plot(time_axis, fai_values, 'b-', linewidth=1.5, alpha=0.8, label='ç»¼åˆè¯Šæ–­æŒ‡æ ‡ Ï†(FAI)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.7, label='ä¸€çº§é˜ˆå€¼')
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        candidate_points = detection_info['candidate_points']
        ax_main.scatter(candidate_points, [fai_values[i] for i in candidate_points],
                       color='orange', s=40, alpha=0.8, label=f'æ£€æµ‹: {len(candidate_points)} ä¸ªå€™é€‰ç‚¹',
                       marker='o', zorder=5)
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax_main.scatter(verified_indices, [fai_values[i] for i in verified_indices],
                       color='red', s=60, alpha=0.9, label=f'éªŒè¯: {len(verified_indices)} ä¸ªç¡®è®¤ç‚¹',
                       marker='^', zorder=6)
        
        # æ˜¾ç¤ºéªŒè¯çª—å£èŒƒå›´
        for v_point in detection_info['verified_points']:
            verify_start, verify_end = v_point['verify_range']
            ax_main.axvspan(verify_start, verify_end, alpha=0.1, color='yellow')
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ•…éšœåŒºåŸŸ
    fault_regions_plotted = set()  # é¿å…é‡å¤ç»˜åˆ¶å›¾ä¾‹
    for i, region in enumerate(detection_info['marked_regions']):
        start, end = region['range']
        label = 'æ ‡è®°: æ•…éšœåŒºåŸŸ' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('æ—¶é—´æ­¥é•¿')
    ax_main.set_ylabel('ç»¼åˆè¯Šæ–­æŒ‡æ ‡ Ï†')
    ax_main.set_title(f'Transformerä¸‰çª—å£æ•…éšœæ£€æµ‹è¿‡ç¨‹ - æ ·æœ¬ {fault_sample_id}', 
                     fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left')
    ax_main.grid(True, alpha=0.3)
    
    # === å­å›¾1ï¼šæ£€æµ‹çª—å£ç»Ÿè®¡ ===
    ax1 = fig.add_subplot(gs[1, 0])
    
    window_stats = detection_info['window_stats']
    detection_data = [
        window_stats['total_candidates'],
        window_stats['verified_candidates'], 
        window_stats['total_fault_points']
    ]
    detection_labels = ['å€™é€‰ç‚¹', 'éªŒè¯ç‚¹', 'æ•…éšœç‚¹']
    colors1 = ['orange', 'red', 'darkred']
    
    bars1 = ax1.bar(detection_labels, detection_data, color=colors1, alpha=0.7)
    ax1.set_title('æ£€æµ‹ç»Ÿè®¡')
    ax1.set_ylabel('æ•°é‡')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, detection_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(value), ha='center', va='bottom')
    
    # === å­å›¾2ï¼šçª—å£å‚æ•°é…ç½® ===
    ax2 = fig.add_subplot(gs[1, 1])
    
    window_params = [
        WINDOW_CONFIG['detection_window'],
        WINDOW_CONFIG['verification_window'],
        WINDOW_CONFIG['marking_window']
    ]
    window_labels = ['æ£€æµ‹çª—å£\n(25)', 'éªŒè¯çª—å£\n(15)', 'æ ‡è®°çª—å£\n(10)']
    colors2 = ['lightblue', 'lightgreen', 'lightcoral']
    
    wedges, texts, autotexts = ax2.pie(window_params, labels=window_labels, colors=colors2,
                                      autopct='%1.0f', startangle=90)
    ax2.set_title('çª—å£å¤§å°\n(é‡‡æ ·ç‚¹æ•°)')
    
    # === å­å›¾3ï¼šéªŒè¯çª—å£è¯¦æƒ… ===
    ax3 = fig.add_subplot(gs[1, 2])
    
    if detection_info['verified_points']:
        continuous_ratios = [v['continuous_ratio'] for v in detection_info['verified_points']]
        verify_points = [v['point'] for v in detection_info['verified_points']]
        
        bars3 = ax3.bar(range(len(continuous_ratios)), continuous_ratios, 
                       color='green', alpha=0.7)
        ax3.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='é˜ˆå€¼ (30%)')
        ax3.set_title('éªŒè¯æ¯”ç‡')
        ax3.set_xlabel('éªŒè¯ç‚¹')
        ax3.set_ylabel('è¿ç»­æ¯”ç‡')
        ax3.set_xticks(range(len(continuous_ratios)))
        ax3.set_xticklabels([f'P{i+1}' for i in range(len(continuous_ratios))])
        ax3.legend()
    else:
        ax3.text(0.5, 0.5, 'æ— éªŒè¯ç‚¹', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('éªŒè¯æ¯”ç‡')
    
    # === å­å›¾4ï¼šTransformeræ€§èƒ½ ===
    ax4 = fig.add_subplot(gs[1, 3])
    
    sample_result = next((r for r in test_results['TRANSFORMER'] if r.get('sample_id') == fault_sample_id), None)
    if sample_result is None:
        sample_result = test_results['TRANSFORMER'][0] if test_results['TRANSFORMER'] else None
    
    if sample_result is None:
        fault_ratio = 0.0
    else:
        detection_info = sample_result.get('detection_info', {})
        window_stats = detection_info.get('window_stats', {})
        fault_ratio = window_stats.get('fault_ratio', 0.0)
    
    bars4 = ax4.bar(['Transformer'], [fault_ratio], color='blue', alpha=0.7)
    ax4.set_title('Transformer\n(æ•…éšœæ£€æµ‹æ¯”ç‡)')
    ax4.set_ylabel('æ•…éšœæ¯”ç‡')
    
    for bar, value in zip(bars4, [fault_ratio]):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === åº•éƒ¨ï¼šè¿‡ç¨‹è¯´æ˜ ===
    process_text = """
    Transformerä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹:
    
    1. æ£€æµ‹çª—å£ (25ç‚¹): æ‰«æå€™é€‰æ•…éšœç‚¹ï¼Œæ¡ä»¶ï¼šÏ†(FAI) > é˜ˆå€¼
    2. éªŒè¯çª—å£ (15ç‚¹): éªŒè¯å€™é€‰ç‚¹ï¼Œæ£€æŸ¥è¿ç»­æ€§ (â‰¥60% è¶…é˜ˆå€¼)
    3. æ ‡è®°çª—å£ (Â±10ç‚¹): æ ‡è®°ç¡®è®¤çš„æ•…éšœåŒºåŸŸ
    
    ä¼˜åŠ¿: åœ¨ä¿æŒé«˜æ•æ„Ÿæ€§çš„åŒæ—¶å‡å°‘è¯¯æŠ¥
    """
    
    fig.text(0.02, 0.02, process_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… Transformerä¸‰çª—å£è¿‡ç¨‹å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»“æœä¿å­˜å‡½æ•°------------------------------
def save_test_results(test_results, performance_metrics):
    """ä¿å­˜Transformeræµ‹è¯•ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜Transformeræµ‹è¯•ç»“æœ...")
    
    # åˆ›å»ºç»“æœç›®å½• - ç»Ÿä¸€ä¿å­˜åˆ°modelsç›®å½•ä¸‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = f"/mnt/bz25t/bzhy/datasave/Transformer/models/test_results_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(f"{result_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{result_dir}/detailed_results", exist_ok=True)
    
    # 1. ä¿å­˜æ€§èƒ½æŒ‡æ ‡JSON
    performance_file = f"{result_dir}/transformer_performance_metrics.json"
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræ€§èƒ½æŒ‡æ ‡ä¿å­˜è‡³: {performance_file}")
    
    # 2. ä¿å­˜è¯¦ç»†ç»“æœ
    detail_file = f"{result_dir}/detailed_results/transformer_detailed_results.pkl"
    with open(detail_file, 'wb') as f:
        pickle.dump(test_results["TRANSFORMER"], f)
    print(f"   âœ… Transformerè¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
    
    # 3. ä¿å­˜å…ƒæ•°æ®
    metadata_file = f"{result_dir}/detailed_results/transformer_test_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(test_results['metadata'], f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræµ‹è¯•å…ƒæ•°æ®ä¿å­˜è‡³: {metadata_file}")
    
    # 4. åˆ›å»ºExcelæ€»ç»“æŠ¥å‘Š
    summary_file = f"{result_dir}/detailed_results/transformer_summary.xlsx"
    
    with pd.ExcelWriter(summary_file) as writer:
        # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
        if "TRANSFORMER" not in performance_metrics:
            print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
            return result_dir
        
        metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
        confusion = performance_metrics["TRANSFORMER"]['confusion_matrix']
        sample_metrics = performance_metrics["TRANSFORMER"]['sample_metrics']
        
        performance_data = [{
            'Model': 'TRANSFORMER',
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'Specificity': metrics['specificity'],
            'TPR': metrics['tpr'],
            'FPR': metrics['fpr'],
            'TP': confusion['TP'],
            'FP': confusion['FP'],
            'TN': confusion['TN'],
            'FN': confusion['FN'],
            'Avg_FAI_Normal': sample_metrics['avg_fai_normal'],
            'Avg_FAI_Fault': sample_metrics['avg_fai_fault']
        }]
        
        performance_df = pd.DataFrame(performance_data)
        performance_df.to_excel(writer, sheet_name='Transformer_Performance', index=False)
        
        # æ ·æœ¬è¯¦æƒ…è¡¨
        sample_details = []
        for result in test_results["TRANSFORMER"]:
            # å®‰å…¨è·å–detection_infoå’Œwindow_stats
            detection_info = result.get('detection_info', {})
            window_stats = detection_info.get('window_stats', {})
            performance_metrics = result.get('performance_metrics', {})
            
            sample_details.append({
                'Sample_ID': result.get('sample_id', 'Unknown'),
                'True_Label': 'Fault' if result.get('label', 0) == 1 else 'Normal',
                'FAI_Mean': performance_metrics.get('fai_mean', 0.0),
                'FAI_Std': performance_metrics.get('fai_std', 0.0),
                'FAI_Max': performance_metrics.get('fai_max', 0.0),
                'Anomaly_Ratio': performance_metrics.get('anomaly_ratio', 0.0),
                'Fault_Detection_Ratio': window_stats.get('fault_ratio', 0.0),
                'Candidates_Found': window_stats.get('total_candidates', 0),
                'Verified_Points': window_stats.get('verified_candidates', 0)
            })
        
        sample_df = pd.DataFrame(sample_details)
        sample_df.to_excel(writer, sheet_name='Sample_Details', index=False)
    
    print(f"   âœ… Transformer Excelæ€»ç»“æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")
    
    return result_dir

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
performance_metrics = calculate_performance_metrics(test_results)

# ä¿å­˜æµ‹è¯•ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
result_dir = save_test_results(test_results, performance_metrics)

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
print("\nğŸ¨ ç”ŸæˆTransformerå¯è§†åŒ–åˆ†æ...")

# ç”ŸæˆROCåˆ†æå›¾
create_roc_analysis(test_results, performance_metrics, f"{result_dir}/visualizations/transformer_roc_analysis.png")

# ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾
create_fault_detection_timeline(test_results, f"{result_dir}/visualizations/transformer_fault_detection_timeline.png")

# ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
create_performance_radar(performance_metrics, f"{result_dir}/visualizations/transformer_performance_radar.png")

# ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å›¾
create_three_window_visualization(test_results, f"{result_dir}/visualizations/transformer_three_window_process.png")

#----------------------------------------æœ€ç»ˆæ€»ç»“------------------------------
print("\n" + "="*80)
print("ğŸ‰ Transformeræ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(ALL_TEST_SAMPLES)} ä¸ª (æ­£å¸¸: {len(TEST_SAMPLES['normal'])}, æ•…éšœ: {len(TEST_SAMPLES['fault'])})")
print(f"   â€¢ æ¨¡å‹ç±»å‹: Transformer")
print(f"   â€¢ æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")

print(f"\nğŸ”¬ Transformeræ€§èƒ½:")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   â€¢ çª—å£é…ç½®: æ£€æµ‹({WINDOW_CONFIG['detection_window']}) â†’ éªŒè¯({WINDOW_CONFIG['verification_window']}) â†’ æ ‡è®°({WINDOW_CONFIG['marking_window']})")
    else:
    print(f"   â€¢ 5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")
print(f"   TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   â€¢ ç»“æœç›®å½•: {result_dir}")
print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {result_dir}/visualizations")
print(f"     - ROCåˆ†æå›¾: transformer_roc_analysis.png")
print(f"     - æ•…éšœæ£€æµ‹æ—¶åºå›¾: transformer_fault_detection_timeline.png") 
print(f"     - æ€§èƒ½é›·è¾¾å›¾: transformer_performance_radar.png")
print(f"     - ä¸‰çª—å£è¿‡ç¨‹å›¾: transformer_three_window_process.png")
print(f"   â€¢ æ€§èƒ½æŒ‡æ ‡: transformer_performance_metrics.json")
print(f"   â€¢ ExcelæŠ¥å‘Š: transformer_summary.xlsx")

# ç»¼åˆæ€§èƒ½è¯„ä¼°
transformer_score = np.mean(list(performance_metrics["TRANSFORMER"]['classification_metrics'].values()))

print(f"\nğŸ† Transformerç»¼åˆæ€§èƒ½è¯„ä¼°:")
    print(f"   ç»¼åˆå¾—åˆ†: {transformer_score:.3f}")

print("\n" + "="*80)
print("Transformeræµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
print("="*80)