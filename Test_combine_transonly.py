# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from Comprehensive_calculation import Comprehensive_calculation

# æ–°å¢å¯¼å…¥
from tqdm import tqdm
import json
import time
from datetime import datetime
from sklearn.metrics import roc_curve, auc, confusion_matrix
import glob

# æ·»åŠ æ¨¡å‹åŠ è½½è¾…åŠ©å‡½æ•°
def remove_module_prefix(state_dict):
    """ç§»é™¤state_dictä¸­çš„module.å‰ç¼€"""
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('module.'):
            new_key = key[7:]  # ç§»é™¤'module.'å‰ç¼€
        else:
            new_key = key
        new_state_dict[new_key] = value
    return new_state_dict

def safe_get_nested(dictionary, keys, default=None):
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸çš„å€¼"""
    try:
        for key in keys:
            dictionary = dictionary[key]
        return dictionary
    except (KeyError, TypeError, IndexError):
        return default

def safe_load_model(model, model_path, model_name):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†DataParallelå‰ç¼€é—®é¢˜"""
    try:
        print(f"   Loading {model_name} model: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"   âŒ Model file not found: {model_path}")
            return False
        
        state_dict = torch.load(model_path, map_location=device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤moduleå‰ç¼€
        has_module_prefix = any(key.startswith('module.') for key in state_dict.keys())
        if has_module_prefix:
            print(f"   Detected DataParallel prefix, removing...")
            state_dict = remove_module_prefix(state_dict)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„åŒ¹é…
        model_state_dict = model.state_dict()
        missing_keys = []
        unexpected_keys = []
        
        for key in model_state_dict.keys():
            if key not in state_dict:
                missing_keys.append(key)
        
        for key in state_dict.keys():
            if key not in model_state_dict:
                unexpected_keys.append(key)
        
        if missing_keys:
            print(f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys[:5]}..." if len(missing_keys) > 5 else f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys}")
        
        if unexpected_keys:
            print(f"   âš ï¸  å¤šä½™é”®: {unexpected_keys[:5]}..." if len(unexpected_keys) > 5 else f"   âš ï¸  å¤šä½™é”®: {unexpected_keys}")
        
        # å°è¯•åŠ è½½
        model.load_state_dict(state_dict, strict=False)
        print(f"   âœ… {model_name} model loaded successfully")
        return True
        
    except Exception as e:
        print(f"   âŒ {model_name} model loading failed: {e}")
        print(f"   Error type: {type(e).__name__}")
        return False

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# GPUé…ç½®æ£€æŸ¥
print("ğŸ–¥ï¸ GPU Configuration Check:")
print(f"   CUDA Available: {torch.cuda.is_available()}")
print(f"   GPU Count: {torch.cuda.device_count()}")
print(f"   Current Device: {device}")

if torch.cuda.is_available():
    print("   GPU Details:")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"     GPU {i}: {props.name}")
        print(f"       Memory: {props.total_memory/1024**3:.1f}GB")
        print(f"       è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print("   âš ï¸ Using CPU mode")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®é»˜è®¤è‹±æ–‡å­—ä½“æ˜¾ç¤º
from matplotlib import rcParams

def setup_english_fonts():
    """è®¾ç½®è‹±æ–‡å­—ä½“é…ç½®"""
    # ä½¿ç”¨é»˜è®¤è‹±æ–‡å­—ä½“é…ç½®
    rcParams['font.family'] = 'sans-serif'
    rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
    rcParams['axes.unicode_minus'] = False
    rcParams['pdf.fonttype'] = 42
    rcParams['ps.fonttype'] = 42
    rcParams['savefig.dpi'] = 300
    rcParams['figure.dpi'] = 100
    rcParams['figure.autolayout'] = False
    rcParams['axes.titlesize'] = 12
    rcParams['axes.labelsize'] = 10
    rcParams['legend.fontsize'] = 9
    rcParams['xtick.labelsize'] = 9
    rcParams['ytick.labelsize'] = 9
    
    print("âœ… Using English fonts configuration")
    return True

# å¸¸ç”¨å›¾è¡¨æ ‡ç­¾å­—å…¸ï¼ˆä¸­è‹±æ–‡å¯¹ç…§ï¼‰
CHART_LABELS = {
    'Ï†æŒ‡æ ‡å€¼': 'Ï† Index Value',
    'è§¦å‘ç‚¹': 'Trigger Points', 
    'éªŒè¯': 'Verified',
    'ä¸ªç¡®è®¤ç‚¹': ' Confirmed Points',
    'ç¡®è®¤ç‚¹': 'Confirmed Points',
    'æ•…éšœåŒºåŸŸ': 'Fault Region',
    'æ£€æµ‹è¿‡ç¨‹': 'Detection Process',
    'æ—¶é—´æ­¥': 'Time Step',
    'æ— æ•°æ®': 'No Data',
    'æ­£å¸¸': 'Normal',
    'æ•…éšœ': 'Fault',
    'å¼‚å¸¸': 'Anomaly',
    'æ£€æµ‹': 'Detection',
    'è¯Šæ–­': 'Diagnosis',
    'æŒ‡æ ‡': 'Index',
    'é˜ˆå€¼': 'Threshold',
    'é‡‡æ ·ç‚¹': 'Sample Points',
    'å¯åŠ¨æœŸ': 'Startup Period',
    'æœ‰æ•ˆåŒºåŸŸ': 'Effective Region',
    'é™å™ªç‡': 'Noise Reduction Rate',
    'è¯¯æŠ¥': 'False Positive',
    'æ¼æŠ¥': 'False Negative',
    'æ ‡è®°': 'Marked',
    'ç»¼åˆè¯Šæ–­æŒ‡æ ‡': 'Comprehensive Diagnostic Index',
    'ä¸‰ç‚¹æ£€æµ‹': 'Three-Point Detection',
    'å€™é€‰ç‚¹': 'Candidate Points',
    'ç»Ÿè®¡ä¿¡æ¯': 'Statistics',
    'ç­‰çº§åˆ†å¸ƒ': 'Level Distribution',
    'è§¦å‘ç­‰çº§': 'Trigger Level',
    'è§¦å‘æ¬¡æ•°': 'Trigger Count',
    'åˆ†å±‚': 'Hierarchy',
    'å·¥ä½œç‚¹': 'Working Point',
    'åˆ†ç±»æŒ‡æ ‡': 'Classification Metrics',
    'æ ·æœ¬çº§æ€§èƒ½': 'Sample-Level Performance',
    'æ ‡è®°çš„': 'Marked:',
    'æµ‹è¯•ä¸­æ–‡': 'Test Chinese'
}

def get_chart_label(chinese_key):
    """è¿”å›è‹±æ–‡å›¾è¡¨æ ‡ç­¾"""
    return CHART_LABELS.get(chinese_key, chinese_key)  # å§‹ç»ˆè¿”å›è‹±æ–‡æ ‡ç­¾

# æ‰§è¡Œè‹±æ–‡å­—ä½“é…ç½®
setup_english_fonts()

#----------------------------------------æ•°æ®é¢„å¤„ç†å‡½æ•°------------------------------
def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
            
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,1]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 1)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 1, col] = 1
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šä¸“ç”¨æ•°æ®åˆ— - ä¿æŒåŸå€¼ä¸å¤„ç†
        # (æ ¹æ®éªŒè¯ç»“æœï¼Œè¿™ä¸€åˆ—åŒ…å«ç‰¹æ®Šæ•°æ®ï¼Œä¸è¿›è¡Œå¤„ç†)
        
        # ç´¢å¼•225ï¼šæ–°å¢çš„ç¬¬4ç»´ç‰¹å¾ - é™åˆ¶åœ¨[0,1]
        feature4_col = 225
        feature4_valid_mask = (data_np[:, feature4_col] >= 0) & (data_np[:, feature4_col] <= 1)
        feature4_invalid_count = (~feature4_valid_mask).sum()
        if feature4_invalid_count > 0:
            data_np[data_np[:, feature4_col] < 0, feature4_col] = 0
            data_np[data_np[:, feature4_col] > 1, feature4_col] = 1
    
    # 3. è¿›ä¸€æ­¥å¤„ç†æ®‹ç•™çš„NaN/Infå€¼
    # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šæ›¿æ¢ä¸ºå…¨å±€ä¸­ä½æ•°
    if np.isnan(data_np).any() or np.isinf(data_np).any():
        for col in range(data_np.shape[1]):
            col_data = data_np[:, col]
            
            # è·³è¿‡ç‰¹æ®Šåˆ—
            if data_np.shape[1] == 226 and col == 224:
                continue
            
            # å¤„ç†NaN
            if np.isnan(col_data).any():
                valid_mask = ~np.isnan(col_data)
                if valid_mask.any():
                    median_val = np.median(col_data[valid_mask])
                    data_np[~valid_mask, col] = median_val
                else:
                    data_np[:, col] = 0.0
            
            # å¤„ç†Inf
            if np.isinf(col_data).any():
                inf_mask = np.isinf(col_data)
                finite_mask = np.isfinite(col_data)
                if finite_mask.any():
                    # æ­£æ— ç©·æ›¿æ¢ä¸ºæœ€å¤§æœ‰é™å€¼ï¼Œè´Ÿæ— ç©·æ›¿æ¢ä¸ºæœ€å°æœ‰é™å€¼
                    max_finite = np.max(col_data[finite_mask])
                    min_finite = np.min(col_data[finite_mask])
                    data_np[col_data == np.inf, col] = max_finite
                    data_np[col_data == -np.inf, col] = min_finite
                else:
                    data_np[inf_mask, col] = 0.0
    
    # ç¡®ä¿æ²¡æœ‰æ®‹ç•™çš„å¼‚å¸¸å€¼
    assert not np.isnan(data_np).any(), f"Still have NaN values after processing"
    assert not np.isinf(data_np).any(), f"Still have Inf values after processing"
    
    # è½¬æ¢å›åŸå§‹æ•°æ®ç±»å‹
    if isinstance(data, torch.Tensor):
        return torch.tensor(data_np, dtype=data.dtype, device=data.device)
    else:
        return data_np

#----------------------------------------æµ‹è¯•é…ç½®------------------------------
print("="*60)
print("ğŸ”¬ Battery Fault Diagnosis System - Transformer Model Testing (Hybrid Feedback Version)")
print("="*60)

TEST_MODE = "TRANSFORMER_ONLY"  # å›ºå®šä¸ºTransformerå•æ¨¡å‹æµ‹è¯•

# æµ‹è¯•æ•°æ®é›†é…ç½® (æ ¹æ®Labels.xlsåŠ¨æ€åŠ è½½)
def load_test_samples():
    """ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬"""
    try:
        import pandas as pd
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–æµ‹è¯•æ ·æœ¬
        all_samples = df['Num'].tolist()
        all_labels = df['Label'].tolist()
        
        # æŒ‡å®šæµ‹è¯•æ ·æœ¬ï¼šæ­£å¸¸æ ·æœ¬10-20 å’Œæ•…éšœæ ·æœ¬340-350
        test_normal_samples = [str(i) for i in range(10, 21)]  # æ­£å¸¸æ ·æœ¬ï¼š10-20
        test_fault_samples = [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        
        print(f"ğŸ“‹ Loading test samples from Labels.xls:")
        print(f"   æµ‹è¯•æ­£å¸¸æ ·æœ¬: {test_normal_samples}")
        print(f"   æµ‹è¯•æ•…éšœæ ·æœ¬: {test_fault_samples}")
        
        return {
            'normal': test_normal_samples,
            'fault': test_fault_samples
        }
    except Exception as e:
        print(f"âŒ Failed to load Labels.xls: {e}")
        print("âš ï¸  Using default test samples")
        return {
            'normal': [str(i) for i in range(10, 21)],  # æ­£å¸¸æ ·æœ¬ï¼š10-20
            'fault': [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        }

TEST_SAMPLES = load_test_samples()
ALL_TEST_SAMPLES = TEST_SAMPLES['normal'] + TEST_SAMPLES['fault']

# æ¨¡å‹è·¯å¾„é…ç½® (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
MODEL_PATHS = {
    "TRANSFORMER": {
        "transformer_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/transformer_model_hybrid_feedback.pth",
        "net_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/net_model_hybrid_feedback.pth", 
        "netx_model": "/mnt/bz25t/bzhy/datasave/Transformer/models/netx_model_hybrid_feedback.pth"
    }
}

# æ£€æµ‹æ¨¡å¼é…ç½®
DETECTION_MODES = {
    "three_window": {
        "name": "Three-Window Detection Mode",
        "description": "Three-window fault detection mechanism based on FAI (Detection->Verification->Marking)",
        "function": "three_window_fault_detection"
    },
    "five_point": {
        "name": "5-Point Detection Mode (Original)", 
        "description": "For fault samples, if a point exceeds threshold and adjacent points also exceed threshold, mark that point and 2 points before/after (total 5 points)",
        "function": "five_point_fault_detection"
    },
    "five_point_improved": {
        "name": "5-Point Detection Mode (Improved)",
        "description": "Improved 5-point detection: Strict trigger conditions + Graded marking range + Effective noise reduction",
        "function": "five_point_fault_detection"
    }
}

# å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ¨¡å¼
CURRENT_DETECTION_MODE = "five_point_improved"  # ä½¿ç”¨æ”¹è¿›çš„5ç‚¹æ£€æµ‹æ¨¡å¼

# åŸºäºFAIçš„ä¸‰çª—å£æ£€æµ‹é…ç½® (ä¸BiLSTMæµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´)
WINDOW_CONFIG = {
    "detection_window": 25,      # æ£€æµ‹çª—å£ï¼š25ä¸ªé‡‡æ ·ç‚¹ (12.5åˆ†é’Ÿ)
    "verification_window": 15,   # éªŒè¯çª—å£ï¼š15ä¸ªé‡‡æ ·ç‚¹ (7.5åˆ†é’Ÿ)
    "marking_window": 10,        # æ ‡è®°çª—å£ï¼š10ä¸ªé‡‡æ ·ç‚¹ (5åˆ†é’Ÿ)
    "verification_threshold": 0.6 # éªŒè¯çª—å£å†…FAIå¼‚å¸¸æ¯”ä¾‹é˜ˆå€¼ (60%)
}

# é«˜åˆ†è¾¨ç‡å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (15, 12),
    "figsize_medium": (12, 8), 
    "bbox_inches": "tight"
}

print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   æµ‹è¯•æ ·æœ¬: {ALL_TEST_SAMPLES}")
print(f"   æ­£å¸¸æ ·æœ¬: {TEST_SAMPLES['normal']}")
print(f"   æ•…éšœæ ·æœ¬: {TEST_SAMPLES['fault']}")
print(f"   æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   ä¸‰çª—å£å‚æ•°: {WINDOW_CONFIG}")
else:
    print(f"   5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

#----------------------------------------æ¨¡å‹æ–‡ä»¶æ£€æŸ¥------------------------------
def check_model_files():
    """æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥Transformeræ¨¡å‹æ–‡ä»¶...")
    
    missing_files = []
    paths = MODEL_PATHS["TRANSFORMER"]
    
    # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶
    for key, path in paths.items():
        if not os.path.exists(path):
            missing_files.append(f"TRANSFORMER: {path}")
            print(f"   âŒ ç¼ºå¤±: {path}")
        else:
            file_size = os.path.getsize(path) / (1024 * 1024)  # MB
            print(f"   âœ… å­˜åœ¨: {path} ({file_size:.1f}MB)")
    
    # æ£€æŸ¥PCAå‚æ•°æ–‡ä»¶ (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params_hybrid_feedback.pkl"
    if not os.path.exists(pca_params_path):
        print(f"   âš ï¸  PCAå‚æ•°pickleæ–‡ä»¶ä¸å­˜åœ¨: {pca_params_path}")
        print(f"   ğŸ” æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶...")
        
        # æ£€æŸ¥å¤‡é€‰çš„npyæ–‡ä»¶
        pca_npy_files = [
            "/mnt/bz25t/bzhy/datasave/Transformer/models/v_I_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_mean_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/data_std_hybrid_feedback.npy",
            "/mnt/bz25t/bzhy/datasave/Transformer/models/T_99_limit_hybrid_feedback.npy"
        ]
        
        npy_files_exist = 0
        for npy_file in pca_npy_files:
            if os.path.exists(npy_file):
                npy_files_exist += 1
                file_size = os.path.getsize(npy_file) / (1024 * 1024)
                print(f"   âœ… å­˜åœ¨: {npy_file} ({file_size:.1f}MB)")
            else:
                print(f"   âŒ ç¼ºå¤±: {npy_file}")
        
        if npy_files_exist >= 3:
            print(f"   âœ… å‘ç°{npy_files_exist}ä¸ªnpyæ–‡ä»¶ï¼Œå¯ä»¥é‡å»ºPCAå‚æ•°")
        else:
            missing_files.append(f"PCA_PARAMS: ç¼ºå°‘è¶³å¤Ÿçš„npyæ–‡ä»¶")
    else:
        file_size = os.path.getsize(pca_params_path) / (1024 * 1024)  # MB
        print(f"   âœ… å­˜åœ¨: {pca_params_path} ({file_size:.1f}MB)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå¤± {len(missing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
        for file in missing_files:
            print(f"   {file}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿å·²è¿è¡ŒTransformerè®­ç»ƒè„šæœ¬")
        print("   2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥æ–‡ä»¶æƒé™")
        raise FileNotFoundError("è¯·å…ˆè¿è¡ŒTransformerè®­ç»ƒè„šæœ¬ç”Ÿæˆæ‰€éœ€æ¨¡å‹æ–‡ä»¶")
    
    print("âœ… Transformeræ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

# æ‰§è¡Œæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
check_model_files()

#----------------------------------------ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶------------------------------
def three_window_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    åŸºäºFAIçš„ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶
    
    åŸç†ï¼š
    1. æ£€æµ‹çª—å£ï¼šåŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    2. éªŒè¯çª—å£ï¼šç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§ï¼Œæ’é™¤éšæœºæ³¢åŠ¨
    3. æ ‡è®°çª—å£ï¼šè€ƒè™‘æ•…éšœçš„å‰åå½±å“èŒƒå›´
    
    Args:
        fai_values: FAIåºåˆ—ï¼ˆç»¼åˆæ•…éšœæŒ‡æ ‡ï¼‰
        threshold1: FAIé˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè®°å½•ï¼‰
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # è·å–çª—å£é…ç½®
    if config is None:
        config = WINDOW_CONFIG
        
    detection_window = config["detection_window"]
    verification_window = config["verification_window"]
    marking_window = config["marking_window"]
    verification_threshold = config["verification_threshold"]
    
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'candidate_points': [],    # å€™é€‰æ•…éšœç‚¹
        'verified_points': [],     # å·²éªŒè¯çš„æ•…éšœç‚¹
        'marked_regions': [],      # æ ‡è®°çš„æ•…éšœåŒºåŸŸ
        'window_stats': {},        # çª—å£ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        }
    }
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - åŸºäºFAIç»Ÿè®¡ç‰¹æ€§è¯†åˆ«å¼‚å¸¸ç‚¹
    candidate_points = []
    for i in range(len(fai_values)):
        if fai_values[i] > threshold1:
            candidate_points.append(i)
    
    detection_info['candidate_points'] = candidate_points
    
    if len(candidate_points) == 0:
        # æ²¡æœ‰å€™é€‰ç‚¹ï¼Œç›´æ¥è¿”å›
        return fault_labels, detection_info
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - ç¡®è®¤FAIå¼‚å¸¸çš„æŒç»­æ€§
    verified_points = []
    for candidate in candidate_points:
        # å®šä¹‰éªŒè¯çª—å£èŒƒå›´ï¼ˆå‰åå„åŠä¸ªçª—å£ï¼‰
        start_verify = max(0, candidate - verification_window//2)
        end_verify = min(len(fai_values), candidate + verification_window//2)
        verify_data = fai_values[start_verify:end_verify]
        
        # è®¡ç®—FAIå¼‚å¸¸æ¯”ä¾‹
        continuous_ratio = np.sum(verify_data > threshold1) / len(verify_data)
        
        # è®¡ç®—FAIåœ¨éªŒè¯çª—å£çš„ç»Ÿè®¡ç‰¹æ€§
        window_stats = {
            'mean': np.mean(verify_data),
            'std': np.std(verify_data),
            'max': np.max(verify_data),
            'min': np.min(verify_data),
            'duration': end_verify - start_verify
        }
        
        # åŸºäºverification_thresholdéªŒè¯æŒç»­æ€§
        if continuous_ratio >= verification_threshold:
            verified_points.append({
                'point': candidate,
                'continuous_ratio': continuous_ratio,
                'verify_range': (start_verify, end_verify),
                'window_stats': window_stats
            })
    
    detection_info['verified_points'] = verified_points
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - è€ƒè™‘æ•…éšœçš„å½±å“èŒƒå›´
    marked_regions = []
    for verified in verified_points:
        candidate = verified['point']
        
        # å®šä¹‰å¯¹ç§°çš„æ ‡è®°çª—å£èŒƒå›´
        start_mark = max(0, candidate - marking_window)
        end_mark = min(len(fai_values), candidate + marking_window)
        
        # æå–æ ‡è®°åŒºåŸŸçš„FAIç‰¹å¾
        mark_data = fai_values[start_mark:end_mark]
        region_stats = {
            'mean_fai': np.mean(mark_data),
            'max_fai': np.max(mark_data),
            'std_fai': np.std(mark_data),
            'duration': end_mark - start_mark
        }
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start_mark:end_mark] = 1
        
        marked_regions.append({
            'center': candidate,
            'range': (start_mark, end_mark),
            'length': end_mark - start_mark,
            'region_stats': region_stats
        })
    
    detection_info['marked_regions'] = marked_regions
    
    # å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯
    detection_info['window_stats'] = {
        'total_candidates': len(candidate_points),
        'verified_candidates': len(verified_points),
        'total_fault_points': np.sum(fault_labels),
        'fault_ratio': np.sum(fault_labels) / len(fault_labels),
        'mean_continuous_ratio': np.mean([v['continuous_ratio'] for v in verified_points]) if verified_points else 0,
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0
    }
    
    return fault_labels, detection_info

def five_point_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    æ”¹è¿›çš„5ç‚¹æ•…éšœæ£€æµ‹æœºåˆ¶ï¼šåŸºäºæºä»£ç è®¾è®¡ï¼Œä»3001ç‚¹å¼€å§‹æ£€æµ‹
    
    è®¾è®¡åŸç†ï¼ˆç¬¦åˆæºä»£ç æ€è·¯ï¼‰ï¼š
    1. å‰3000ç‚¹ä¸ºç³»ç»Ÿå¯åŠ¨/ä¸ç¨³å®šæœŸï¼Œä¸è¿›è¡Œæ•…éšœæ£€æµ‹
    2. ä»ç¬¬3001ç‚¹å¼€å§‹åº”ç”¨ä¸‰çº§åˆ†å±‚æ£€æµ‹æœºåˆ¶
    3. ä¸é˜ˆå€¼è®¡ç®—åŸºçº¿ä¿æŒä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨3000ç‚¹åçš„æ•°æ®ï¼‰
    
    Args:
        fai_values: ç»¼åˆè¯Šæ–­æŒ‡æ ‡åºåˆ—
        threshold1: ä¸€çº§é¢„è­¦é˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè°ƒè¯•ï¼‰
        config: é…ç½®å‚æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå¯åŒ…å«threshold2, threshold3ï¼‰
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=è½»å¾®æ•…éšœ, 2=ä¸­ç­‰æ•…éšœ, 3=ä¸¥é‡æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šç¬¦åˆæºä»£ç è®¾è®¡ï¼Œå‰3000ç‚¹ä¸æ£€æµ‹
    STARTUP_PERIOD = 3000  # æºä»£ç ä¸­çš„nmå€¼ï¼Œå¯åŠ¨/ä¸ç¨³å®šæœŸ
    
    # æå‰å®šä¹‰æœ‰æ•ˆåŒºåŸŸå˜é‡ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
    effective_fai = fai_values[STARTUP_PERIOD:] if len(fai_values) > STARTUP_PERIOD else fai_values
    
    # åˆå§‹åŒ–è¾“å‡º
    fault_labels = np.zeros(len(fai_values), dtype=int)
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    detection_info = {
        'trigger_points': [],      # è§¦å‘5ç‚¹æ£€æµ‹çš„ç‚¹
        'marked_regions': [],      # æ ‡è®°çš„5ç‚¹åŒºåŸŸ
        'detection_stats': {},     # æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
        'fai_stats': {            # FAIç»Ÿè®¡ä¿¡æ¯
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        },
        'startup_period': STARTUP_PERIOD,  # è®°å½•å¯åŠ¨æœŸé•¿åº¦
        'effective_detection_length': max(0, len(fai_values) - STARTUP_PERIOD)  # æœ‰æ•ˆæ£€æµ‹é•¿åº¦
    }
    
    # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ
    if len(fai_values) <= STARTUP_PERIOD:
        print(f"   âš ï¸ è­¦å‘Šï¼šæ•°æ®é•¿åº¦({len(fai_values)})ä¸è¶³å¯åŠ¨æœŸ({STARTUP_PERIOD})ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆæ£€æµ‹")
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'insufficient_data',
            'skip_reason': f'data_length_{len(fai_values)}_less_than_startup_{STARTUP_PERIOD}'
        }
        return fault_labels, detection_info
    
    print(f"   ğŸ“Š æ£€æµ‹é…ç½®ï¼šè·³è¿‡å‰{STARTUP_PERIOD}ç‚¹ï¼ˆå¯åŠ¨æœŸï¼‰ï¼Œä»ç¬¬{STARTUP_PERIOD+1}ç‚¹å¼€å§‹æ£€æµ‹")
    print(f"   ğŸ“Š æœ‰æ•ˆæ£€æµ‹é•¿åº¦ï¼š{len(fai_values) - STARTUP_PERIOD}ç‚¹")
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ ·æœ¬IDç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    # å°†sample_idè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
    sample_id_str = str(sample_id)
    is_fault_sample = sample_id_str in TEST_SAMPLES['fault']
    
    # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    print(f"   ğŸ“Š æ ·æœ¬åˆ†ç±»æ£€æŸ¥:")
    print(f"      åŸå§‹sample_id: {sample_id} (ç±»å‹: {type(sample_id)})")
    print(f"      å­—ç¬¦ä¸²sample_id: {sample_id_str}")
    print(f"      æ•…éšœæ ·æœ¬åˆ—è¡¨: {TEST_SAMPLES['fault']}")
    print(f"      æ­£å¸¸æ ·æœ¬åˆ—è¡¨: {TEST_SAMPLES['normal']}")
    print(f"      æ˜¯æ•…éšœæ ·æœ¬: {is_fault_sample}")
    
    # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦åœ¨æ­£å¸¸æ ·æœ¬ä¸­
    is_normal_sample = sample_id_str in TEST_SAMPLES['normal']
    print(f"      æ˜¯æ­£å¸¸æ ·æœ¬: {is_normal_sample}")
    
    if not is_fault_sample and not is_normal_sample:
        print(f"   âš ï¸ è­¦å‘Šï¼šæ ·æœ¬{sample_id}æ—¢ä¸åœ¨æ•…éšœåˆ—è¡¨ä¹Ÿä¸åœ¨æ­£å¸¸åˆ—è¡¨ä¸­ï¼Œé»˜è®¤ä¸ºæ•…éšœæ ·æœ¬è¿›è¡Œæ£€æµ‹")
        is_fault_sample = True
    
    if not is_fault_sample:
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£å¸¸æ ·æœ¬ä¸è¿›è¡Œæ•…éšœæ£€æµ‹ï¼Œå‰3000ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œåç»­ç‚¹ä¹Ÿä¸æ£€æµ‹æ•…éšœ
        print(f"   â†’ æ ·æœ¬{sample_id}ä¸ºæ­£å¸¸æ ·æœ¬ï¼Œå‰{STARTUP_PERIOD}ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œå…¶ä½™ç‚¹ä¹Ÿä¸æ£€æµ‹æ•…éšœ")
        print(f"   â†’ æ­£å¸¸æ ·æœ¬ä¸­è¶…è¿‡é˜ˆå€¼çš„ç‚¹éƒ½æ˜¯å‡é˜³æ€§ï¼ˆè¯¯æŠ¥ï¼‰ï¼Œä¸åº”æ ‡è®°ä¸ºæ•…éšœ")
        
        # åˆ†åˆ«ç»Ÿè®¡å¯åŠ¨æœŸå’Œç¨³å®šæœŸçš„å‡é˜³æ€§
        startup_fai = fai_values[:STARTUP_PERIOD] if len(fai_values) > STARTUP_PERIOD else fai_values
        stable_fai = fai_values[STARTUP_PERIOD:] if len(fai_values) > STARTUP_PERIOD else []
        
        startup_fp = np.sum(startup_fai > threshold1) if len(startup_fai) > 0 else 0
        stable_fp = np.sum(stable_fai > threshold1) if len(stable_fai) > 0 else 0
        total_fp = startup_fp + stable_fp
        
        print(f"   â†’ å‡é˜³æ€§ç»Ÿè®¡:")
        print(f"     å¯åŠ¨æœŸ({STARTUP_PERIOD}ç‚¹): {startup_fp}ä¸ªè¶…é˜ˆå€¼ ({startup_fp/len(startup_fai)*100:.1f}%)")
        if len(stable_fai) > 0:
            print(f"     ç¨³å®šæœŸ({len(stable_fai)}ç‚¹): {stable_fp}ä¸ªè¶…é˜ˆå€¼ ({stable_fp/len(stable_fai)*100:.1f}%)")
        print(f"     æ€»è®¡: {total_fp}ä¸ªè¶…é˜ˆå€¼ ({total_fp/len(fai_values)*100:.1f}%)")
        
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'normal_sample',
            'startup_false_positives': startup_fp,
            'stable_false_positives': stable_fp,
            'total_false_positives': total_fp,
            'startup_fp_ratio': startup_fp/len(startup_fai) if len(startup_fai) > 0 else 0,
            'stable_fp_ratio': stable_fp/len(stable_fai) if len(stable_fai) > 0 else 0,
            'total_fp_ratio': total_fp/len(fai_values)
        }
        # ä¸ºå…¼å®¹æ€§æ·»åŠ ç©ºå­—æ®µ
        detection_info['trigger_points'] = []
        detection_info['marked_regions'] = []
        detection_info['candidate_points'] = []
        detection_info['verified_points'] = []
        
        # ç¡®ä¿fault_labelsç¡®å®æ˜¯å…¨0ï¼ˆæ­£å¸¸æ ·æœ¬ä¸æ ‡è®°ä»»ä½•æ•…éšœç‚¹ï¼‰
        fault_labels.fill(0)
        print(f"   â†’ fault_labelsæ€»å’Œ: {np.sum(fault_labels)} (æ­£å¸¸æ ·æœ¬åº”è¯¥ä¸º0)")
        return fault_labels, detection_info
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„é˜ˆå€¼ï¼Œé¿å…é‡å¤è®¡ç®—
    if config and 'threshold2' in config and 'threshold3' in config:
        threshold2 = config['threshold2']
        threshold3 = config['threshold3']
        print(f"   âœ… ä½¿ç”¨å¤–éƒ¨ä¼ å…¥é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    else:
        # é™çº§ï¼šé‡æ–°è®¡ç®—é˜ˆå€¼ï¼ˆä½†è¿™ä¸åº”è¯¥å‘ç”Ÿï¼‰
        print(f"   âš ï¸ è­¦å‘Šï¼šå¤–éƒ¨é˜ˆå€¼ç¼ºå¤±ï¼Œé‡æ–°è®¡ç®—ï¼ˆå¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ï¼‰")
        nm = 3000
        mm = len(fai_values)
        
        if mm > nm:
            # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼ï¼ˆä¸æºä»£ç ä¸€è‡´ï¼‰
            baseline_fai = fai_values[nm:mm]
            mean_fai = np.mean(baseline_fai)
            std_fai = np.std(baseline_fai)
            
            threshold1_calc = mean_fai + 3 * std_fai      # å¯¹åº”æºä»£ç threshold1
            threshold2 = mean_fai + 4.5 * std_fai        # å¯¹åº”æºä»£ç threshold2  
            threshold3 = mean_fai + 6 * std_fai          # å¯¹åº”æºä»£ç threshold3
            
            # éªŒè¯threshold1æ˜¯å¦ä¸ä¼ å…¥çš„ä¸€è‡´ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"   å†…éƒ¨é‡æ–°è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
        else:
            # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
            mean_fai = np.mean(fai_values)
            std_fai = np.std(fai_values)
            
            threshold1_calc = mean_fai + 3 * std_fai
            threshold2 = mean_fai + 4.5 * std_fai
            threshold3 = mean_fai + 6 * std_fai
            
            print(f"   çŸ­æ•°æ®é‡æ–°è®¡ç®—: T1={threshold1_calc:.4f}(ä¼ å…¥{threshold1:.4f}), T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    # ğŸ”§ æ•…éšœæ ·æœ¬ï¼šä»3001ç‚¹å¼€å§‹è¿›è¡Œæ•…éšœæ£€æµ‹ï¼ˆç¬¦åˆæºä»£ç è®¾è®¡ï¼‰
    print(f"   â†’ æ ·æœ¬{sample_id}ä¸ºæ•…éšœæ ·æœ¬ï¼Œä»ç¬¬{STARTUP_PERIOD+1}ç‚¹å¼€å§‹æ•…éšœé‡‡æ ·ç‚¹æ£€æµ‹")
    print(f"   â†’ è¯´æ˜ï¼šå‰{STARTUP_PERIOD}ç‚¹ä¸ºå¯åŠ¨æœŸï¼Œæ•…éšœæ£€æµ‹ä»ç¨³å®šæœŸå¼€å§‹")
    print(f"   â†’ æ•…éšœæ ·æœ¬ä¸­ç¨³å®šæœŸçš„é‡‡æ ·ç‚¹æœ‰äº›æ˜¯æ•…éšœçš„ï¼Œæœ‰äº›æ˜¯æ­£å¸¸çš„")
    print(f"   â†’ ç›®æ ‡ï¼šé€šè¿‡3ç‚¹æ£€æµ‹æ–¹å¼è¯†åˆ«çœŸæ­£çš„æ•…éšœé‡‡æ ·ç‚¹")
    
    trigger_points = []
    marked_regions = []
    
    # ç­–ç•¥4.0ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥ï¼ˆåŸºäºæºä»£ç é˜ˆå€¼ï¼‰
    print(f"   ğŸ”§ ç­–ç•¥4.0: ä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥ï¼ˆä¸¥æ ¼æŒ‰ç…§æºä»£ç Test_.pyï¼‰...")
    print(f"   å¼‚å¸¸ç‚¹ç»Ÿè®¡: è¶…3Ïƒ({np.sum(fai_values > threshold1)}ä¸ª), è¶…4.5Ïƒ({np.sum(fai_values > threshold2)}ä¸ª), è¶…6Ïƒ({np.sum(fai_values > threshold3)}ä¸ª)")
    print(f"   å¼‚å¸¸æ¯”ä¾‹: {np.sum(fai_values > threshold1)/len(fai_values)*100:.2f}%")
    
    detection_config = {
        'mode': 'hierarchical_v2',
        'level_3': {
            'center_threshold': threshold3,      # 6Ïƒ
            'neighbor_threshold': None,          # æ— é‚»åŸŸè¦æ±‚
            'min_neighbors': 0,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level3_high_confidence'
        },
        'level_2': {
            'center_threshold': threshold2,      # 4.5Ïƒ  
            'neighbor_threshold': threshold1,    # 3Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level2_medium_confidence'
        },
        'level_1': {
            'center_threshold': threshold1,      # 3Ïƒ
            'neighbor_threshold': threshold1 * 0.67,  # çº¦2Ïƒ (ä¼˜åŒ–å)
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1 (3ä¸ªç‚¹)
            'condition': 'level1_basic_confidence'
        }
    }
    
    print(f"   æ£€æµ‹å‚æ•°:")
    print(f"   Level 3 (6Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold3:.4f}, æ— é‚»åŸŸè¦æ±‚, æ ‡è®°3ç‚¹")
    print(f"   Level 2 (4.5Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold2:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    print(f"   Level 1 (3Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold1:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1*0.67:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹å®ç°ï¼Œä»STARTUP_PERIOD+2å¼€å§‹ï¼ˆç¡®ä¿é‚»åŸŸå®Œæ•´ï¼‰
    triggers = []
    detection_start = max(STARTUP_PERIOD + 2, 2)  # ç¡®ä¿æ—¢è·³è¿‡å¯åŠ¨æœŸï¼Œåˆæœ‰è¶³å¤Ÿé‚»åŸŸ
    detection_end = len(fai_values) - 2
    
    print(f"   ğŸ” æ£€æµ‹èŒƒå›´ï¼šç´¢å¼•[{detection_start}:{detection_end}]ï¼Œå…±{detection_end - detection_start}ä¸ªæ£€æµ‹ç‚¹")
    
    for i in range(detection_start, detection_end):
        neighborhood = fai_values[i-2:i+3]  # 5ä¸ªç‚¹çš„é‚»åŸŸ
        neighbors = [fai_values[i-2], fai_values[i-1], fai_values[i+1], fai_values[i+2]]  # 4ä¸ªé‚»å±…
        center = fai_values[i]
        
        triggered = False
        trigger_level = None
        trigger_condition = None
        detection_details = {}
        
        # Level 3: æœ€ä¸¥æ ¼é˜ˆå€¼ï¼Œæœ€å®½æ¾æ¡ä»¶ (6Ïƒ)
        if center > detection_config['level_3']['center_threshold']:
            triggered = True
            trigger_level = 3
            trigger_condition = detection_config['level_3']['condition']
            marking_range = detection_config['level_3']['marking_range']
            detection_details = {
                'center_value': center,
                'center_threshold': detection_config['level_3']['center_threshold'],
                'neighbors_above_threshold': 'N/A (no requirement)',
                'required_neighbors': 0,
                'neighborhood_values': neighborhood.tolist(),
                'trigger_reason': '6Ïƒ high confidence detection'
            }
            
        # Level 2: ä¸­ç­‰é˜ˆå€¼ï¼Œä¸­ç­‰æ¡ä»¶ (4.5Ïƒ) 
        elif center > detection_config['level_2']['center_threshold']:
            neighbors_above_t1 = np.sum(np.array(neighbors) > detection_config['level_2']['neighbor_threshold'])
            if neighbors_above_t1 >= detection_config['level_2']['min_neighbors']:
                triggered = True
                trigger_level = 2
                trigger_condition = detection_config['level_2']['condition']
                marking_range = detection_config['level_2']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_2']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_t1,
                    'required_neighbors': detection_config['level_2']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '4.5Ïƒ medium confidence detection'
                }
                
        # Level 1: æœ€ä½é˜ˆå€¼ï¼Œç›¸å¯¹ä¸¥æ ¼æ¡ä»¶ (3Ïƒ)
        elif center > detection_config['level_1']['center_threshold']:
            neighbors_above_2sigma = np.sum(np.array(neighbors) > detection_config['level_1']['neighbor_threshold'])
            if neighbors_above_2sigma >= detection_config['level_1']['min_neighbors']:
                triggered = True
                trigger_level = 1
                trigger_condition = detection_config['level_1']['condition']
                marking_range = detection_config['level_1']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_1']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_2sigma,
                    'required_neighbors': detection_config['level_1']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '3Ïƒ basic confidence detection'
                }
        
        if triggered:
            # è®¡ç®—æ ‡è®°èŒƒå›´
            start_mark = max(0, i + min(marking_range))
            end_mark = min(len(fai_values), i + max(marking_range) + 1)
            
            triggers.append({
                'center': i,
                'level': trigger_level,
                'range': (start_mark, end_mark),
                'trigger_condition': trigger_condition,
                'detection_details': detection_details
            })
    
    # ç¬¬äºŒè½®ï¼šå¤„ç†æ‰€æœ‰è§¦å‘ç‚¹ï¼ˆåˆ†çº§å¤„ç†ï¼‰
    processed_triggers = []
    level_counts = {1: 0, 2: 0, 3: 0}
    
    for trigger in triggers:
        start, end = trigger['range']
        center = trigger['center']
        level = trigger['level']
        
        # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
        level_counts[level] += 1
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start:end] = level  # ä½¿ç”¨çº§åˆ«ä½œä¸ºæ ‡è®°å€¼
        trigger_points.append(center)
        
        # è®°å½•åŒºåŸŸä¿¡æ¯
        region_data = fai_values[start:end]
        region_stats = {
            'mean_fai': np.mean(region_data),
            'max_fai': np.max(region_data),
            'min_fai': np.min(region_data),
            'std_fai': np.std(region_data),
            'length': end - start
        }
        
        marked_regions.append({
            'trigger_point': center,
            'level': level,  # åˆ†çº§æ ‡è®°
            'range': (start, end),
            'length': end - start,
            'region_stats': region_stats,
            'trigger_condition': trigger['trigger_condition'],
            'trigger_values': {
                'center': fai_values[center],
                'detection_level': f"Level {level}",
                'trigger_reason': trigger['detection_details']['trigger_reason']
            }
        })
        
        processed_triggers.append(trigger)
    
    print(f"   è§¦å‘ç»Ÿè®¡: Level 3({level_counts[3]}æ¬¡), Level 2({level_counts[2]}æ¬¡), Level 1({level_counts[1]}æ¬¡)")
    
    detection_info['trigger_points'] = trigger_points
    detection_info['marked_regions'] = marked_regions
    detection_info['processed_triggers'] = processed_triggers
    
    # ä¸ºå…¼å®¹ä¸‰çª—å£æ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–ä»£ç ï¼Œæ·»åŠ ç©ºçš„å…¼å®¹å­—æ®µ
    detection_info['candidate_points'] = []  # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    detection_info['verified_points'] = []   # 5ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    
    # ğŸ”§ ä¿®æ”¹ï¼šç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†çº§æ£€æµ‹ï¼ŒåŸºäºæœ‰æ•ˆåŒºåŸŸï¼‰
    fault_count = np.sum(fault_labels > 0)  # ä»»ä½•çº§åˆ«éƒ½ç®—æ•…éšœï¼ˆå…¨åºåˆ—ï¼‰
    # æ›´æ–°effective_labelsä¸ºæœ€æ–°çš„fault_labelsåˆ‡ç‰‡
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    effective_fault_count = np.sum(effective_labels > 0) if len(effective_labels) > 0 else 0  # æœ‰æ•ˆåŒºåŸŸæ•…éšœ
    level1_count = np.sum(fault_labels == 1)
    level2_count = np.sum(fault_labels == 2)
    level3_count = np.sum(fault_labels == 3)
    
    detection_info['detection_stats'] = {
        'total_trigger_points': len(trigger_points),
        'total_marked_regions': len(marked_regions),
        'total_fault_points': fault_count,  # å…¨åºåˆ—æ•…éšœç‚¹
        'effective_fault_points': effective_fault_count,  # æœ‰æ•ˆåŒºåŸŸæ•…éšœç‚¹
        'fault_ratio': fault_count / len(fault_labels),  # å…¨åºåˆ—æ•…éšœç‡
        'effective_fault_ratio': effective_fault_count / len(effective_labels) if len(effective_labels) > 0 else 0,  # æœ‰æ•ˆåŒºåŸŸæ•…éšœç‡
        'detection_mode': 'hierarchical_three_level_with_startup_skip',
        'startup_period': STARTUP_PERIOD,
        'effective_length': len(effective_labels) if len(effective_labels) > 0 else 0,
        'level_statistics': {
            'level_1_points': level1_count,
            'level_2_points': level2_count,
            'level_3_points': level3_count,
            'level_1_triggers': level_counts[1],
            'level_2_triggers': level_counts[2],
            'level_3_triggers': level_counts[3]
        },
        'mean_region_length': np.mean([m['length'] for m in marked_regions]) if marked_regions else 0,
        'mean_trigger_fai': np.mean([m['trigger_values']['center'] for m in marked_regions]) if marked_regions else 0,
        'strategy_used': 'strategy_4_hierarchical_detection_startup_aware',
        'parameters': detection_config
    }
    
    print(f"   â†’ ç­–ç•¥4.0æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={fault_count}ä¸ª ({fault_count/len(fault_labels)*100:.2f}%)")
    print(f"   â†’ åˆ†çº§ç»Ÿè®¡: L1={level1_count}ç‚¹, L2={level2_count}ç‚¹, L3={level3_count}ç‚¹")
    print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(triggers)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
    
    # ğŸ”§ ä¿®æ”¹ï¼šæ›´æ–°æœ‰æ•ˆåŒºåŸŸå˜é‡ï¼ˆfault_labelså·²ç»è¢«ä¿®æ”¹ï¼‰
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    
    original_anomaly_count_total = np.sum(fai_values > threshold1)  # å…¨åºåˆ—å¼‚å¸¸ç‚¹
    original_anomaly_count_effective = np.sum(effective_fai > threshold1) if len(effective_fai) > 0 else 0  # æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹
    detected_fault_count = np.sum(effective_labels > 0) if len(effective_labels) > 0 else 0  # æ£€æµ‹åˆ°çš„æ•…éšœç‚¹
    
    noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count_effective) if original_anomaly_count_effective > 0 else 0
    
    print(f"   â†’ é™å™ªæ•ˆæœåˆ†æ:")
    print(f"     å…¨åºåˆ—å¼‚å¸¸ç‚¹: {original_anomaly_count_total}ä¸ª ({original_anomaly_count_total/len(fai_values)*100:.1f}%)")
    print(f"     æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹: {original_anomaly_count_effective}ä¸ª ({original_anomaly_count_effective/len(effective_fai)*100:.1f}%)" if len(effective_fai) > 0 else "     æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹: 0ä¸ª")
    print(f"     æ£€æµ‹æ•…éšœç‚¹: {detected_fault_count}ä¸ª, é™å™ªç‡: {noise_reduction_ratio:.2%}")
    
    # ğŸ”§ æ·»åŠ æ£€æµ‹æ•ˆæœè¯Šæ–­ï¼ˆåŸºäºæœ‰æ•ˆåŒºåŸŸï¼‰
    if detected_fault_count == 0 and original_anomaly_count_effective > 0:
        print(f"   âš ï¸ æ£€æµ‹æ•ˆæœè¯Šæ–­: æœ‰æ•ˆåŒºåŸŸæœ‰{original_anomaly_count_effective}ä¸ªå¼‚å¸¸ç‚¹ä½†0ä¸ªæ£€æµ‹ç‚¹")
        print(f"   âš ï¸ å¯èƒ½åŸå› :")
        print(f"      1. é˜ˆå€¼è®¾ç½®è¿‡é«˜ (T1={threshold1:.4f})")
        print(f"      2. é‚»åŸŸéªŒè¯æ¡ä»¶è¿‡ä¸¥")
        print(f"      3. å¼‚å¸¸ç‚¹åˆ†å¸ƒè¿‡äºåˆ†æ•£ï¼Œæ— æ³•æ»¡è¶³è¿ç»­æ€§è¦æ±‚")
        
        # ğŸ”§ ä¸¥æ ¼æŒ‰ç…§æºä»£ç ï¼šåªæä¾›é˜ˆå€¼åˆ†æï¼Œä¸å»ºè®®æ›¿ä»£æ–¹æ¡ˆ
        if len(effective_fai) > 0:
            print(f"   ğŸ“Š æºä»£ç é˜ˆå€¼åˆ†æ:")
            print(f"      T1(3Ïƒ)={threshold1:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold1)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"      T2(4.5Ïƒ)={threshold2:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold2)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"      T3(6Ïƒ)={threshold3:.4f} å¯¹åº”æœ‰æ•ˆåŒºåŸŸ {np.sum(effective_fai > threshold3)/len(effective_fai)*100:.1f}% åˆ†ä½æ•°")
            print(f"   ğŸ’¡ è¯´æ˜ï¼šæºä»£ç é˜ˆå€¼åœ¨å½“å‰æ•°æ®ä¸­çš„å®é™…ä¸¥æ ¼ç¨‹åº¦")
    
    # å¦‚æœç­–ç•¥1æ²¡æœ‰æ£€æµ‹åˆ°æ•…éšœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2
    if detected_fault_count == 0 and is_fault_sample and original_anomaly_count_effective > 0:
        print(f"   âš ï¸  ç­–ç•¥1æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2...")
        print(f"   ğŸ”§ ç­–ç•¥2: è¿›ä¸€æ­¥æ”¾å®½é‚»åŸŸè¦æ±‚ï¼ˆé‚»åŸŸé˜ˆå€¼=0.6Ã—3Ïƒ, æ— é‚»å±…è¦æ±‚ï¼‰")
        
        # é‡ç½®æ ‡ç­¾å’Œåˆ—è¡¨
        fault_labels.fill(0)
        trigger_points.clear()
        marked_regions.clear()
        
        # ç­–ç•¥2å‚æ•°
        strategy2_config = {
            'center_threshold': threshold1,           # ä¿æŒ3Ïƒé˜ˆå€¼
            'neighbor_threshold': threshold1 * 0.6,  # è¿›ä¸€æ­¥é™ä½é‚»åŸŸè¦æ±‚åˆ°60%
            'min_neighbors': 0,                      # ä¸è¦æ±‚é‚»å±…ï¼ˆçº¯ä¸­å¿ƒç‚¹æ£€æµ‹ï¼‰
            'marking_range': 2,                      # æ ‡è®°Â±2ä¸ªç‚¹
            'condition': 'strategy2_relaxed'
        }
        
        # ç­–ç•¥2æ£€æµ‹
        strategy2_triggers = []
        for i in range(2, len(fai_values) - 2):
            center = fai_values[i]
            
            # ç­–ç•¥2æ¡ä»¶ï¼šåªæ£€æŸ¥ä¸­å¿ƒç‚¹
            if center > strategy2_config['center_threshold']:
                start_mark = max(0, i - strategy2_config['marking_range'])
                end_mark = min(len(fai_values), i + strategy2_config['marking_range'] + 1)
                
                fault_labels[start_mark:end_mark] = 1
                trigger_points.append(i)
                
                region_data = fai_values[start_mark:end_mark]
                marked_regions.append({
                    'trigger_point': i,
                    'level': 1,
                    'range': (start_mark, end_mark),
                    'length': end_mark - start_mark,
                    'region_stats': {
                        'mean_fai': np.mean(region_data),
                        'max_fai': np.max(region_data),
                        'std_fai': np.std(region_data),
                        'length': end_mark - start_mark
                    },
                    'trigger_condition': strategy2_config['condition'],
                    'trigger_values': {
                        'center': center
                    }
                })
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detected_fault_count = np.sum(fault_labels > 0)
        
        # æ›´æ–°detection_info
        detection_info['trigger_points'] = trigger_points
        detection_info['marked_regions'] = marked_regions
        detection_info['detection_stats']['total_trigger_points'] = len(trigger_points)
        detection_info['detection_stats']['total_marked_regions'] = len(marked_regions)
        detection_info['detection_stats']['total_fault_points'] = detected_fault_count
        detection_info['detection_stats']['fault_ratio'] = detected_fault_count / len(fault_labels)
        detection_info['detection_stats']['strategy_used'] = 'strategy_2_center_only'
        detection_info['detection_stats']['parameters'] = strategy2_config
        
        noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count_effective) if original_anomaly_count_effective > 0 else 0
        
        print(f"   â†’ ç­–ç•¥2æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={detected_fault_count}ä¸ª ({detected_fault_count/len(fault_labels)*100:.2f}%)")
        print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(trigger_points)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
        print(f"   â†’ ç­–ç•¥2é™å™ªæ•ˆæœ: æœ‰æ•ˆåŒºåŸŸå¼‚å¸¸ç‚¹={original_anomaly_count_effective}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    elif detected_fault_count == 0:
        print(f"   âš ï¸  æ­£å¸¸æ ·æœ¬æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œç¬¦åˆé¢„æœŸ")
        print(f"   â†’ æ£€æµ‹é€»è¾‘å·¥ä½œæ­£å¸¸")
    
    return fault_labels, detection_info

#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_test_sample(sample_id):
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    base_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}'
    
    # æ£€æŸ¥æ ·æœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_path):
        raise FileNotFoundError(f"æµ‹è¯•æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_path}")
    
    # åŠ è½½vin_1, vin_2, vin_3æ•°æ®
    try:
        with open(f'{base_path}/vin_1.pkl', 'rb') as f:
            vin1_data = pickle.load(f)
        with open(f'{base_path}/vin_2.pkl', 'rb') as f:
            vin2_data = pickle.load(f) 
        with open(f'{base_path}/vin_3.pkl', 'rb') as f:
            vin3_data = pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"æ ·æœ¬ {sample_id} æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        
    return vin1_data, vin2_data, vin3_data

def load_models():
    """åŠ è½½Transformeræ¨¡å‹"""
    models = {}
    
    print("ğŸ”§ å¼€å§‹åŠ è½½Transformeræ¨¡å‹...")
    
    # åŠ è½½Transformeræ¨¡å‹
    from Train_Transformer_HybridFeedback import TransformerPredictor
    models['transformer'] = TransformerPredictor().to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['transformer'], 
                          MODEL_PATHS["TRANSFORMER"]["transformer_model"], 
                          "Transformer"):
        raise RuntimeError("Transformeræ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½MC-AEæ¨¡å‹
    models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                              activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                               activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['net'], 
                          MODEL_PATHS["TRANSFORMER"]["net_model"], 
                          "MC-AE1"):
        raise RuntimeError("MC-AE1æ¨¡å‹åŠ è½½å¤±è´¥")
    
    if not safe_load_model(models['netx'], 
                          MODEL_PATHS["TRANSFORMER"]["netx_model"], 
                          "MC-AE2"):
        raise RuntimeError("MC-AE2æ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½PCAå‚æ•° (ä»æ··åˆåé¦ˆè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params_hybrid_feedback.pkl"
    try:
        with open(pca_params_path, 'rb') as f:
            models['pca_params'] = pickle.load(f)
        print(f"âœ… PCAå‚æ•°å·²åŠ è½½: {pca_params_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½PCAå‚æ•°å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•ä»å•ç‹¬çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°...")
        try:
            # ä»è®­ç»ƒè„šæœ¬ä¿å­˜çš„npyæ–‡ä»¶é‡å»ºPCAå‚æ•°
            pca_base_path = "/mnt/bz25t/bzhy/datasave/Transformer/models/"
            models['pca_params'] = {
                'v_I': np.load(f"{pca_base_path}v_I_hybrid_feedback.npy"),
                'v': np.load(f"{pca_base_path}v_hybrid_feedback.npy"),
                'v_ratio': np.load(f"{pca_base_path}v_ratio_hybrid_feedback.npy"),
                'p_k': np.load(f"{pca_base_path}p_k_hybrid_feedback.npy"),
                'data_mean': np.load(f"{pca_base_path}data_mean_hybrid_feedback.npy"),
                'data_std': np.load(f"{pca_base_path}data_std_hybrid_feedback.npy"),
                'T_95_limit': np.load(f"{pca_base_path}T_95_limit_hybrid_feedback.npy"),
                'T_99_limit': np.load(f"{pca_base_path}T_99_limit_hybrid_feedback.npy"),
                'SPE_95_limit': np.load(f"{pca_base_path}SPE_95_limit_hybrid_feedback.npy"),
                'SPE_99_limit': np.load(f"{pca_base_path}SPE_99_limit_hybrid_feedback.npy"),
                'P': np.load(f"{pca_base_path}P_hybrid_feedback.npy"),
                'k': np.load(f"{pca_base_path}k_hybrid_feedback.npy"),
                'P_t': np.load(f"{pca_base_path}P_t_hybrid_feedback.npy"),
                'X': np.load(f"{pca_base_path}X_hybrid_feedback.npy"),
                'data_nor': np.load(f"{pca_base_path}data_nor_hybrid_feedback.npy")
            }
            print(f"âœ… PCAå‚æ•°ä»npyæ–‡ä»¶é‡å»ºæˆåŠŸ")
        except Exception as e2:
            print(f"âŒ PCAå‚æ•°é‡å»ºä¹Ÿå¤±è´¥: {e2}")
        raise RuntimeError("PCAå‚æ•°åŠ è½½å¤±è´¥")
    
    return models

#----------------------------------------å•æ ·æœ¬å¤„ç†å‡½æ•°------------------------------
def process_single_sample(sample_id, models, config=None):
    """
    å¤„ç†å•ä¸ªæµ‹è¯•æ ·æœ¬
    
    Args:
        sample_id: æ ·æœ¬ID
        models: åŠ è½½çš„æ¨¡å‹
        config: çª—å£é…ç½®,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ•°æ®é¢„å¤„ç†ï¼ˆä¸BiLSTMä¿æŒä¸€è‡´ï¼‰
    print(f"   ğŸ“Š åŸå§‹æ•°æ®: vin2_shape={vin2_data.shape}, vin3_shape={vin3_data.shape}")
    
    # å¯¹vin2_dataè¿›è¡Œç‰©ç†çº¦æŸå¤„ç†
    vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
    vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
    
    print(f"   âœ… å¤„ç†åæ•°æ®: vin2_shape={vin2_processed.shape}, vin3_shape={vin3_processed.shape}")
    
    # ä½¿ç”¨å¤„ç†åçš„æ•°æ®
    vin2_data = vin2_processed
    vin3_data = vin3_processed
    
    # æ•°æ®é¢„å¤„ç†
    if len(vin1_data.shape) == 2:
        vin1_data = vin1_data.unsqueeze(1)
    vin1_data = vin1_data.to(torch.float32).to(device)

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è€Œä¸æ˜¯é‡æ–°è®¡ç®—
    pca_params = models['pca_params']
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
    
    # MC-AEæ¨ç†
    models['net'].eval()
    models['netx'].eval()
    
    with torch.no_grad():
        models['net'] = models['net'].double()
        models['netx'] = models['netx'].double()
        
        recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
        reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    AA = recon_imtest[0].cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU

    BB = reconx_imtest[0].cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX

    # è¯Šæ–­ç‰¹å¾æå–
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—
    time = np.arange(df_data.shape[0])
    
    lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
        df_data.values, 
        pca_params['data_mean'], 
        pca_params['data_std'], 
        pca_params['v'].reshape(len(pca_params['v']), 1), 
        pca_params['p_k'], 
        pca_params['v_I'], 
        pca_params['T_99_limit'], 
        pca_params['SPE_99_limit'], 
        pca_params['X'], 
        time
    )
    
    # ğŸ”§ ä¸¥æ ¼æŒ‰ç…§æºä»£ç Test_.pyçš„é˜ˆå€¼è®¡ç®—æ–¹å¼
    # æºä»£ç æ³¨é‡Šä¸­çš„è®¡ç®—æ–¹æ³•ï¼š
    # nm = 3000
    # mm = len(fai)
    # threshold1 = np.mean(fai[nm:mm]) + 3*np.std(fai[nm:mm])
    # threshold2 = np.mean(fai[nm:mm]) + 4.5*np.std(fai[nm:mm]) 
    # threshold3 = np.mean(fai[nm:mm]) + 6*np.std(fai[nm:mm])
    
    nm = 3000  # æºä»£ç å›ºå®šå€¼
    mm = len(fai)  # æ•°æ®æ€»é•¿åº¦
    
    print(f"   ğŸ“Š é˜ˆå€¼è®¡ç®—: nm={nm}, mm={mm}, ä½¿ç”¨æ•°æ®æ®µ=[{nm}:{mm}]")
    
    # ğŸ”§ æ·»åŠ FAIåˆ†å¸ƒåˆ†æ
    print(f"   ğŸ“Š FAIå€¼åˆ†å¸ƒåˆ†æ:")
    print(f"      å…¨åºåˆ—ç»Ÿè®¡: å‡å€¼={np.mean(fai):.6f}, æ ‡å‡†å·®={np.std(fai):.6f}")
    print(f"      å…¨åºåˆ—èŒƒå›´: æœ€å°å€¼={np.min(fai):.6f}, æœ€å¤§å€¼={np.max(fai):.6f}")
    print(f"      åˆ†ä½æ•°: 50%={np.percentile(fai, 50):.6f}, 95%={np.percentile(fai, 95):.6f}, 99%={np.percentile(fai, 99):.6f}")
    
    if mm > nm:
        # ä¸¥æ ¼æŒ‰ç…§æºä»£ç ï¼šä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
        fai_baseline = fai[nm:mm]
        mean_baseline = np.mean(fai_baseline)
        std_baseline = np.std(fai_baseline)
        
        # ğŸ”§ æ·»åŠ åŸºçº¿æ•°æ®åˆç†æ€§æ£€æŸ¥
        fai_early = fai[:nm] if nm < len(fai) else fai[:len(fai)//2]
        mean_early = np.mean(fai_early)
        std_early = np.std(fai_early)
        
        print(f"   ğŸ” åŸºçº¿æ•°æ®åˆç†æ€§æ£€æŸ¥:")
        print(f"      å‰æ®µæ•°æ®(0:{min(nm, len(fai)//2)}): å‡å€¼={mean_early:.6f}, æ ‡å‡†å·®={std_early:.6f}")
        print(f"      åæ®µæ•°æ®({nm}:{mm}): å‡å€¼={mean_baseline:.6f}, æ ‡å‡†å·®={std_baseline:.6f}")
        print(f"      ç»Ÿè®¡å·®å¼‚: å‡å€¼å·®={abs(mean_baseline-mean_early):.6f}, æ ‡å‡†å·®æ¯”={std_baseline/std_early:.2f}")
        
        # å¦‚æœå‰åæ®µå·®å¼‚è¿‡å¤§ï¼Œç»™å‡ºè­¦å‘Š
        if abs(mean_baseline - mean_early) > std_early or std_baseline/std_early > 2.0 or std_baseline/std_early < 0.5:
            print(f"   âš ï¸ è­¦å‘Šï¼šå‰åæ®µæ•°æ®å·®å¼‚è¾ƒå¤§ï¼ŒåŸºçº¿é€‰æ‹©å¯èƒ½ä¸åˆç†")
            print(f"   ğŸ’¡ å»ºè®®ï¼šè€ƒè™‘ä½¿ç”¨å…¨æ•°æ®æˆ–æ›´ç¨³å®šçš„åˆ†æ®µæ–¹å¼")
        
        threshold1 = mean_baseline + 3 * std_baseline      # 3Ïƒ
        threshold2 = mean_baseline + 4.5 * std_baseline    # 4.5Ïƒ  
        threshold3 = mean_baseline + 6 * std_baseline      # 6Ïƒ
        
        print(f"   âœ… æºä»£ç æ–¹å¼è®¡ç®—é˜ˆå€¼:")
        print(f"      åŸºçº¿æ®µç»Ÿè®¡: å‡å€¼={mean_baseline:.6f}, æ ‡å‡†å·®={std_baseline:.6f}")
        print(f"      T1(3Ïƒ)={threshold1:.6f}, T2(4.5Ïƒ)={threshold2:.6f}, T3(6Ïƒ)={threshold3:.6f}")
        
        # ğŸ”§ æ·»åŠ é˜ˆå€¼åˆç†æ€§åˆ†æ
        print(f"   ğŸ” é˜ˆå€¼åˆç†æ€§åˆ†æ:")
        beyond_t1 = np.sum(fai > threshold1)
        beyond_t2 = np.sum(fai > threshold2)
        beyond_t3 = np.sum(fai > threshold3)
        print(f"      è¶…è¿‡T1çš„ç‚¹æ•°: {beyond_t1} ({beyond_t1/len(fai)*100:.2f}%)")
        print(f"      è¶…è¿‡T2çš„ç‚¹æ•°: {beyond_t2} ({beyond_t2/len(fai)*100:.2f}%)")
        print(f"      è¶…è¿‡T3çš„ç‚¹æ•°: {beyond_t3} ({beyond_t3/len(fai)*100:.2f}%)")
        
        # æ˜¾ç¤ºé˜ˆå€¼ä¸æœ€å¤§å€¼çš„å…³ç³»
        fai_max = np.max(fai)
        print(f"      FAIæœ€å¤§å€¼: {fai_max:.6f}")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT1: {fai_max/threshold1:.2f}å€")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT2: {fai_max/threshold2:.2f}å€")
        print(f"      æœ€å¤§å€¼ç›¸å¯¹äºT3: {fai_max/threshold3:.2f}å€")
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆä½†è®°å½•è­¦å‘Šï¼‰
        print(f"   âš ï¸ è­¦å‘Šï¼šæ ·æœ¬{sample_id}æ•°æ®é•¿åº¦({mm})ä¸è¶³3000ï¼Œæ— æ³•æŒ‰æºä»£ç æ–¹å¼è®¡ç®—")
        print(f"   âš ï¸ é™çº§ä¸ºå…¨æ•°æ®è®¡ç®—ï¼Œå¯èƒ½ä¸æºä»£ç ç»“æœä¸ä¸€è‡´")
        
        mean_all = np.mean(fai)
        std_all = np.std(fai)
        
        threshold1 = mean_all + 3 * std_all
        threshold2 = mean_all + 4.5 * std_all  
        threshold3 = mean_all + 6 * std_all
        
        print(f"      å…¨æ•°æ®ç»Ÿè®¡: å‡å€¼={mean_all:.6f}, æ ‡å‡†å·®={std_all:.6f}")
        print(f"      T1(3Ïƒ)={threshold1:.6f}, T2(4.5Ïƒ)={threshold2:.6f}, T3(6Ïƒ)={threshold3:.6f}")
    
    # æ ¹æ®æ£€æµ‹æ¨¡å¼é€‰æ‹©æ£€æµ‹å‡½æ•°
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†è®¡ç®—å¥½çš„é˜ˆå€¼ä¼ é€’ç»™æ£€æµ‹å‡½æ•°
    threshold_config = {
        'threshold1': threshold1,
        'threshold2': threshold2, 
        'threshold3': threshold3
    }
    if config:
        threshold_config.update(config)
    
    print(f"   ğŸ“Š ä¼ é€’ç»™æ£€æµ‹å‡½æ•°çš„é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    if CURRENT_DETECTION_MODE == "five_point" or CURRENT_DETECTION_MODE == "five_point_improved":
        fault_labels, detection_info = five_point_fault_detection(fai, threshold1, sample_id, threshold_config)
    else:
        fault_labels, detection_info = three_window_fault_detection(fai, threshold1, sample_id, threshold_config)
    
    # æ„å»ºç»“æœ
    sample_result = {
        'sample_id': sample_id,
        'model_type': 'TRANSFORMER',
        'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
        'df_data': df_data.values,
        'fai': fai,
        'T_squared': t_total,
        'SPE': q_total,
        'thresholds': {
            'threshold1': threshold1,
            'threshold2': threshold2, 
            'threshold3': threshold3
        },
        'fault_labels': fault_labels,
        'detection_info': detection_info,
        'performance_metrics': {
            'fai_mean': np.mean(fai),
            'fai_std': np.std(fai),
            'fai_max': np.max(fai),
            'fai_min': np.min(fai),
            'anomaly_count': np.sum(fai > threshold1),
            'anomaly_ratio': np.sum(fai > threshold1) / len(fai)
        }
    }
    
    return sample_result

#----------------------------------------ä¸»æµ‹è¯•æµç¨‹------------------------------
def main_test_process():
    """ä¸»è¦æµ‹è¯•æµç¨‹"""
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    test_results = {
        "TRANSFORMER": [],
        "metadata": {
            "test_samples": TEST_SAMPLES,
            "window_config": WINDOW_CONFIG,
            "detection_modes": DETECTION_MODES,
            "current_mode": CURRENT_DETECTION_MODE,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    # Transformerå•æ¨¡å‹æµ‹è¯•
    total_operations = len(ALL_TEST_SAMPLES)
    
    print(f"\nğŸš€ å¼€å§‹Transformeræ¨¡å‹æµ‹è¯•...")
    print(f"æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
    print(f"æ£€æµ‹æè¿°: {DETECTION_MODES[CURRENT_DETECTION_MODE]['description']}")
    print(f"æ€»å…±éœ€è¦å¤„ç†: {total_operations} ä¸ªæ ·æœ¬")
    
    with tqdm(total=total_operations, desc="Transformeræµ‹è¯•è¿›åº¦",
              bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}]') as pbar:
        
        print(f"\n{'='*20} æµ‹è¯• Transformer æ¨¡å‹ {'='*20}")
        
        # åŠ è½½æ¨¡å‹
        pbar.set_description(f"åŠ è½½Transformeræ¨¡å‹")
        models = load_models()
        print(f"âœ… Transformer æ¨¡å‹åŠ è½½å®Œæˆ")
        
        for sample_id in ALL_TEST_SAMPLES:
            pbar.set_description(f"Transformer-æ ·æœ¬{sample_id}")
            
            try:
                # å¤„ç†å•ä¸ªæ ·æœ¬
                sample_result = process_single_sample(sample_id, models, WINDOW_CONFIG)
                test_results["TRANSFORMER"].append(sample_result)
                
                # è¾“å‡ºç®€è¦ç»“æœ
                metrics = sample_result.get('performance_metrics', {})
                detection_info = sample_result.get('detection_info', {})
                
                # 5ç‚¹æ£€æµ‹æ¨¡å¼ - å®‰å…¨è·å–æ£€æµ‹ç»Ÿè®¡
                detection_stats = detection_info.get('detection_stats', {})
                detection_ratio = detection_stats.get('fault_ratio', 0.0)
                
                print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics.get('fai_mean', 0.0):.6f}, "
                      f"å¼‚å¸¸ç‡={metrics.get('anomaly_ratio', 0.0):.2%}, "
                      f"æ£€æµ‹ç‡={detection_ratio:.2%}")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                continue
            
            pbar.update(1)
            time.sleep(0.1)  # é¿å…è¿›åº¦æ¡æ›´æ–°è¿‡å¿«
    
    print(f"\nâœ… Transformeræµ‹è¯•å®Œæˆ!")
    print(f"   Transformer: æˆåŠŸå¤„ç† {len(test_results['TRANSFORMER'])} ä¸ªæ ·æœ¬")
    
    return test_results

#----------------------------------------æ€§èƒ½åˆ†æå‡½æ•°------------------------------
def calculate_performance_metrics(test_results):
    """è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ”¬ è®¡ç®—Transformeræ€§èƒ½æŒ‡æ ‡...")
    
    performance_metrics = {}
    model_results = test_results["TRANSFORMER"]
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_true_labels = []
    all_fai_values = []
    all_fault_predictions = []
    
    for result in model_results:
        true_label = result.get('label', 0)
        fai_values = result.get('fai', [])
        fault_labels = result.get('fault_labels', [])
        thresholds = result.get('thresholds', {})
        threshold1 = thresholds.get('threshold1', 0.0)
        
        # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
        for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§BiLSTMçš„æ–¹å¼è®¾ç½®ç‚¹çº§åˆ«çœŸå®æ ‡ç­¾
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                point_true_label = 0  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
            else:  # æ•…éšœæ ·æœ¬
                point_true_label = fault_pred  # æ•…éšœæ ·æœ¬ä½¿ç”¨ä¸‰ç‚¹æ£€æµ‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾
            
            all_true_labels.append(point_true_label)  # ä½¿ç”¨ç‚¹çº§åˆ«æ ‡ç­¾
            all_fai_values.append(fai_val)
            
            # ğŸ”§ ç®€åŒ–é¢„æµ‹é€»è¾‘ï¼šåªåŸºäºFAIé˜ˆå€¼åˆ¤æ–­ï¼ˆä¸BiLSTMä¿æŒä¸€è‡´ï¼‰
            prediction = 1 if fai_val > threshold1 else 0
            all_fault_predictions.append(prediction)
    
    # è®¡ç®—ROCæŒ‡æ ‡
    all_true_labels = np.array(all_true_labels)
    all_fai_values = np.array(all_fai_values)
    all_fault_predictions = np.array(all_fault_predictions)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
    fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
    fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
    tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    tpr = recall  # True Positive Rate = Recall
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
    
    # æ ·æœ¬çº§ç»Ÿè®¡
    sample_metrics = {
        'total_samples': len(model_results),
        'normal_samples': len([r for r in model_results if r['label'] == 0]),
        'fault_samples': len([r for r in model_results if r['label'] == 1]),
        'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                 for r in model_results if r['label'] == 0]),
        'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                for r in model_results if r['label'] == 1]),
        'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                           for r in model_results if r['label'] == 0]),
        'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                          for r in model_results if r['label'] == 1])
    }
    
    performance_metrics["TRANSFORMER"] = {
        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
        'classification_metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1_score),
            'specificity': float(specificity),
            'tpr': float(tpr),
            'fpr': float(fpr)
        },
        'sample_metrics': sample_metrics,
        'roc_data': {
            'true_labels': all_true_labels.tolist(),
            'fai_values': all_fai_values.tolist(),
            'predictions': all_fault_predictions.tolist()
        }
    }
    
    return performance_metrics

# æ‰§è¡Œä¸»æµ‹è¯•æµç¨‹
test_results = main_test_process()

#----------------------------------------ROCæ›²çº¿å¯¹æ¯”------------------------------
def create_roc_analysis(test_results, performance_metrics, save_path):
    """ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ"""
    print("   ğŸ“ˆ ç”ŸæˆTransformer ROCæ›²çº¿åˆ†æ...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"], constrained_layout=True)
    
    # === å­å›¾1: è¿ç»­é˜ˆå€¼ROCæ›²çº¿ ===
    ax1.set_title('(a) Transformer ROC Curve\n(Continuous Threshold Scan)')
    
    # ä½¿ç”¨Transformerç»“æœ
    model_results = test_results["TRANSFORMER"]
    if not model_results:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„Transformerç»“æœ")
        return
    
    # æ”¶é›†æ‰€æœ‰faiå€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè¿ç»­é˜ˆå€¼ROC
    all_fai = []
    all_labels = []
    all_fault_labels = []
    
    for result in model_results:
        all_fai.extend(result['fai'])
        all_labels.extend([result['label']] * len(result['fai']))
        all_fault_labels.extend(result['fault_labels'])
    
    all_fai = np.array(all_fai)
    all_labels = np.array(all_labels)
    all_fault_labels = np.array(all_fault_labels)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(all_fai) == 0:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„FAIæ•°æ®ï¼Œè·³è¿‡ROCæ›²çº¿ç”Ÿæˆ")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„ROCå›¾
        ax1.text(0.5, 0.5, get_chart_label('æ— æ•°æ®'), ha='center', va='center', 
                transform=ax1.transAxes, fontsize=12)
        ax1.set_title(f'(a) Transformer ROC Curve\n({get_chart_label("æ— æ•°æ®")})')
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.grid(True, alpha=0.3)
        return
    
    # ğŸ”§ æ·»åŠ æ•°æ®ç»Ÿè®¡åˆ†æ
    print(f"   ğŸ“Š ROCæ›²çº¿æ•°æ®ç»Ÿè®¡:")
    print(f"      æ€»æ•°æ®ç‚¹: {len(all_fai)}")
    print(f"      æ­£å¸¸æ ·æœ¬ç‚¹: {np.sum(all_labels == 0)}")
    print(f"      æ•…éšœæ ·æœ¬ç‚¹: {np.sum(all_labels == 1)}")
    print(f"      æ•…éšœæ ‡è®°ç‚¹: {np.sum(all_fault_labels == 1)}")
    print(f"      FAIèŒƒå›´: [{np.min(all_fai):.6f}, {np.max(all_fai):.6f}]")
    
    # ğŸ”§ æ”¹è¿›é˜ˆå€¼æ‰«æç­–ç•¥ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
    # æ–¹æ³•1ï¼šä½¿ç”¨åˆ†ä½æ•°èŒƒå›´é¿å…æç«¯å€¼å½±å“
    fai_p1 = np.percentile(all_fai, 1)   # 1%åˆ†ä½æ•°
    fai_p99 = np.percentile(all_fai, 99) # 99%åˆ†ä½æ•°
    fai_median = np.median(all_fai)
    fai_mean = np.mean(all_fai)
    
    print(f"   ğŸ“Š FAIåˆ†å¸ƒåˆ†æ:")
    print(f"      1%åˆ†ä½æ•°: {fai_p1:.6f}")
    print(f"      50%åˆ†ä½æ•°(ä¸­ä½æ•°): {fai_median:.6f}")
    print(f"      å‡å€¼: {fai_mean:.6f}")
    print(f"      99%åˆ†ä½æ•°: {fai_p99:.6f}")
    print(f"      æœ€å¤§å€¼: {np.max(all_fai):.6f}")
    
    # ä½¿ç”¨æ›´æ™ºèƒ½çš„é˜ˆå€¼èŒƒå›´ï¼šä»æœ€å°å€¼åˆ°99%åˆ†ä½æ•°ï¼Œé¿å…æç«¯å€¼
    threshold_min = np.min(all_fai)
    threshold_max = fai_p99  # ä½¿ç”¨99%åˆ†ä½æ•°è€Œä¸æ˜¯æœ€å¤§å€¼
    
    # ğŸ”§ ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ï¼šçº¿æ€§+å¯¹æ•°å°ºåº¦
    if threshold_max > threshold_min * 10:  # å¦‚æœèŒƒå›´è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°å°ºåº¦
        # æ–¹æ³•1ï¼šå¯¹æ•°å°ºåº¦æ‰«æï¼ˆå¤„ç†å¤§èŒƒå›´çš„æ•°æ®ï¼‰
        log_min = np.log10(max(threshold_min, 1e-10))  # é¿å…log(0)
        log_max = np.log10(threshold_max)
        log_thresholds = np.logspace(log_min, log_max, 50)
        
        # æ–¹æ³•2ï¼šçº¿æ€§æ‰«æï¼ˆå¤„ç†å°èŒƒå›´çš„æ•°æ®ï¼‰
        linear_thresholds = np.linspace(threshold_min, min(threshold_max, fai_median*3), 50)
        
        # åˆå¹¶å¹¶å»é‡
        thresholds = np.unique(np.concatenate([linear_thresholds, log_thresholds]))
        thresholds = np.sort(thresholds)
        
        print(f"   ğŸ“Š ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ (çº¿æ€§+å¯¹æ•°): {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    else:
        # èŒƒå›´è¾ƒå°æ—¶ä½¿ç”¨çº¿æ€§æ‰«æ
        thresholds = np.linspace(threshold_min, threshold_max, 100)
        print(f"   ğŸ“Š ä½¿ç”¨çº¿æ€§æ‰«æç­–ç•¥: {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    
    print(f"   ğŸ“Š é˜ˆå€¼æ‰«æèŒƒå›´: [{threshold_min:.6f}, {threshold_max:.6f}]")
    
    tpr_list = []
    fpr_list = []
    
    for threshold in thresholds:
        tp = fp = tn = fn = 0
        
        for i, (fai_val, true_label, fault_pred) in enumerate(zip(all_fai, all_labels, all_fault_labels)):
            # ğŸ”§ ä¿®å¤ROCæ›²çº¿é€»è¾‘ï¼šä½¿ç”¨ç‚¹çº§åˆ«çš„çœŸå®æ ‡ç­¾
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
                point_true_label = 0
            else:  # æ•…éšœæ ·æœ¬ï¼šä½¿ç”¨æ•…éšœæ£€æµ‹ç®—æ³•çš„ç»“æœä½œä¸ºç‚¹çº§åˆ«çœŸå®æ ‡ç­¾
                point_true_label = fault_pred
            
            # é¢„æµ‹æ ‡ç­¾ï¼šç®€å•åŸºäºFAIé˜ˆå€¼
            predicted_label = 1 if fai_val > threshold else 0
            
            # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
            if point_true_label == 0 and predicted_label == 0:
                tn += 1
            elif point_true_label == 0 and predicted_label == 1:
                fp += 1
            elif point_true_label == 1 and predicted_label == 0:
                fn += 1
            elif point_true_label == 1 and predicted_label == 1:
                tp += 1
        
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        tpr_list.append(tpr)
        fpr_list.append(fpr)
    
    # è®¡ç®—AUC - éœ€è¦ç¡®ä¿fpr_listæ˜¯å•è°ƒé€’å¢çš„
    from sklearn.metrics import auc
    
    # ğŸ”§ ä¿®å¤AUCè®¡ç®—ï¼šç¡®ä¿FPRå•è°ƒé€’å¢
    combined = list(zip(fpr_list, tpr_list))
    combined.sort(key=lambda x: x[0])  # æŒ‰FPRæ’åº
    fpr_sorted, tpr_sorted = zip(*combined)
    
    auc_score = auc(fpr_sorted, tpr_sorted)
    
    print(f"   ğŸ“Š ROCæ›²çº¿è®¡ç®—ç»“æœ:")
    print(f"      é˜ˆå€¼æ•°é‡: {len(thresholds)}")
    print(f"      FPRèŒƒå›´: [{min(fpr_sorted):.3f}, {max(fpr_sorted):.3f}]")
    print(f"      TPRèŒƒå›´: [{min(tpr_sorted):.3f}, {max(tpr_sorted):.3f}]")
    print(f"      AUCå¾—åˆ†: {auc_score:.6f}")
    
    # ç»˜åˆ¶ROCæ›²çº¿ - ä½¿ç”¨æ’åºåçš„æ•°æ®
    ax1.plot(fpr_sorted, tpr_sorted, color='blue', linewidth=2,
            label=f'Transformer (AUC={auc_score:.3f})')
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: å›ºå®šé˜ˆå€¼å·¥ä½œç‚¹ ===
    ax2.set_title('(b) Working Point\n(Three-Level Alarm Threshold)')
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
    ax2.scatter(metrics['fpr'], metrics['tpr'], 
               s=200, color='blue', 
               label=f'Transformer\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
               marker='o', edgecolors='black', linewidth=2)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: æ€§èƒ½æŒ‡æ ‡å±•ç¤º ===
    ax3.set_title('(c) Transformer Classification Metrics')
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    metric_mapping = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1_score', 'Specificity': 'specificity'}
    transformer_values = [performance_metrics["TRANSFORMER"]['classification_metrics'][metric_mapping[m]] 
                         for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.6
    
    bars = ax3.bar(x, transformer_values, width, color='blue', alpha=0.7)
    ax3.set_xlabel('Metrics')
    ax3.set_ylabel('Score')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics_names, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_values):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å±•ç¤º ===
    ax4.set_title('(d) Transformer Sample-Level Performance')
    
    sample_metrics = ['Avg Ï†(Normal)', 'Avg Ï†(Fault)', 'Anomaly Rate(Normal)', 'Anomaly Rate(Fault)']
    transformer_sample_values = [
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_fai_fault'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics["TRANSFORMER"]['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    
    x = np.arange(len(sample_metrics))
    bars = ax4.bar(x, transformer_sample_values, width, color='blue', alpha=0.7)
    
    ax4.set_xlabel('Sample Metrics')
    ax4.set_ylabel('Value')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_metrics, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, transformer_sample_values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformer ROCåˆ†æå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºå›¾------------------------------
def create_fault_detection_timeline(test_results, save_path):
    """ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    print("   ğŸ“Š ç”ŸæˆTransformeræ•…éšœæ£€æµ‹æ—¶åºå›¾...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    # Debug: check TEST_SAMPLES type and content
    print(f"DEBUG: TEST_SAMPLES type: {type(TEST_SAMPLES)}")
    print(f"DEBUG: TEST_SAMPLES content: {TEST_SAMPLES}")
    
    try:
        fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
    except (TypeError, KeyError) as e:
        print(f"ERROR accessing TEST_SAMPLES['fault'][0]: {e}")
        fault_sample_id = '335'  # ä½¿ç”¨é»˜è®¤æ•…éšœæ ·æœ¬
    
    fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True, constrained_layout=True)
    
    # æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„ç»“æœ
    sample_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if sample_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        sample_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if sample_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = sample_result.get('fai', [])
    fault_labels = sample_result.get('fault_labels', [])
    thresholds = sample_result.get('thresholds', {})
    time_axis = np.arange(len(fai_values))
    
    # å­å›¾1: ç»¼åˆè¯Šæ–­æŒ‡æ ‡æ—¶åº
    ax1 = axes[0]
    ax1.plot(time_axis, fai_values, color='blue', linewidth=1, alpha=0.8,
           label='Transformer FAI')
    ax1.axhline(y=thresholds['threshold1'], color='orange', linestyle='--', alpha=0.7,
              label='Level 1 Threshold')
    ax1.axhline(y=thresholds['threshold2'], color='red', linestyle='--', alpha=0.7,
              label='Level 2 Threshold')
    ax1.axhline(y=thresholds['threshold3'], color='darkred', linestyle='--', alpha=0.7,
              label='Level 3 Threshold')
    
    ax1.set_ylabel('Transformer\nComprehensive Diagnostic Index Ï†')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_title(f'Transformer - Sample {fault_sample_id} (Fault Sample)')
    
    # å­å›¾2: æ•…éšœæ£€æµ‹ç»“æœ
    ax2 = axes[1]
    
    # å°†æ•…éšœæ ‡ç­¾è½¬æ¢ä¸ºå¯è§†åŒ–åŒºåŸŸ
    fault_regions = np.where(fault_labels == 1, 0.8, 0)
    ax2.fill_between(time_axis, fault_regions, 
                    alpha=0.6, color='blue',
                    label='Transformer Fault Detection')
    
    ax2.set_ylabel('Fault Detection Result')
    ax2.set_ylim(0, 1)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_title('Transformer Fault Detection Result')
    
    # å­å›¾3: ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹
    ax3 = axes[2]
    detection_info = sample_result['detection_info']
    
    ax3.plot(time_axis, fai_values, 'b-', alpha=0.5, label=get_chart_label('Ï†æŒ‡æ ‡å€¼'))
    
    # ğŸ”§ ä¿®å¤ï¼šä¸‰ç‚¹æ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–
    # æ ‡è®°è§¦å‘ç‚¹ï¼ˆå¯¹åº”åŸæ¥çš„å€™é€‰ç‚¹ï¼‰
    if detection_info.get('trigger_points'):
        ax3.scatter(detection_info['trigger_points'], 
                   [fai_values[i] for i in detection_info['trigger_points']],
                   color='orange', s=30, label=get_chart_label('è§¦å‘ç‚¹'), alpha=0.8)
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    marked_regions = detection_info.get('marked_regions', [])
    for i, region in enumerate(marked_regions):
        start, end = region['range']
        label = get_chart_label('æ•…éšœåŒºåŸŸ') if i == 0 else ""
        ax3.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax3.set_ylabel('Three-Point Detection\nProcess')
    ax3.set_xlabel('Time Step')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_title('Three-Point Detection Process (Transformer)')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformeræ—¶åºå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾------------------------------
def create_performance_radar(performance_metrics, save_path):
    """ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    print("   ğŸ•¸ï¸ ç”ŸæˆTransformeræ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'Accuracy': 'accuracy',
        'Precision': 'precision', 
        'Recall': 'recall',
        'F1-Score': 'f1_score',
        'Specificity': 'specificity',
        'Early Warning': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'False Alarm Control': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'Detection Stability': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
    if "TRANSFORMER" not in performance_metrics:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
        return
    
    # æ•°æ®é¢„å¤„ç†ï¼šFPRéœ€è¦è½¬æ¢ä¸ºæ§åˆ¶èƒ½åŠ› (1-FPR)
    transformer_values = []
    
    for metric_name, metric_key in radar_metrics.items():
        transformer_val = performance_metrics["TRANSFORMER"]['classification_metrics'][metric_key]
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
        if metric_name == 'False Alarm Control':
            transformer_val = 1 - transformer_val
            
        transformer_values.append(transformer_val)
    
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    transformer_values += transformer_values[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize_medium"], subplot_kw=dict(projection='polar'), constrained_layout=True)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles, transformer_values, 'o-', linewidth=2, label='Transformer', color='blue')
    ax.fill(angles, transformer_values, alpha=0.25, color='blue')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(list(radar_metrics.keys()))
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('Transformer Performance Metrics Radar Chart', 
              pad=20, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # æ·»åŠ æ€§èƒ½æ€»ç»“
    transformer_avg = np.mean(transformer_values[:-1])
    
    plt.figtext(0.02, 0.02, f'Transformer Overall Performance: {transformer_avg:.3f}', 
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformeré›·è¾¾å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–------------------------------
def create_three_window_visualization(test_results, save_path):
    """ç”ŸæˆTransformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–"""
    print("   ğŸ” ç”ŸæˆTransformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    # Debug: check TEST_SAMPLES type and content
    print(f"DEBUG: TEST_SAMPLES type: {type(TEST_SAMPLES)}")
    print(f"DEBUG: TEST_SAMPLES content: {TEST_SAMPLES}")
    
    try:
        fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'
    except (TypeError, KeyError) as e:
        print(f"ERROR accessing TEST_SAMPLES['fault'][0]: {e}")
        fault_sample_id = '335'  # ä½¿ç”¨é»˜è®¤æ•…éšœæ ·æœ¬
    
    fig = plt.figure(figsize=(16, 10), constrained_layout=True)
    
    # ä½¿ç”¨GridSpecè¿›è¡Œå¤æ‚å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1])
    
    # === ä¸»å›¾ï¼šä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹æ—¶åºå›¾ ===
    ax_main = fig.add_subplot(gs[0, :])
    
    # é€‰æ‹©Transformerç»“æœè¿›è¡Œå¯è§†åŒ–
    transformer_result = next((r for r in test_results["TRANSFORMER"] if r.get('sample_id') == fault_sample_id), None)
    
    if transformer_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        transformer_result = test_results["TRANSFORMER"][0] if test_results["TRANSFORMER"] else None
    
    if transformer_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = transformer_result.get('fai', [])
    detection_info = transformer_result.get('detection_info', {})
    thresholds = transformer_result.get('thresholds', {})
    threshold1 = thresholds.get('threshold1', 0.0)
    
    time_axis = np.arange(len(fai_values))
    
    # ç»˜åˆ¶FAIæ—¶åº
    ax_main.plot(time_axis, fai_values, 'b-', linewidth=1.5, alpha=0.8, label='Comprehensive Diagnostic Index Ï†(FAI)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.7, label='Level 1 Threshold')
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        candidate_points = detection_info['candidate_points']
        ax_main.scatter(candidate_points, [fai_values[i] for i in candidate_points],
                       color='orange', s=40, alpha=0.8, label=f'Detection: {len(candidate_points)} Candidate Points',
                       marker='o', zorder=5)
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax_main.scatter(verified_indices, [fai_values[i] for i in verified_indices],
                       color='red', s=60, alpha=0.9, label=f'{get_chart_label("éªŒè¯")}: {len(verified_indices)}{get_chart_label("ä¸ªç¡®è®¤ç‚¹")}',
                       marker='^', zorder=6)
        
        # æ˜¾ç¤ºéªŒè¯çª—å£èŒƒå›´
        for v_point in detection_info['verified_points']:
            verify_start, verify_end = v_point['verify_range']
            ax_main.axvspan(verify_start, verify_end, alpha=0.1, color='yellow')
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ•…éšœåŒºåŸŸ
    fault_regions_plotted = set()  # é¿å…é‡å¤ç»˜åˆ¶å›¾ä¾‹
    for i, region in enumerate(detection_info['marked_regions']):
        start, end = region['range']
        label = f'{get_chart_label("æ ‡è®°çš„")} {get_chart_label("æ•…éšœåŒºåŸŸ")}' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('Time Step')
    ax_main.set_ylabel('Comprehensive Diagnostic Index Ï†')
    ax_main.set_title(f'Transformer Three-Point Fault Detection Process - Sample {fault_sample_id}', 
                     fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left')
    ax_main.grid(True, alpha=0.3)
    
    # === å­å›¾1ï¼šæ£€æµ‹çª—å£ç»Ÿè®¡ ===
    ax1 = fig.add_subplot(gs[1, 0])
    
    # ğŸ”§ ä¿®å¤ï¼šä¸‰ç‚¹æ£€æµ‹æ¨¡å¼æ²¡æœ‰window_statsï¼Œä½¿ç”¨detection_stats
    detection_stats = detection_info.get('detection_stats', {})
    detection_data = [
        detection_stats.get('total_trigger_points', 0),    # è§¦å‘ç‚¹æ•°ï¼ˆå¯¹åº”å€™é€‰ç‚¹ï¼‰
        detection_stats.get('total_marked_regions', 0),    # æ ‡è®°åŒºåŸŸæ•°ï¼ˆå¯¹åº”éªŒè¯ç‚¹ï¼‰
        detection_stats.get('total_fault_points', 0)       # æ•…éšœç‚¹æ•°
    ]
    detection_labels = ['Candidate Points', 'Verified Points', 'Fault Points']
    colors1 = ['orange', 'red', 'darkred']
    
    bars1 = ax1.bar(detection_labels, detection_data, color=colors1, alpha=0.7)
    ax1.set_title('Detection Statistics')
    ax1.set_ylabel('Count')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, detection_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(value), ha='center', va='bottom')
    
    # === å­å›¾2ï¼šThree-Point Detection Parameters ===
    ax2 = fig.add_subplot(gs[1, 1])
    
    # æ˜¾ç¤ºä¸‰ç‚¹æ£€æµ‹çš„é˜ˆå€¼å‚æ•°
    thresholds = transformer_result.get('thresholds', {})
    threshold_names = ['3Ïƒ Threshold', '4.5Ïƒ Threshold', '6Ïƒ Threshold']
    threshold_values = [
        thresholds.get('threshold1', 0),
        thresholds.get('threshold2', 0),
        thresholds.get('threshold3', 0)
    ]
    colors2 = ['lightblue', 'orange', 'red']
    
    bars2 = ax2.bar(threshold_names, threshold_values, color=colors2, alpha=0.7)
    ax2.set_title('Detection Thresholds\n(Three-Level Hierarchy)')
    ax2.set_ylabel('Threshold Value')
    ax2.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars2, threshold_values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.2f}', ha='center', va='bottom')
    
    # === å­å›¾3ï¼šè§¦å‘çº§åˆ«åˆ†å¸ƒ ===
    ax3 = fig.add_subplot(gs[1, 2])
    
    # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
    detection_stats = detection_info.get('detection_stats', {})
    level_stats = detection_stats.get('level_statistics', {})
    
    if level_stats:
        levels = ['Level 1', 'Level 2', 'Level 3']
        trigger_counts = [
            level_stats.get('level_1_triggers', 0),
            level_stats.get('level_2_triggers', 0), 
            level_stats.get('level_3_triggers', 0)
        ]
        colors = ['lightblue', 'orange', 'red']
        
        bars3 = ax3.bar(levels, trigger_counts, color=colors, alpha=0.7)
        ax3.set_title('Trigger Level Distribution')
        ax3.set_xlabel('Detection Level')
        ax3.set_ylabel('Trigger Count')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars3, trigger_counts):
            if count > 0:
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(count), ha='center', va='bottom')
    else:
        ax3.text(0.5, 0.5, 'No Trigger Data', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('Trigger Level Distribution')
    
    # === å­å›¾4ï¼šTransformer Performance ===
    ax4 = fig.add_subplot(gs[1, 3])
    
    sample_result = next((r for r in test_results['TRANSFORMER'] if r.get('sample_id') == fault_sample_id), None)
    if sample_result is None:
        sample_result = test_results['TRANSFORMER'][0] if test_results['TRANSFORMER'] else None
    
    if sample_result is None:
        fault_ratio = 0.0
    else:
        detection_info = sample_result.get('detection_info', {})
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨detection_statsæ›¿ä»£window_stats
    detection_stats = detection_info.get('detection_stats', {})
    fault_ratio = detection_stats.get('fault_ratio', 0.0)
    
    bars4 = ax4.bar(['Transformer'], [fault_ratio], color='blue', alpha=0.7)
    ax4.set_title('Transformer\n(Fault Detection Ratio)')
    ax4.set_ylabel('Fault Ratio')
    
    for bar, value in zip(bars4, [fault_ratio]):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === åº•éƒ¨ï¼šè¿‡ç¨‹è¯´æ˜ ===
    process_text = """
    Transformer Three-Point Detection Process:
    
    1. Level 3 (6Ïƒ): Center point exceeds 6Ïƒ threshold, no neighborhood requirement, directly mark 3 points
    2. Level 2 (4.5Ïƒ): Center point exceeds 4.5Ïƒ + at least 1 neighbor exceeds 3Ïƒ, mark 3 points
    3. Level 1 (3Ïƒ): Center point exceeds 3Ïƒ + at least 1 neighbor exceeds 2Ïƒ, mark 3 points
    
    Advantages: Hierarchical detection + neighborhood verification, effective noise reduction while maintaining sensitivity
    """
    
    fig.text(0.02, 0.02, process_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… Transformerä¸‰ç‚¹æ£€æµ‹è¿‡ç¨‹å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»“æœä¿å­˜å‡½æ•°------------------------------
def save_test_results(test_results, performance_metrics):
    """ä¿å­˜Transformeræµ‹è¯•ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜Transformeræµ‹è¯•ç»“æœ...")
    
    # åˆ›å»ºç»“æœç›®å½• - ç»Ÿä¸€ä¿å­˜åˆ°modelsç›®å½•ä¸‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = f"/mnt/bz25t/bzhy/datasave/Transformer/models/test_results_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(f"{result_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{result_dir}/detailed_results", exist_ok=True)
    
    # 1. ä¿å­˜æ€§èƒ½æŒ‡æ ‡JSON
    performance_file = f"{result_dir}/transformer_performance_metrics.json"
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræ€§èƒ½æŒ‡æ ‡ä¿å­˜è‡³: {performance_file}")
    
    # 2. ä¿å­˜è¯¦ç»†ç»“æœ
    detail_file = f"{result_dir}/detailed_results/transformer_detailed_results.pkl"
    with open(detail_file, 'wb') as f:
        pickle.dump(test_results["TRANSFORMER"], f)
    print(f"   âœ… Transformerè¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
    
    # 3. ä¿å­˜å…ƒæ•°æ®
    metadata_file = f"{result_dir}/detailed_results/transformer_test_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(test_results['metadata'], f, indent=2, ensure_ascii=False)
    print(f"   âœ… Transformeræµ‹è¯•å…ƒæ•°æ®ä¿å­˜è‡³: {metadata_file}")
    
    # 4. åˆ›å»ºExcelæ€»ç»“æŠ¥å‘Š
    summary_file = f"{result_dir}/detailed_results/transformer_summary.xlsx"
    
    with pd.ExcelWriter(summary_file) as writer:
        # ä½¿ç”¨Transformeræ€§èƒ½æŒ‡æ ‡
        if "TRANSFORMER" not in performance_metrics:
            print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æŒ‡æ ‡")
            return result_dir
        
        metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
        confusion = performance_metrics["TRANSFORMER"]['confusion_matrix']
        sample_metrics = performance_metrics["TRANSFORMER"]['sample_metrics']
        
        performance_data = [{
            'Model': 'TRANSFORMER',
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'Specificity': metrics['specificity'],
            'TPR': metrics['tpr'],
            'FPR': metrics['fpr'],
            'TP': confusion['TP'],
            'FP': confusion['FP'],
            'TN': confusion['TN'],
            'FN': confusion['FN'],
            'Avg_FAI_Normal': sample_metrics['avg_fai_normal'],
            'Avg_FAI_Fault': sample_metrics['avg_fai_fault']
        }]
        
        performance_df = pd.DataFrame(performance_data)
        performance_df.to_excel(writer, sheet_name='Transformer_Performance', index=False)
        
        # æ ·æœ¬è¯¦æƒ…è¡¨
        sample_details = []
        for result in test_results["TRANSFORMER"]:
            # å®‰å…¨è·å–detection_infoå’Œdetection_stats
            detection_info = result.get('detection_info', {})
            detection_stats = detection_info.get('detection_stats', {})
            performance_metrics = result.get('performance_metrics', {})
            
            sample_details.append({
                'Sample_ID': result.get('sample_id', 'Unknown'),
                'True_Label': 'Fault' if result.get('label', 0) == 1 else 'Normal',
                'FAI_Mean': performance_metrics.get('fai_mean', 0.0),
                'FAI_Std': performance_metrics.get('fai_std', 0.0),
                'FAI_Max': performance_metrics.get('fai_max', 0.0),
                'Anomaly_Ratio': performance_metrics.get('anomaly_ratio', 0.0),
                'Fault_Detection_Ratio': detection_stats.get('fault_ratio', 0.0),
                'Candidates_Found': detection_stats.get('total_trigger_points', 0),
                'Verified_Points': detection_stats.get('total_marked_regions', 0)
            })
        
        sample_df = pd.DataFrame(sample_details)
        sample_df.to_excel(writer, sheet_name='Sample_Details', index=False)
    
    print(f"   âœ… Transformer Excelæ€»ç»“æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")
    
    return result_dir

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
performance_metrics = calculate_performance_metrics(test_results)

# ä¿å­˜æµ‹è¯•ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
result_dir = save_test_results(test_results, performance_metrics)

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
print("\nğŸ¨ ç”ŸæˆTransformerå¯è§†åŒ–åˆ†æ...")

# ç”ŸæˆROCåˆ†æå›¾
create_roc_analysis(test_results, performance_metrics, f"{result_dir}/visualizations/transformer_roc_analysis.png")

# ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾
create_fault_detection_timeline(test_results, f"{result_dir}/visualizations/transformer_fault_detection_timeline.png")

# ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
create_performance_radar(performance_metrics, f"{result_dir}/visualizations/transformer_performance_radar.png")

# ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å›¾
create_three_window_visualization(test_results, f"{result_dir}/visualizations/transformer_three_window_process.png")

#----------------------------------------ç‰¹å®šæ ·æœ¬å¯è§†åŒ–------------------------------
print("\nğŸ¯ ç”Ÿæˆç‰¹å®šæ ·æœ¬çš„è¯¦ç»†å¯è§†åŒ–å›¾è¡¨...")

# æŒ‡å®šè¦ç”Ÿæˆè¯¦ç»†å›¾è¡¨çš„æ ·æœ¬
target_samples = {
    'normal': ['12', '13', '14'],      # æ­£å¸¸æ ·æœ¬ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼ŒåŒ¹é…load_test_samplesè¿”å›æ ¼å¼ï¼‰
    'fault': ['340', '345', '346', '347']  # æ•…éšœæ ·æœ¬ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼ŒåŒ¹é…load_test_samplesè¿”å›æ ¼å¼ï¼‰
}

def create_sample_specific_visualizations(test_results, target_samples, base_dir):
    """ä¸ºæŒ‡å®šæ ·æœ¬ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–å›¾è¡¨"""
    
    # åˆ›å»ºæ ·æœ¬ä¸“ç”¨ç›®å½•
    sample_viz_dir = f"{base_dir}/sample_visualizations"
    os.makedirs(sample_viz_dir, exist_ok=True)
    
    print(f"   ğŸ“ æ ·æœ¬å¯è§†åŒ–ç›®å½•: {sample_viz_dir}")
    
    # æ‰¾å‡ºæ‰€æœ‰éœ€è¦å¤„ç†çš„æ ·æœ¬
    all_target_samples = target_samples['normal'] + target_samples['fault']
    
    for sample_id in all_target_samples:
        print(f"   ğŸ” å¤„ç†æ ·æœ¬ {sample_id}...")
        
        # ä»æµ‹è¯•ç»“æœä¸­æ‰¾åˆ°å¯¹åº”çš„æ ·æœ¬
        sample_result = None
        for result in test_results["TRANSFORMER"]:
            if result.get('sample_id') == sample_id:
                sample_result = result
                break
        
        if sample_result is None:
            print(f"   âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æ ·æœ¬ {sample_id} çš„æµ‹è¯•ç»“æœ")
            continue
        
        # ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾ï¼ˆå•æ ·æœ¬ç‰ˆæœ¬ï¼‰
        try:
            create_single_sample_timeline(sample_result, f"{sample_viz_dir}/transformer_fault_detection_timeline_sample_{sample_id}.png")
            print(f"   âœ… æ ·æœ¬ {sample_id} æ•…éšœæ£€æµ‹æ—¶åºå›¾å·²ç”Ÿæˆ")
        except Exception as e:
            print(f"   âŒ æ ·æœ¬ {sample_id} æ•…éšœæ£€æµ‹æ—¶åºå›¾ç”Ÿæˆå¤±è´¥: {e}")
        
        # ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å›¾ï¼ˆå•æ ·æœ¬ç‰ˆæœ¬ï¼‰
        try:
            create_single_sample_three_window(sample_result, f"{sample_viz_dir}/transformer_three_window_process_sample_{sample_id}.png")
            print(f"   âœ… æ ·æœ¬ {sample_id} ä¸‰çª—å£è¿‡ç¨‹å›¾å·²ç”Ÿæˆ")
        except Exception as e:
            print(f"   âŒ æ ·æœ¬ {sample_id} ä¸‰çª—å£è¿‡ç¨‹å›¾ç”Ÿæˆå¤±è´¥: {e}")

def create_single_sample_timeline(sample_result, save_path):
    """ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    
    sample_id = sample_result.get('sample_id', 'Unknown')
    fai_values = np.array(sample_result.get('fai', []))
    fault_labels = np.array(sample_result.get('fault_labels', []))
    true_label = sample_result.get('label', 0)
    thresholds = sample_result.get('thresholds', {})
    
    if len(fai_values) == 0:
        print(f"   âš ï¸ æ ·æœ¬ {sample_id} æ— FAIæ•°æ®")
        return
    
    # è®¾ç½®å­—ä½“
    setup_english_fonts()
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(3, 1, figsize=(15, 12), constrained_layout=True)
    
    time_steps = np.arange(len(fai_values))
    
    # === å­å›¾1: FAIå€¼æ—¶åºå›¾ ===
    ax1 = axes[0]
    ax1.plot(time_steps, fai_values, 'b-', linewidth=1, alpha=0.8, label=f'{get_chart_label("Ï†æŒ‡æ ‡å€¼")}')
    
    # ç»˜åˆ¶é˜ˆå€¼çº¿
    threshold1 = thresholds.get('threshold1', 0.0)
    threshold2 = thresholds.get('threshold2', 0.0)
    threshold3 = thresholds.get('threshold3', 0.0)
    
    ax1.axhline(y=threshold1, color='orange', linestyle='--', alpha=0.8, label=f'Level 1 Threshold')
    if threshold2 > 0:
        ax1.axhline(y=threshold2, color='red', linestyle='--', alpha=0.8, label=f'Level 2 Threshold')
    if threshold3 > 0:
        ax1.axhline(y=threshold3, color='darkred', linestyle='--', alpha=0.8, label=f'Level 3 Threshold')
    
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Comprehensive Diagnostic Index Ï†')
    ax1.set_title(f'Transformer - Sample {sample_id} ({get_chart_label("æ•…éšœåŒºåŸŸ") if true_label == 1 else "Normal Sample"})')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: æ•…éšœæ£€æµ‹ç»“æœ ===
    ax2 = axes[1]
    ax2.fill_between(time_steps, 0, fault_labels, alpha=0.6, color='blue', 
                     label=f'Transformer Fault Detection')
    
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Fault Detection Result')
    ax2.set_title(f'Transformer Fault Detection Result')
    ax2.set_ylim(-0.1, 1.1)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: ç»¼åˆåˆ†æ ===
    ax3 = axes[2]
    ax3.plot(time_steps, fai_values, 'b-', linewidth=1, alpha=0.7, label=f'{get_chart_label("Ï†æŒ‡æ ‡å€¼")}')
    ax3.axhline(y=threshold1, color='orange', linestyle='--', alpha=0.8, label=f'Level 1 Threshold')
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    fault_regions = np.where(fault_labels == 1)[0]
    if len(fault_regions) > 0:
        # æ‰¾è¿ç»­åŒºåŸŸ
        regions = []
        start = fault_regions[0]
        for i in range(1, len(fault_regions)):
            if fault_regions[i] - fault_regions[i-1] > 1:
                regions.append((start, fault_regions[i-1]))
                start = fault_regions[i]
        regions.append((start, fault_regions[-1]))
        
        for i, (start, end) in enumerate(regions):
            label = f'{get_chart_label("æ•…éšœåŒºåŸŸ")}' if i == 0 else ""
            ax3.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax3.set_xlabel('Time Step')
    ax3.set_ylabel('Comprehensive Diagnostic Index Ï†')
    ax3.set_title(f'Transformer Fault Detection Process - Sample {sample_id}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # ä¿å­˜å›¾è¡¨
    try:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ ·æœ¬ {sample_id} æ—¶åºå›¾å¤±è´¥: {e}")
        plt.close()

def create_single_sample_three_window(sample_result, save_path):
    """ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å›¾"""
    
    sample_id = sample_result.get('sample_id', 'Unknown')
    fai_values = np.array(sample_result.get('fai', []))
    true_label = sample_result.get('label', 0)
    thresholds = sample_result.get('thresholds', {})
    detection_info = sample_result.get('detection_info', {})
    
    if len(fai_values) == 0:
        print(f"   âš ï¸ æ ·æœ¬ {sample_id} æ— FAIæ•°æ®")
        return
    
    # è®¾ç½®å­—ä½“
    setup_english_fonts()
    
    # åˆ›å»ºä¸»å›¾å’Œå­å›¾
    fig = plt.figure(figsize=(16, 10))
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1], 
                         hspace=0.3, wspace=0.3)
    
    # ä¸»å›¾ï¼šä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹
    ax_main = fig.add_subplot(gs[0, :])
    
    time_steps = np.arange(len(fai_values))
    threshold1 = thresholds.get('threshold1', 0.0)
    
    # ç»˜åˆ¶FAIå€¼
    ax_main.plot(time_steps, fai_values, 'b-', linewidth=1, alpha=0.8, 
                label=f'Comprehensive Diagnostic Index Ï†(Î”t)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.8, 
                   label=f'Level 1 Threshold')
    
    # é˜¶æ®µ1ï¼šè§¦å‘ç‚¹
    trigger_points = detection_info.get('trigger_points', [])
    if trigger_points:
        trigger_indices = [p['index'] for p in trigger_points]
        ax_main.scatter(trigger_indices, fai_values[trigger_indices], 
                       color='red', s=30, marker='o', alpha=0.8, 
                       label=f'{get_chart_label("è§¦å‘ç‚¹")}', zorder=5)
    
    # é˜¶æ®µ2ï¼šéªŒè¯ç‚¹
    verified_points = detection_info.get('verified_points', [])
    if verified_points:
        verified_indices = [p['index'] for p in verified_points]
        ax_main.scatter(verified_indices, fai_values[verified_indices], 
                       color='orange', s=50, marker='^', alpha=0.8, 
                       label=f'{get_chart_label("éªŒè¯")}: {len(verified_points)} {get_chart_label("ç¡®è®¤ç‚¹")}', zorder=5)
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ•…éšœåŒºåŸŸ
    fault_regions_plotted = set()  # é¿å…é‡å¤ç»˜åˆ¶å›¾ä¾‹
    marked_regions = detection_info.get('marked_regions', [])
    for i, region in enumerate(marked_regions):
        start, end = region['range']
        label = f'{get_chart_label("æ ‡è®°çš„")} {get_chart_label("æ•…éšœåŒºåŸŸ")}' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('Time Step')
    ax_main.set_ylabel('Comprehensive Diagnostic Index Ï†')
    ax_main.set_title(f'Transformer Three-Point Fault Detection Process - Sample {sample_id}', 
                     fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left', fontsize=9)
    ax_main.grid(True, alpha=0.3)
    
    # å­å›¾1ï¼šæ£€æµ‹ç»Ÿè®¡
    ax1 = fig.add_subplot(gs[1, 0])
    detection_stats = detection_info.get('detection_stats', {})
    candidates = detection_stats.get('total_trigger_points', 0)
    fault_points = detection_stats.get('total_marked_regions', 0)
    
    bars = ax1.bar(['Candidate Points', 'Fault Points'], [candidates, fault_points], 
                  color=['orange', 'red'], alpha=0.7)
    ax1.set_ylabel('Count')
    ax1.set_title('Detection Statistics')
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, [candidates, fault_points]):
        if value > 0:
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(value), ha='center', va='bottom')
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šå±‚çº§é˜ˆå€¼åˆ†å¸ƒ
    ax2 = fig.add_subplot(gs[1, 1])
    thresholds_data = [threshold1, thresholds.get('threshold2', 0), thresholds.get('threshold3', 0)]
    threshold_names = ['Level 1', 'Level 2', 'Level 3']
    colors = ['lightblue', 'orange', 'red']
    
    bars = ax2.bar(threshold_names, thresholds_data, color=colors, alpha=0.7)
    ax2.set_ylabel('Threshold Value')
    ax2.set_title('Detection Thresholds\n(Three-Level Hierarchy)')
    ax2.tick_params(axis='x', rotation=45)
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, thresholds_data):
        if value > 0:
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{value:.2f}', ha='center', va='bottom', fontsize=8)
    ax2.grid(True, alpha=0.3)
    
    # å­å›¾3ï¼šè§¦å‘ç‚¹åˆ†å¸ƒ
    ax3 = fig.add_subplot(gs[1, 2])
    if trigger_points:
        level_counts = {'Level 1': 0, 'Level 2': 0, 'Level 3': 0}
        for point in trigger_points:
            level = point.get('level', 1)
            level_counts[f'Level {level}'] += 1
        
        levels = list(level_counts.keys())
        counts = list(level_counts.values())
        colors = ['lightblue', 'orange', 'red']
        
        bars = ax3.bar(levels, counts, color=colors, alpha=0.7)
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, counts):
            if value > 0:
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                        str(value), ha='center', va='bottom')
    else:
        ax3.text(0.5, 0.5, get_chart_label('æ— æ•°æ®'), ha='center', va='center', 
                transform=ax3.transAxes)
    
    ax3.set_ylabel('Trigger Count')
    ax3.set_title('Trigger Level Distribution')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # å­å›¾4ï¼šæ•…éšœæ£€æµ‹ç‡
    ax4 = fig.add_subplot(gs[1, 3])
    fault_ratio = detection_stats.get('fault_ratio', 0.0)
    ax4.bar(['Transformer'], [fault_ratio], color='blue', alpha=0.7, width=0.5)
    ax4.set_ylabel('Fault Ratio')
    ax4.set_title(f'{fault_ratio:.3f}\nTransformer\n(Fault Detection Ratio)')
    ax4.set_ylim(0, 1)
    ax4.grid(True, alpha=0.3)
    
    # åº•éƒ¨æ–‡å­—è¯´æ˜
    explanation_ax = fig.add_subplot(gs[2, :])
    explanation_ax.axis('off')
    
    explanation_text = f"""Transformer Three-Point Detection Process:
1. Level 3 (5Ïƒ): Center point exceeds 5Ïƒ threshold, requirement, directly mark 1 points
2. Level 2 (4.5Ïƒ): Center point succeeds 4.5Ïƒ + at least 1 neighbor succeeding 5Ïƒ in 3 points
3. Level 1 (3Ïƒ): Center point succeeds 3Ïƒ + at least 1 neighbor succeeding 4Ïƒ or 2 neighbors out 3 points

Advantages: Hierarchical detection + neighborhood verification, effective noise reduction while maintaining sensitivity"""
    
    explanation_ax.text(0.05, 0.8, explanation_text, transform=explanation_ax.transAxes, 
                       fontsize=10, verticalalignment='top', 
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    # ä¿å­˜å›¾è¡¨
    try:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ ·æœ¬ {sample_id} ä¸‰çª—å£è¿‡ç¨‹å›¾å¤±è´¥: {e}")
        plt.close()

# æ‰§è¡Œç‰¹å®šæ ·æœ¬å¯è§†åŒ–
create_sample_specific_visualizations(test_results, target_samples, result_dir)

#----------------------------------------æœ€ç»ˆæ€»ç»“------------------------------
print("\n" + "="*80)
print("ğŸ‰ Transformeræ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(ALL_TEST_SAMPLES)} ä¸ª (æ­£å¸¸: {len(TEST_SAMPLES['normal'])}, æ•…éšœ: {len(TEST_SAMPLES['fault'])})")
print(f"   â€¢ æ¨¡å‹ç±»å‹: Transformer")
print(f"   â€¢ æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")

print(f"\nğŸ”¬ Transformeræ€§èƒ½:")
if CURRENT_DETECTION_MODE == "three_window":
    print(f"   â€¢ çª—å£é…ç½®: æ£€æµ‹({WINDOW_CONFIG['detection_window']}) â†’ éªŒè¯({WINDOW_CONFIG['verification_window']}) â†’ æ ‡è®°({WINDOW_CONFIG['marking_window']})")
else:
    print(f"   â€¢ 5ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°5ç‚¹åŒºåŸŸ")

metrics = performance_metrics["TRANSFORMER"]['classification_metrics']
print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")
print(f"   TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   â€¢ ç»“æœç›®å½•: {result_dir}")
print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {result_dir}/visualizations")
print(f"     - ROCåˆ†æå›¾: transformer_roc_analysis.png")
print(f"     - æ•…éšœæ£€æµ‹æ—¶åºå›¾: transformer_fault_detection_timeline.png") 
print(f"     - æ€§èƒ½é›·è¾¾å›¾: transformer_performance_radar.png")
print(f"     - ä¸‰çª—å£è¿‡ç¨‹å›¾: transformer_three_window_process.png")
print(f"   â€¢ ç‰¹å®šæ ·æœ¬å¯è§†åŒ–: {result_dir}/sample_visualizations")
print(f"     - æ­£å¸¸æ ·æœ¬ (12,13,14) å’Œæ•…éšœæ ·æœ¬ (340,345,346,347) çš„è¯¦ç»†å›¾è¡¨")
print(f"     - æ–‡ä»¶æ ¼å¼: transformer_*_sample_[æ ·æœ¬ç¼–å·].png")
print(f"   â€¢ æ€§èƒ½æŒ‡æ ‡: transformer_performance_metrics.json")
print(f"   â€¢ ExcelæŠ¥å‘Š: transformer_summary.xlsx")

# ç»¼åˆæ€§èƒ½è¯„ä¼°
transformer_score = np.mean(list(performance_metrics["TRANSFORMER"]['classification_metrics'].values()))

print(f"\nğŸ† Transformerç»¼åˆæ€§èƒ½è¯„ä¼°:")
print(f"   ç»¼åˆå¾—åˆ†: {transformer_score:.3f}")

print("\n" + "="*80)
print("Transformeræµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
print("="*80)