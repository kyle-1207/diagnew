#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ­£è´Ÿåé¦ˆæ··åˆè®­ç»ƒè„šæœ¬ (Positive-Negative Hybrid Feedback Training)
åŸºäºTransformerçš„ç”µæ± æ•…éšœæ£€æµ‹ç³»ç»Ÿ

è®­ç»ƒæ ·æœ¬é…ç½®ï¼š
- è®­ç»ƒæ ·æœ¬ï¼š0-100 (åŸºç¡€è®­ç»ƒæ•°æ®)
- æ­£åé¦ˆæ ·æœ¬ï¼š101-120 (æ­£å¸¸æ ·æœ¬ï¼Œç”¨äºé™ä½å‡é˜³æ€§)
- è´Ÿåé¦ˆæ ·æœ¬ï¼š340-350 (æ•…éšœæ ·æœ¬ï¼Œç”¨äºå¢å¼ºåŒºåˆ†åº¦)

æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import warnings
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import platform
from datetime import datetime
import time
from tqdm import tqdm
import json

# æ·»åŠ æºä»£ç è·¯å¾„
sys.path.append('./æºä»£ç å¤‡ä»½')
sys.path.append('.')

# å¯¼å…¥å¿…è¦æ¨¡å—
from Function_ import *
from Class_ import *
from create_dataset import series_to_supervised
from sklearn import preprocessing
import scipy.io as scio
import torch.nn.functional as F
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

#=================================== é…ç½®å‚æ•° ===================================

# æ­£è´Ÿåé¦ˆæ··åˆè®­ç»ƒé…ç½®
PN_HYBRID_FEEDBACK_CONFIG = {
    # æ ·æœ¬é…ç½®
    'train_samples': list(range(0, 101)),        # 0-100: åŸºç¡€è®­ç»ƒæ ·æœ¬
    'positive_feedback_samples': list(range(101, 121)),  # 101-120: æ­£åé¦ˆæ ·æœ¬(æ­£å¸¸)
    'negative_feedback_samples': list(range(340, 351)),  # 340-350: è´Ÿåé¦ˆæ ·æœ¬(æ•…éšœ)
    
    # è®­ç»ƒé˜¶æ®µé…ç½®
    'training_phases': {
        'phase1_transformer': {
            'epochs': 50,
            'description': 'åŸºç¡€Transformerè®­ç»ƒ'
        },
        'phase2_mcae': {
            'epochs': 80,
            'description': 'MC-AEè®­ç»ƒ(ä½¿ç”¨Transformerå¢å¼ºæ•°æ®)'
        },
        'phase3_feedback': {
            'epochs': 30,
            'description': 'æ­£è´Ÿåé¦ˆæ··åˆä¼˜åŒ–'
        }
    },
    
    # æ­£åé¦ˆé…ç½®
    'positive_feedback': {
        'enable': True,
        'weight': 0.3,              # æ­£åé¦ˆæƒé‡
        'start_epoch': 10,          # å¼€å§‹è½®æ¬¡
        'frequency': 5,             # è¯„ä¼°é¢‘ç‡
        'target_fpr': 0.01,         # ç›®æ ‡å‡é˜³æ€§ç‡ 1%
        'adjustment_factor': 0.1    # è°ƒæ•´å› å­
    },
    
    # è´Ÿåé¦ˆé…ç½®
    'negative_feedback': {
        'enable': True,
        'alpha': 0.4,               # æ­£å¸¸æ ·æœ¬æŸå¤±æƒé‡
        'beta': 1.2,                # æ•…éšœæ ·æœ¬æŸå¤±æƒé‡  
        'margin': 0.15,             # å¯¹æ¯”å­¦ä¹ è¾¹ç•Œ
        'start_epoch': 20,          # å¼€å§‹è½®æ¬¡
        'evaluation_frequency': 3,   # è¯„ä¼°é¢‘ç‡
        'min_separation': 0.1       # æœ€å°åˆ†ç¦»åº¦è¦æ±‚
    },
    
    # æ¨¡å‹ä¿å­˜è·¯å¾„
    'save_base_path': '/mnt/bz25t/bzhy/datasave/Transformer/models/PN_model/',
    
    # è®­ç»ƒå‚æ•°
    'batch_size': 512,
    'learning_rate': 0.001,
    'device': 'cuda:0' if torch.cuda.is_available() else 'cpu'
}

print("ğŸš€ æ­£è´Ÿåé¦ˆæ··åˆè®­ç»ƒé…ç½®:")
print(f"   è®­ç»ƒæ ·æœ¬: {len(PN_HYBRID_FEEDBACK_CONFIG['train_samples'])}ä¸ª (0-100)")
print(f"   æ­£åé¦ˆæ ·æœ¬: {len(PN_HYBRID_FEEDBACK_CONFIG['positive_feedback_samples'])}ä¸ª (101-120)")
print(f"   è´Ÿåé¦ˆæ ·æœ¬: {len(PN_HYBRID_FEEDBACK_CONFIG['negative_feedback_samples'])}ä¸ª (340-350)")
print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {PN_HYBRID_FEEDBACK_CONFIG['save_base_path']}")

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(PN_HYBRID_FEEDBACK_CONFIG['save_base_path'], exist_ok=True)

#=================================== è®¾å¤‡é…ç½® ===================================

device = torch.device(PN_HYBRID_FEEDBACK_CONFIG['device'])
print(f"\nğŸ–¥ï¸ è®¾å¤‡é…ç½®: {device}")

if torch.cuda.is_available():
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {props.name} ({props.total_memory/1024**3:.1f}GB)")

#=================================== è¾…åŠ©å‡½æ•° ===================================

def print_gpu_memory():
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"   GPUå†…å­˜: å·²åˆ†é… {allocated:.2f}GB, å·²é¢„ç•™ {reserved:.2f}GB")

def setup_chinese_fonts():
    """é…ç½®ä¸­æ–‡å­—ä½“"""
    system = platform.system()
    if system == "Windows":
        chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi']
    elif system == "Linux":
        chinese_fonts = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'Source Han Sans CN', 'DejaVu Sans']
    elif system == "Darwin":
        chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
    else:
        chinese_fonts = ['DejaVu Sans', 'Arial Unicode MS']
    
    for font in chinese_fonts:
        try:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            break
        except:
            continue

def physics_based_data_processing_silent(data, feature_type='general'):
    """é™é»˜çš„åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†"""
    if isinstance(data, torch.Tensor):
        data_np = data.detach().cpu().numpy()
        is_tensor = True
        original_dtype = data.dtype
        original_device = data.device
    else:
        data_np = np.array(data)
        is_tensor = False
    
    if data_np.size == 0:
        return data if not is_tensor else torch.tensor(data_np, dtype=original_dtype, device=original_device)
    
    # å¤„ç†NaNå’ŒInf
    for col in range(data_np.shape[1] if len(data_np.shape) > 1 else 1):
        if len(data_np.shape) > 1:
            col_data = data_np[:, col]
        else:
            col_data = data_np
            
        # å¤„ç†NaN
        if np.isnan(col_data).any():
            valid_mask = ~np.isnan(col_data)
            if valid_mask.any():
                median_val = np.median(col_data[valid_mask])
                if len(data_np.shape) > 1:
                    data_np[~valid_mask, col] = median_val
                else:
                    data_np[~valid_mask] = median_val
        
        # å¤„ç†Inf
        if np.isinf(col_data).any():
            finite_mask = np.isfinite(col_data)
            if finite_mask.any():
                max_finite = np.max(col_data[finite_mask])
                min_finite = np.min(col_data[finite_mask])
                if len(data_np.shape) > 1:
                    data_np[col_data == np.inf, col] = max_finite
                    data_np[col_data == -np.inf, col] = min_finite
                else:
                    data_np[col_data == np.inf] = max_finite
                    data_np[col_data == -np.inf] = min_finite
    
    # åº”ç”¨ç‰©ç†çº¦æŸ
    if feature_type == 'voltage':
        data_np = np.clip(data_np, 2.5, 4.2)
    elif feature_type == 'soc':
        data_np = np.clip(data_np, 0.0, 1.0)
    elif feature_type == 'temperature':
        data_np = np.clip(data_np, -40, 80)
    
    if is_tensor:
        return torch.tensor(data_np, dtype=original_dtype, device=original_device)
    else:
        return data_np

#=================================== å¯¹æ¯”æŸå¤±å‡½æ•° ===================================

class ContrastiveMCAELoss(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œç”¨äºMC-AEè´Ÿåé¦ˆè®­ç»ƒ"""
    
    def __init__(self, alpha=0.4, beta=1.2, margin=0.15):
        super(ContrastiveMCAELoss, self).__init__()
        self.alpha = alpha      # æ­£å¸¸æ ·æœ¬æƒé‡
        self.beta = beta        # æ•…éšœæ ·æœ¬æƒé‡
        self.margin = margin    # å¯¹æ¯”è¾¹ç•Œ
        self.mse_loss = nn.MSELoss(reduction='mean')
    
    def forward(self, recon_normal, target_normal, recon_fault=None, target_fault=None):
        # æ­£å¸¸æ ·æœ¬é‡æ„æŸå¤±ï¼ˆå¸Œæœ›æœ€å°åŒ–ï¼‰
        positive_loss = self.mse_loss(recon_normal, target_normal)
        
        if recon_fault is not None and target_fault is not None:
            # æ•…éšœæ ·æœ¬é‡æ„æŸå¤±ï¼ˆå¸Œæœ›æœ€å¤§åŒ–ï¼Œä½†æœ‰è¾¹ç•Œï¼‰
            fault_loss = self.mse_loss(recon_fault, target_fault)
            
            # å¯¹æ¯”æŸå¤±ï¼šé¼“åŠ±æ•…éšœæ ·æœ¬æœ‰æ›´é«˜çš„é‡æ„è¯¯å·®
            negative_loss = torch.clamp(self.margin - fault_loss, min=0.0)
            
            # æ€»æŸå¤±
            total_loss = self.alpha * positive_loss + self.beta * negative_loss
            
            return total_loss, positive_loss, negative_loss
        else:
            return positive_loss, positive_loss, torch.tensor(0.0, device=positive_loss.device)

#=================================== Transformeræ¨¡å‹ ===================================

class TransformerPredictor(nn.Module):
    """åŸºäºTransformerçš„é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2):
        super(TransformerPredictor, self).__init__()
        self.input_size = input_size
        self.d_model = d_model
        self.output_size = output_size
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, d_model)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, output_size)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # x: [batch, input_size]
        batch_size = x.size(0)
        
        # æŠ•å½±åˆ°transformerç»´åº¦
        x = self.input_projection(x)  # [batch, d_model]
        
        # æ·»åŠ åºåˆ—ç»´åº¦
        x = x.unsqueeze(1)  # [batch, 1, d_model]
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # [batch, 1, d_model]
        
        # ç§»é™¤åºåˆ—ç»´åº¦å¹¶è¾“å‡º
        x = x.squeeze(1)  # [batch, d_model]
        output = self.output_projection(x)  # [batch, output_size]
        
        return output

#=================================== æ•°æ®åŠ è½½å‡½æ•° ===================================

def load_sample_data(sample_id, data_type='train'):
    """åŠ è½½å•ä¸ªæ ·æœ¬æ•°æ®"""
    try:
        if data_type == 'train':
            base_path = '/mnt/bz25t/bzhy/zhanglikang/project/DTI'
        else:
            base_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS'
        
        sample_path = f"{base_path}/{sample_id}"
        
        # åŠ è½½æ•°æ®æ–‡ä»¶
        vin_1 = pickle.load(open(f"{sample_path}/vin_1.pkl", 'rb'))
        vin_2 = pickle.load(open(f"{sample_path}/vin_2.pkl", 'rb'))
        vin_3 = pickle.load(open(f"{sample_path}/vin_3.pkl", 'rb'))
        targets = pickle.load(open(f"{sample_path}/targets.pkl", 'rb'))
        
        return {
            'vin_1': vin_1,
            'vin_2': vin_2, 
            'vin_3': vin_3,
            'targets': targets,
            'sample_id': sample_id
        }
    except Exception as e:
        print(f"   âŒ åŠ è½½æ ·æœ¬ {sample_id} å¤±è´¥: {e}")
        return None

def load_training_data(sample_ids):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    print(f"\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ® ({len(sample_ids)}ä¸ªæ ·æœ¬)...")
    
    all_vin1, all_targets = [], []
    successful_samples = []
    
    for sample_id in tqdm(sample_ids, desc="åŠ è½½è®­ç»ƒæ ·æœ¬"):
        data = load_sample_data(str(sample_id), 'train')
        if data is not None:
            all_vin1.append(data['vin_1'])
            all_targets.append(data['targets'])
            successful_samples.append(sample_id)
    
    if not all_vin1:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•è®­ç»ƒæ ·æœ¬ï¼")
    
    # åˆå¹¶æ•°æ®
    vin1_combined = np.vstack(all_vin1)
    targets_combined = np.vstack(all_targets)
    
    print(f"   âœ… æˆåŠŸåŠ è½½ {len(successful_samples)} ä¸ªæ ·æœ¬")
    print(f"   æ•°æ®å½¢çŠ¶: vin1 {vin1_combined.shape}, targets {targets_combined.shape}")
    
    return vin1_combined, targets_combined, successful_samples

def load_feedback_data(sample_ids, data_type='feedback'):
    """åŠ è½½åé¦ˆæ•°æ®"""
    print(f"\nğŸ“Š åŠ è½½{data_type}æ•°æ® ({len(sample_ids)}ä¸ªæ ·æœ¬)...")
    
    all_data = []
    successful_samples = []
    
    for sample_id in tqdm(sample_ids, desc=f"åŠ è½½{data_type}æ ·æœ¬"):
        # åé¦ˆæ ·æœ¬ä»QASç›®å½•åŠ è½½
        data = load_sample_data(str(sample_id), 'feedback')
        if data is not None:
            all_data.append(data)
            successful_samples.append(sample_id)
    
    print(f"   âœ… æˆåŠŸåŠ è½½ {len(successful_samples)} ä¸ª{data_type}æ ·æœ¬")
    return all_data, successful_samples

#=================================== æ•°æ®é›†ç±» ===================================

class TransformerDataset(Dataset):
    """Transformerè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, vin1_data, targets_data):
        self.vin1_data = torch.FloatTensor(vin1_data)
        self.targets_data = torch.FloatTensor(targets_data)
        
        # æ•°æ®å¤„ç†
        self.vin1_data = physics_based_data_processing_silent(self.vin1_data, 'general')
        self.targets_data = physics_based_data_processing_silent(self.targets_data, 'general')
    
    def __len__(self):
        return len(self.vin1_data)
    
    def __getitem__(self, idx):
        return self.vin1_data[idx], self.targets_data[idx]

class MCDataset(Dataset):
    """MC-AEè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, x, y, z, q):
        self.x = torch.FloatTensor(x)
        self.y = torch.FloatTensor(y) 
        self.z = torch.FloatTensor(z)
        self.q = torch.FloatTensor(q)
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.z[idx], self.q[idx]

#=================================== è¯„ä¼°å‡½æ•° ===================================

def evaluate_mcae_discrimination(mcae_model, normal_data, fault_data, device):
    """è¯„ä¼°MC-AEçš„åŒºåˆ†èƒ½åŠ›"""
    mcae_model.eval()
    
    normal_errors, fault_errors = [], []
    
    with torch.no_grad():
        # æ­£å¸¸æ ·æœ¬é‡æ„è¯¯å·®
        for data in normal_data:
            x, y = data[:2], data[2:]
            x, y = x.to(device), y.to(device)
            
            recon_x, recon_y = mcae_model(x, y)
            error = F.mse_loss(torch.cat([recon_x, recon_y], dim=1), 
                              torch.cat([x, y], dim=1), reduction='none').mean(dim=1)
            normal_errors.extend(error.cpu().numpy())
        
        # æ•…éšœæ ·æœ¬é‡æ„è¯¯å·®
        for data in fault_data:
            x, y = data[:2], data[2:]
            x, y = x.to(device), y.to(device)
            
            recon_x, recon_y = mcae_model(x, y)
            error = F.mse_loss(torch.cat([recon_x, recon_y], dim=1),
                              torch.cat([x, y], dim=1), reduction='none').mean(dim=1)
            fault_errors.extend(error.cpu().numpy())
    
    normal_errors = np.array(normal_errors)
    fault_errors = np.array(fault_errors)
    
    # è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
    normal_mean = np.mean(normal_errors)
    fault_mean = np.mean(fault_errors)
    separation = (fault_mean - normal_mean) / (np.std(normal_errors) + np.std(fault_errors) + 1e-8)
    
    return {
        'normal_mean': normal_mean,
        'fault_mean': fault_mean,
        'separation': separation,
        'normal_errors': normal_errors,
        'fault_errors': fault_errors
    }

#=================================== ä¸»è®­ç»ƒå‡½æ•° ===================================

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("="*80)
    print("ğŸš€ æ­£è´Ÿåé¦ˆæ··åˆè®­ç»ƒå¼€å§‹")
    print("="*80)
    
    config = PN_HYBRID_FEEDBACK_CONFIG
    
    # é…ç½®ä¸­æ–‡å­—ä½“
    setup_chinese_fonts()
    
    #=== ç¬¬1é˜¶æ®µ: åŠ è½½è®­ç»ƒæ•°æ® ===
    print("\n" + "="*50)
    print("ğŸ“Š ç¬¬1é˜¶æ®µ: æ•°æ®åŠ è½½")
    print("="*50)
    
    # åŠ è½½åŸºç¡€è®­ç»ƒæ•°æ®
    train_vin1, train_targets, successful_train = load_training_data(config['train_samples'])
    
    # åŠ è½½æ­£åé¦ˆæ•°æ®
    positive_data, successful_positive = load_feedback_data(
        config['positive_feedback_samples'], 'æ­£åé¦ˆ'
    )
    
    # åŠ è½½è´Ÿåé¦ˆæ•°æ®  
    negative_data, successful_negative = load_feedback_data(
        config['negative_feedback_samples'], 'è´Ÿåé¦ˆ'
    )
    
    print(f"\nğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(successful_train)} ä¸ª")
    print(f"   æ­£åé¦ˆæ ·æœ¬: {len(successful_positive)} ä¸ª") 
    print(f"   è´Ÿåé¦ˆæ ·æœ¬: {len(successful_negative)} ä¸ª")
    
    #=== ç¬¬2é˜¶æ®µ: TransformeråŸºç¡€è®­ç»ƒ ===
    print("\n" + "="*50)
    print("ğŸ¤– ç¬¬2é˜¶æ®µ: TransformeråŸºç¡€è®­ç»ƒ")
    print("="*50)
    
    # åˆ›å»ºTransformeræ¨¡å‹
    transformer = TransformerPredictor(
        input_size=7, 
        d_model=128, 
        nhead=8, 
        num_layers=3, 
        output_size=2
    ).to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TransformerDataset(train_vin1, train_targets)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    # è®­ç»ƒé…ç½®
    transformer_optimizer = optim.Adam(transformer.parameters(), lr=config['learning_rate'])
    transformer_criterion = nn.MSELoss()
    transformer_scheduler = optim.lr_scheduler.StepLR(transformer_optimizer, step_size=20, gamma=0.8)
    
    # è®­ç»ƒå¾ªç¯
    transformer_losses = []
    phase1_epochs = config['training_phases']['phase1_transformer']['epochs']
    
    print(f"å¼€å§‹Transformerè®­ç»ƒ ({phase1_epochs} è½®)...")
    
    for epoch in range(phase1_epochs):
        transformer.train()
        epoch_losses = []
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{phase1_epochs}")
        for batch_vin1, batch_targets in pbar:
            batch_vin1 = batch_vin1.to(device)
            batch_targets = batch_targets.to(device)
            
            # å‰å‘ä¼ æ’­
            transformer_optimizer.zero_grad()
            predictions = transformer(batch_vin1)
            loss = transformer_criterion(predictions, batch_targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0)
            transformer_optimizer.step()
            
            epoch_losses.append(loss.item())
            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})
        
        avg_loss = np.mean(epoch_losses)
        transformer_losses.append(avg_loss)
        transformer_scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}: Loss={avg_loss:.6f}, LR={transformer_scheduler.get_last_lr()[0]:.6f}")
            print_gpu_memory()
    
    print("âœ… TransformeråŸºç¡€è®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜Transformeræ¨¡å‹
    transformer_save_path = os.path.join(config['save_base_path'], 'transformer_model_pn.pth')
    torch.save(transformer.state_dict(), transformer_save_path)
    print(f"   æ¨¡å‹å·²ä¿å­˜: {transformer_save_path}")
    
    #=== ç¬¬3é˜¶æ®µ: ç”Ÿæˆå¢å¼ºæ•°æ®å¹¶è®­ç»ƒMC-AE ===
    print("\n" + "="*50)
    print("ğŸ”§ ç¬¬3é˜¶æ®µ: MC-AEè®­ç»ƒ(ä½¿ç”¨Transformerå¢å¼ºæ•°æ®)")
    print("="*50)
    
    # ä½¿ç”¨è®­ç»ƒå¥½çš„Transformerç”Ÿæˆé¢„æµ‹æ•°æ®
    transformer.eval()
    enhanced_vin2_data, enhanced_vin3_data = [], []
    
    print("ç”ŸæˆTransformerå¢å¼ºæ•°æ®...")
    with torch.no_grad():
        for batch_vin1, _ in tqdm(train_loader, desc="ç”Ÿæˆå¢å¼ºæ•°æ®"):
            batch_vin1 = batch_vin1.to(device)
            predictions = transformer(batch_vin1)
            
            # åˆ†ç¦»ç”µå‹å’ŒSOCé¢„æµ‹
            volt_pred = predictions[:, 0:1]  # ç”µå‹é¢„æµ‹
            soc_pred = predictions[:, 1:2]   # SOCé¢„æµ‹
            
            enhanced_vin2_data.append(volt_pred.cpu().numpy())
            enhanced_vin3_data.append(soc_pred.cpu().numpy())
    
    # åˆå¹¶å¢å¼ºæ•°æ®
    enhanced_vin2 = np.vstack(enhanced_vin2_data)
    enhanced_vin3 = np.vstack(enhanced_vin3_data)
    
    print(f"å¢å¼ºæ•°æ®ç”Ÿæˆå®Œæˆ: vin2 {enhanced_vin2.shape}, vin3 {enhanced_vin3.shape}")
    
    # å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®
    # è¿™é‡Œéœ€è¦æ ¹æ®åŸå§‹ä»£ç çš„æ•°æ®åˆ‡ç‰‡é€»è¾‘æ¥å‡†å¤‡x, y, z, qæ•°æ®
    # æš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“æ•°æ®ç»“æ„è°ƒæ•´
    
    print("å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®...")
    # ä»ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬è·å–æ•°æ®ç»“æ„ä¿¡æ¯
    sample_data = load_sample_data(str(successful_train[0]), 'train')
    vin_2_sample = sample_data['vin_2']
    vin_3_sample = sample_data['vin_3']
    
    # æ•°æ®ç»´åº¦ä¿¡æ¯
    dim_x, dim_y = 2, 3  # æ ¹æ®åŸå§‹ä»£ç è®¾å®š
    dim_z, dim_q = 2, 4  # æ ¹æ®åŸå§‹ä»£ç è®¾å®š
    
    # æ¨¡æ‹Ÿæ•°æ®åˆ‡ç‰‡ï¼ˆå®é™…åº”ç”¨æ—¶éœ€è¦å®Œæ•´å®ç°ï¼‰
    mc_x_data = enhanced_vin2[:, :dim_x] if enhanced_vin2.shape[1] >= dim_x else enhanced_vin2
    mc_y_data = np.random.randn(len(enhanced_vin2), dim_y)  # ä¸´æ—¶æ•°æ®
    mc_z_data = enhanced_vin3[:, :dim_z] if enhanced_vin3.shape[1] >= dim_z else enhanced_vin3
    mc_q_data = np.random.randn(len(enhanced_vin3), dim_q)  # ä¸´æ—¶æ•°æ®
    
    # åˆ›å»ºMC-AEæ¨¡å‹
    net_model = CombinedAE(
        input_size=dim_x, 
        encode2_input_size=dim_y,
        output_size=110,
        activation_fn=custom_activation,
        use_dx_in_forward=True
    ).to(device)
    
    netx_model = CombinedAE(
        input_size=dim_z,
        encode2_input_size=dim_q, 
        output_size=110,
        activation_fn=torch.sigmoid,
        use_dx_in_forward=True
    ).to(device)
    
    # MC-AEè®­ç»ƒæ•°æ®é›†
    mc_dataset = MCDataset(mc_x_data, mc_y_data, mc_z_data, mc_q_data)
    mc_loader = DataLoader(
        mc_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    # MC-AEè®­ç»ƒé…ç½®
    net_optimizer = optim.Adam(net_model.parameters(), lr=config['learning_rate'])
    netx_optimizer = optim.Adam(netx_model.parameters(), lr=config['learning_rate'])
    
    # è´Ÿåé¦ˆæŸå¤±å‡½æ•°
    contrastive_loss = ContrastiveMCAELoss(
        alpha=config['negative_feedback']['alpha'],
        beta=config['negative_feedback']['beta'],
        margin=config['negative_feedback']['margin']
    )
    
    phase2_epochs = config['training_phases']['phase2_mcae']['epochs']
    net_losses, netx_losses = [], []
    
    print(f"å¼€å§‹MC-AEè®­ç»ƒ ({phase2_epochs} è½®)...")
    
    for epoch in range(phase2_epochs):
        net_model.train()
        netx_model.train()
        
        epoch_net_losses, epoch_netx_losses = [], []
        
        pbar = tqdm(mc_loader, desc=f"MC-AE Epoch {epoch+1}/{phase2_epochs}")
        for batch_x, batch_y, batch_z, batch_q in pbar:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device) 
            batch_z = batch_z.to(device)
            batch_q = batch_q.to(device)
            
            # è®­ç»ƒnet_model (MC-AE1)
            net_optimizer.zero_grad()
            recon_x, recon_y = net_model(batch_x, batch_y)
            
            # ä½¿ç”¨è´Ÿåé¦ˆæŸå¤±
            if (epoch >= config['negative_feedback']['start_epoch'] and 
                config['negative_feedback']['enable'] and
                len(negative_data) > 0):
                
                # è¿™é‡Œåº”è¯¥åŠ è½½è´Ÿåé¦ˆæ ·æœ¬æ•°æ®ï¼Œæš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                net_loss, pos_loss, neg_loss = contrastive_loss(
                    torch.cat([recon_x, recon_y], dim=1),
                    torch.cat([batch_x, batch_y], dim=1)
                )
            else:
                net_loss = F.mse_loss(torch.cat([recon_x, recon_y], dim=1),
                                     torch.cat([batch_x, batch_y], dim=1))
            
            net_loss.backward()
            net_optimizer.step()
            epoch_net_losses.append(net_loss.item())
            
            # è®­ç»ƒnetx_model (MC-AE2)
            netx_optimizer.zero_grad()
            recon_z, recon_q = netx_model(batch_z, batch_q)
            
            if (epoch >= config['negative_feedback']['start_epoch'] and 
                config['negative_feedback']['enable'] and
                len(negative_data) > 0):
                
                netx_loss, pos_loss, neg_loss = contrastive_loss(
                    torch.cat([recon_z, recon_q], dim=1),
                    torch.cat([batch_z, batch_q], dim=1)
                )
            else:
                netx_loss = F.mse_loss(torch.cat([recon_z, recon_q], dim=1),
                                      torch.cat([batch_z, batch_q], dim=1))
            
            netx_loss.backward()
            netx_optimizer.step()
            epoch_netx_losses.append(netx_loss.item())
            
            pbar.set_postfix({
                'Net Loss': f'{net_loss.item():.6f}',
                'NetX Loss': f'{netx_loss.item():.6f}'
            })
        
        avg_net_loss = np.mean(epoch_net_losses)
        avg_netx_loss = np.mean(epoch_netx_losses)
        net_losses.append(avg_net_loss)
        netx_losses.append(avg_netx_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"MC-AE Epoch {epoch+1}: Net Loss={avg_net_loss:.6f}, NetX Loss={avg_netx_loss:.6f}")
            print_gpu_memory()
    
    print("âœ… MC-AEè®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜MC-AEæ¨¡å‹
    net_save_path = os.path.join(config['save_base_path'], 'net_model_pn.pth')
    netx_save_path = os.path.join(config['save_base_path'], 'netx_model_pn.pth')
    
    torch.save(net_model.state_dict(), net_save_path)
    torch.save(netx_model.state_dict(), netx_save_path)
    
    print(f"   MC-AE1æ¨¡å‹å·²ä¿å­˜: {net_save_path}")
    print(f"   MC-AE2æ¨¡å‹å·²ä¿å­˜: {netx_save_path}")
    
    #=== ç¬¬4é˜¶æ®µ: PCAåˆ†æå’Œé˜ˆå€¼è®¡ç®— ===
    print("\n" + "="*50)
    print("ğŸ“Š ç¬¬4é˜¶æ®µ: PCAåˆ†æå’Œé˜ˆå€¼è®¡ç®—")
    print("="*50)
    
    # è®¡ç®—é‡æ„è¯¯å·®ç‰¹å¾
    print("è®¡ç®—é‡æ„è¯¯å·®ç‰¹å¾...")
    net_model.eval()
    netx_model.eval()
    
    all_features = []
    with torch.no_grad():
        for batch_x, batch_y, batch_z, batch_q in tqdm(mc_loader, desc="è®¡ç®—ç‰¹å¾"):
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            batch_z = batch_z.to(device) 
            batch_q = batch_q.to(device)
            
            # MC-AE1é‡æ„è¯¯å·®
            recon_x, recon_y = net_model(batch_x, batch_y)
            error1 = F.mse_loss(torch.cat([recon_x, recon_y], dim=1),
                               torch.cat([batch_x, batch_y], dim=1), 
                               reduction='none').mean(dim=1)
            
            # MC-AE2é‡æ„è¯¯å·®
            recon_z, recon_q = netx_model(batch_z, batch_q)
            error2 = F.mse_loss(torch.cat([recon_z, recon_q], dim=1),
                               torch.cat([batch_z, batch_q], dim=1),
                               reduction='none').mean(dim=1)
            
            # åˆå¹¶ç‰¹å¾
            features = torch.stack([error1, error2], dim=1)
            all_features.append(features.cpu().numpy())
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    features_combined = np.vstack(all_features)
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features_combined.shape}")
    
    # PCAåˆ†æ
    print("æ‰§è¡ŒPCAåˆ†æ...")
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_combined)
    
    pca = PCA()
    pca_features = pca.fit_transform(features_scaled)
    
    # é€‰æ‹©ä¸»æˆåˆ†æ•°é‡(ä¿ç•™90%æ–¹å·®)
    cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
    n_components = np.argmax(cumsum_ratio >= 0.90) + 1
    
    print(f"PCAåˆ†æå®Œæˆ:")
    print(f"   ä¸»æˆåˆ†æ•°é‡: {n_components}")
    print(f"   æ–¹å·®è§£é‡Šæ¯”ä¾‹: {cumsum_ratio[n_components-1]:.4f}")
    
    # è®¡ç®—æ§åˆ¶é™
    pca_reduced = pca_features[:, :n_components]
    
    # TÂ²ç»Ÿè®¡é‡
    eigenvalues = pca.explained_variance_[:n_components]
    T2_stats = np.sum((pca_reduced ** 2) / eigenvalues, axis=1)
    
    # SPEç»Ÿè®¡é‡  
    reconstructed = pca_reduced @ pca.components_[:n_components]
    residuals = features_scaled - reconstructed
    SPE_stats = np.sum(residuals ** 2, axis=1)
    
    # è®¡ç®—æ§åˆ¶é™
    T2_99_limit = np.percentile(T2_stats, 99)
    SPE_99_limit = np.percentile(SPE_stats, 99)
    
    # ç»¼åˆæ•…éšœæŒ‡æ ‡
    FAI = (T2_stats / T2_99_limit + SPE_stats / SPE_99_limit) / 2
    
    print(f"æ§åˆ¶é™è®¡ç®—å®Œæˆ:")
    print(f"   TÂ²-99%æ§åˆ¶é™: {T2_99_limit:.4f}")
    print(f"   SPE-99%æ§åˆ¶é™: {SPE_99_limit:.4f}")
    print(f"   FAIèŒƒå›´: [{np.min(FAI):.4f}, {np.max(FAI):.4f}]")
    
    # ä¿å­˜PCAå‚æ•°
    pca_params = {
        'pca_model': pca,
        'scaler': scaler,
        'n_components': n_components,
        'T2_99_limit': T2_99_limit,
        'SPE_99_limit': SPE_99_limit,
        'eigenvalues': eigenvalues,
        'explained_variance_ratio': pca.explained_variance_ratio_,
        'components': pca.components_
    }
    
    pca_save_path = os.path.join(config['save_base_path'], 'pca_params_pn.pkl')
    with open(pca_save_path, 'wb') as f:
        pickle.dump(pca_params, f)
    print(f"   PCAå‚æ•°å·²ä¿å­˜: {pca_save_path}")
    
    #=== ç¬¬5é˜¶æ®µ: è®­ç»ƒç»“æœå¯è§†åŒ– ===
    print("\n" + "="*50)
    print("ğŸ“ˆ ç¬¬5é˜¶æ®µ: è®­ç»ƒç»“æœå¯è§†åŒ–")
    print("="*50)
    
    # åˆ›å»ºè®­ç»ƒæŸå¤±å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # TransformeræŸå¤±
    axes[0, 0].plot(transformer_losses, 'b-', linewidth=2)
    axes[0, 0].set_title('Transformerè®­ç»ƒæŸå¤±', fontsize=14)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].grid(True, alpha=0.3)
    
    # MC-AEæŸå¤±
    axes[0, 1].plot(net_losses, 'r-', label='MC-AE1', linewidth=2)
    axes[0, 1].plot(netx_losses, 'g-', label='MC-AE2', linewidth=2)
    axes[0, 1].set_title('MC-AEè®­ç»ƒæŸå¤±', fontsize=14)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # FAIåˆ†å¸ƒ
    axes[1, 0].hist(FAI, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[1, 0].axvline(1.0, color='red', linestyle='--', linewidth=2, label='é˜ˆå€¼=1.0')
    axes[1, 0].set_title('FAIåˆ†å¸ƒ', fontsize=14)
    axes[1, 0].set_xlabel('FAIå€¼')
    axes[1, 0].set_ylabel('é¢‘æ•°')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # PCAæ–¹å·®è§£é‡Šæ¯”ä¾‹
    axes[1, 1].plot(range(1, len(cumsum_ratio)+1), cumsum_ratio, 'mo-', linewidth=2)
    axes[1, 1].axhline(0.90, color='red', linestyle='--', linewidth=2, label='90%é˜ˆå€¼')
    axes[1, 1].axvline(n_components, color='green', linestyle='--', linewidth=2, 
                      label=f'é€‰æ‹©{n_components}ä¸ªä¸»æˆåˆ†')
    axes[1, 1].set_title('PCAç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹', fontsize=14)
    axes[1, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
    axes[1, 1].set_ylabel('ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    plot_save_path = os.path.join(config['save_base_path'], 'pn_training_results.png')
    plt.savefig(plot_save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"   è®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {plot_save_path}")
    
    #=== è®­ç»ƒå®Œæˆæ€»ç»“ ===
    print("\n" + "="*80)
    print("ğŸ‰ æ­£è´Ÿåé¦ˆæ··åˆè®­ç»ƒå®Œæˆï¼")
    print("="*80)
    
    print("ğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(successful_train)} ä¸ª (0-100)")
    print(f"   æ­£åé¦ˆæ ·æœ¬: {len(successful_positive)} ä¸ª (101-120)")
    print(f"   è´Ÿåé¦ˆæ ·æœ¬: {len(successful_negative)} ä¸ª (340-350)")
    print(f"   Transformeræœ€ç»ˆæŸå¤±: {transformer_losses[-1]:.6f}")
    print(f"   MC-AE1æœ€ç»ˆæŸå¤±: {net_losses[-1]:.6f}")
    print(f"   MC-AE2æœ€ç»ˆæŸå¤±: {netx_losses[-1]:.6f}")
    print(f"   PCAä¸»æˆåˆ†æ•°é‡: {n_components}")
    print(f"   FAIå¹³å‡å€¼: {np.mean(FAI):.4f}")
    
    print(f"\nğŸ’¾ æ¨¡å‹æ–‡ä»¶:")
    print(f"   Transformer: {transformer_save_path}")
    print(f"   MC-AE1: {net_save_path}")
    print(f"   MC-AE2: {netx_save_path}")
    print(f"   PCAå‚æ•°: {pca_save_path}")
    print(f"   è®­ç»ƒç»“æœå›¾: {plot_save_path}")
    
    # ä¿å­˜è®­ç»ƒé…ç½®å’Œç»“æœ
    results_summary = {
        'config': config,
        'training_results': {
            'successful_train_samples': successful_train,
            'successful_positive_samples': successful_positive,
            'successful_negative_samples': successful_negative,
            'transformer_final_loss': transformer_losses[-1],
            'mcae1_final_loss': net_losses[-1],
            'mcae2_final_loss': netx_losses[-1],
            'pca_components': n_components,
            'fai_mean': float(np.mean(FAI)),
            'fai_std': float(np.std(FAI)),
            'T2_99_limit': float(T2_99_limit),
            'SPE_99_limit': float(SPE_99_limit)
        },
        'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    summary_save_path = os.path.join(config['save_base_path'], 'training_summary_pn.json')
    with open(summary_save_path, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    print(f"   è®­ç»ƒæ€»ç»“: {summary_save_path}")
    print("\nğŸš€ è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    main()