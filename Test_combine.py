# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from Comprehensive_calculation import Comprehensive_calculation

# æ–°å¢å¯¼å…¥
from tqdm import tqdm
import json
import time
from datetime import datetime
from sklearn.metrics import roc_curve, auc, confusion_matrix
import glob

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
print(torch.cuda.device_count())

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK SC']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

#----------------------------------------æµ‹è¯•é…ç½®------------------------------
print("="*60)
print("ğŸ”¬ ç”µæ± æ•…éšœè¯Šæ–­ç³»ç»Ÿ - åŒæ¨¡å‹æ¯”å¯¹æµ‹è¯•")
print("="*60)

TEST_MODE = "COMPARE"  # å›ºå®šä¸ºæ¯”å¯¹æ¨¡å¼

# æµ‹è¯•æ•°æ®é›†é…ç½® (æ ¹æ®Labels.xlsåŠ¨æ€åŠ è½½)
def load_test_samples():
    """ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬"""
    try:
        import pandas as pd
        labels_path = '../QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–æµ‹è¯•æ ·æœ¬
        all_samples = df['Num'].tolist()
        all_labels = df['Label'].tolist()
        
        # 201-334çš„æ­£å¸¸æ ·æœ¬
        test_normal_samples = [str(i) for i in all_samples if 201 <= i <= 334 and all_labels[all_samples.index(i)] == 0]
        # 335-392çš„æ•…éšœæ ·æœ¬
        test_fault_samples = [str(i) for i in all_samples if 335 <= i <= 392 and all_labels[all_samples.index(i)] == 1]
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬:")
        print(f"   æµ‹è¯•æ­£å¸¸æ ·æœ¬ (201-334): {len(test_normal_samples)} ä¸ª")
        print(f"   æµ‹è¯•æ•…éšœæ ·æœ¬ (335-392): {len(test_fault_samples)} ä¸ª")
        
        return {
            'normal': test_normal_samples,
            'fault': test_fault_samples
        }
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æµ‹è¯•æ ·æœ¬")
        return {
            'normal': ['166', '209'],
            'fault': ['335', '386']
        }

TEST_SAMPLES = load_test_samples()
ALL_TEST_SAMPLES = TEST_SAMPLES['normal'] + TEST_SAMPLES['fault']

# æ¨¡å‹è·¯å¾„é…ç½®
MODEL_PATHS = {
    "BILSTM": {
        "net_model": "./models/net_model_bilstm_baseline.pth",
        "netx_model": "./models/netx_model_bilstm_baseline.pth",
        "pca_files": ["./models/v_I_bilstm_baseline.npy", "./models/v_bilstm_baseline.npy", 
                     "./models/v_ratio_bilstm_baseline.npy", "./models/p_k_bilstm_baseline.npy",
                     "./models/data_mean_bilstm_baseline.npy", "./models/data_std_bilstm_baseline.npy",
                     "./models/T_95_limit_bilstm_baseline.npy", "./models/T_99_limit_bilstm_baseline.npy",
                     "./models/SPE_95_limit_bilstm_baseline.npy", "./models/SPE_99_limit_bilstm_baseline.npy",
                     "./models/P_bilstm_baseline.npy", "./models/k_bilstm_baseline.npy",
                     "./models/P_t_bilstm_baseline.npy", "./models/X_bilstm_baseline.npy",
                     "./models/data_nor_bilstm_baseline.npy"]
    },
    "TRANSFORMER": {
        "transformer_model": "./models/transformer_model.pth",
        "net_model": "./models/net_model_transformer.pth", 
        "netx_model": "./models/netx_model_transformer.pth",
        "pca_files": ["./models/v_I_transformer.npy", "./models/v_transformer.npy",
                     "./models/v_ratio_transformer.npy", "./models/p_k_transformer.npy",
                     "./models/data_mean_transformer.npy", "./models/data_std_transformer.npy",
                     "./models/T_95_limit_transformer.npy", "./models/T_99_limit_transformer.npy",
                     "./models/SPE_95_limit_transformer.npy", "./models/SPE_99_limit_transformer.npy",
                     "./models/P_transformer.npy", "./models/k_transformer.npy",
                     "./models/P_t_transformer.npy", "./models/X_transformer.npy",
                     "./models/data_nor_transformer.npy"]
    }
}

# ä¸‰çª—å£å›ºå®šå‚æ•°
WINDOW_CONFIG = {
    "detection_window": 100,     # æ£€æµ‹çª—å£ï¼š100ä¸ªé‡‡æ ·ç‚¹
    "verification_window": 50,   # éªŒè¯çª—å£ï¼š50ä¸ªé‡‡æ ·ç‚¹  
    "marking_window": 50        # æ ‡è®°çª—å£ï¼šå‰åå„50ä¸ªé‡‡æ ·ç‚¹
}

# é«˜åˆ†è¾¨ç‡å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (15, 12),
    "figsize_medium": (12, 8), 
    "bbox_inches": "tight"
}

print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   æµ‹è¯•æ ·æœ¬: {ALL_TEST_SAMPLES}")
print(f"   æ­£å¸¸æ ·æœ¬: {TEST_SAMPLES['normal']}")
print(f"   æ•…éšœæ ·æœ¬: {TEST_SAMPLES['fault']}")
print(f"   ä¸‰çª—å£å‚æ•°: {WINDOW_CONFIG}")

#----------------------------------------æ¨¡å‹æ–‡ä»¶æ£€æŸ¥------------------------------
def check_model_files():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    
    missing_files = []
    
    for model_type, paths in MODEL_PATHS.items():
        print(f"  æ£€æŸ¥ {model_type} æ¨¡å‹æ–‡ä»¶...")
        
        # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶
        for key, path in paths.items():
            if key != "pca_files":
                if not os.path.exists(path):
                    missing_files.append(f"{model_type}: {path}")
            else:
                # æ£€æŸ¥PCAç›¸å…³æ–‡ä»¶
                for pca_file in path:
                    if not os.path.exists(pca_file):
                        missing_files.append(f"{model_type}: {pca_file}")
    
    if missing_files:
        print("âŒ ç¼ºå¤±æ¨¡å‹æ–‡ä»¶:")
        for file in missing_files:
            print(f"   {file}")
        raise FileNotFoundError("è¯·å…ˆè¿è¡Œå¯¹åº”çš„è®­ç»ƒè„šæœ¬ç”Ÿæˆæ‰€éœ€æ¨¡å‹æ–‡ä»¶")
    
    print("âœ… æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

# æ‰§è¡Œæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
check_model_files()

#----------------------------------------ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶------------------------------
def three_window_fault_detection(fai_values, threshold1, sample_id):
    """
    ä¸‰çª—å£æ•…éšœæ£€æµ‹æœºåˆ¶ï¼šæ£€æµ‹â†’éªŒè¯â†’æ ‡è®°
    
    Args:
        fai_values: ç»¼åˆè¯Šæ–­æŒ‡æ ‡åºåˆ—
        threshold1: ä¸€çº§é¢„è­¦é˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    detection_window = WINDOW_CONFIG["detection_window"]
    verification_window = WINDOW_CONFIG["verification_window"] 
    marking_window = WINDOW_CONFIG["marking_window"]
    
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'candidate_points': [],
        'verified_points': [],
        'marked_regions': [],
        'window_stats': {}
    }
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - å¯»æ‰¾å€™é€‰æ•…éšœç‚¹
    candidate_points = []
    for i in range(len(fai_values)):
        if fai_values[i] > threshold1:
            candidate_points.append(i)
    
    detection_info['candidate_points'] = candidate_points
    
    if len(candidate_points) == 0:
        # æ²¡æœ‰å€™é€‰ç‚¹ï¼Œç›´æ¥è¿”å›
        return fault_labels, detection_info
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - æ£€æŸ¥æŒç»­æ€§
    verified_points = []
    for candidate in candidate_points:
        # å®šä¹‰éªŒè¯çª—å£èŒƒå›´
        start_verify = max(0, candidate - verification_window//2)
        end_verify = min(len(fai_values), candidate + verification_window//2)
        verify_data = fai_values[start_verify:end_verify]
        
        # æŒç»­æ€§åˆ¤æ–­ï¼šéªŒè¯çª—å£å†…è¶…é˜ˆå€¼ç‚¹æ¯”ä¾‹
        continuous_ratio = np.sum(verify_data > threshold1) / len(verify_data)
        
        # 30%ä»¥ä¸Šè¶…é˜ˆå€¼è®¤ä¸ºæŒç»­å¼‚å¸¸
        if continuous_ratio >= 0.3:
            verified_points.append({
                'point': candidate,
                'continuous_ratio': continuous_ratio,
                'verify_range': (start_verify, end_verify)
            })
    
    detection_info['verified_points'] = verified_points
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ ‡è®°æ•…éšœåŒºåŸŸ
    marked_regions = []
    for verified in verified_points:
        candidate = verified['point']
        
        # å®šä¹‰æ ‡è®°çª—å£èŒƒå›´
        start_mark = max(0, candidate - marking_window)
        end_mark = min(len(fai_values), candidate + marking_window)
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start_mark:end_mark] = 1
        
        marked_regions.append({
            'center': candidate,
            'range': (start_mark, end_mark),
            'length': end_mark - start_mark
        })
    
    detection_info['marked_regions'] = marked_regions
    
    # ç»Ÿè®¡ä¿¡æ¯
    detection_info['window_stats'] = {
        'total_candidates': len(candidate_points),
        'verified_candidates': len(verified_points),
        'total_fault_points': np.sum(fault_labels),
        'fault_ratio': np.sum(fault_labels) / len(fault_labels)
    }
    
    return fault_labels, detection_info

#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_test_sample(sample_id):
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    base_path = f'../QAS/{sample_id}'
    
    # æ£€æŸ¥æ ·æœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_path):
        raise FileNotFoundError(f"æµ‹è¯•æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_path}")
    
    # åŠ è½½vin_1, vin_2, vin_3æ•°æ®
    try:
        with open(f'{base_path}/vin_1.pkl', 'rb') as f:
            vin1_data = pickle.load(f)
        with open(f'{base_path}/vin_2.pkl', 'rb') as f:
            vin2_data = pickle.load(f) 
        with open(f'{base_path}/vin_3.pkl', 'rb') as f:
            vin3_data = pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"æ ·æœ¬ {sample_id} æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        
    return vin1_data, vin2_data, vin3_data

def load_models(model_type):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    models = {}
    
    if model_type == "BILSTM":
        # åŠ è½½BiLSTMåŸºå‡†æ¨¡å‹
        models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                            activation_fn=custom_activation, use_dx_in_forward=True).to(device)
        models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                                   activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)

    # åŠ è½½æ¨¡å‹å‚æ•°
        models['net'].load_state_dict(torch.load(MODEL_PATHS[model_type]["net_model"]))
        models['netx'].load_state_dict(torch.load(MODEL_PATHS[model_type]["netx_model"]))
        
        # åŠ è½½PCAå‚æ•°
        models['pca_params'] = {}
        pca_files = MODEL_PATHS[model_type]["pca_files"]
        models['pca_params']['v_I'] = np.load(pca_files[0])
        models['pca_params']['v'] = np.load(pca_files[1])
        models['pca_params']['v_ratio'] = np.load(pca_files[2])
        models['pca_params']['p_k'] = np.load(pca_files[3])
        models['pca_params']['data_mean'] = np.load(pca_files[4])
        models['pca_params']['data_std'] = np.load(pca_files[5])
        models['pca_params']['T_95_limit'] = np.load(pca_files[6])
        models['pca_params']['T_99_limit'] = np.load(pca_files[7])
        models['pca_params']['SPE_95_limit'] = np.load(pca_files[8])
        models['pca_params']['SPE_99_limit'] = np.load(pca_files[9])
        models['pca_params']['P'] = np.load(pca_files[10])
        models['pca_params']['k'] = np.load(pca_files[11])
        models['pca_params']['P_t'] = np.load(pca_files[12])
        models['pca_params']['X'] = np.load(pca_files[13])
        models['pca_params']['data_nor'] = np.load(pca_files[14])
        
    elif model_type == "TRANSFORMER":
        # åŠ è½½Transformeræ¨¡å‹
        from Train_Transformer import TransformerPredictor
        models['transformer'] = TransformerPredictor().to(device)
        models['transformer'].load_state_dict(torch.load(MODEL_PATHS[model_type]["transformer_model"]))
        
        # åŠ è½½MC-AEæ¨¡å‹
        models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                                  activation_fn=custom_activation, use_dx_in_forward=True).to(device)
        models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                                   activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
        
        models['net'].load_state_dict(torch.load(MODEL_PATHS[model_type]["net_model"]))
        models['netx'].load_state_dict(torch.load(MODEL_PATHS[model_type]["netx_model"]))
        
        # åŠ è½½PCAå‚æ•°
        models['pca_params'] = {}
        pca_files = MODEL_PATHS[model_type]["pca_files"]
        models['pca_params']['v_I'] = np.load(pca_files[0])
        models['pca_params']['v'] = np.load(pca_files[1])
        models['pca_params']['v_ratio'] = np.load(pca_files[2])
        models['pca_params']['p_k'] = np.load(pca_files[3])
        models['pca_params']['data_mean'] = np.load(pca_files[4])
        models['pca_params']['data_std'] = np.load(pca_files[5])
        models['pca_params']['T_95_limit'] = np.load(pca_files[6])
        models['pca_params']['T_99_limit'] = np.load(pca_files[7])
        models['pca_params']['SPE_95_limit'] = np.load(pca_files[8])
        models['pca_params']['SPE_99_limit'] = np.load(pca_files[9])
        models['pca_params']['P'] = np.load(pca_files[10])
        models['pca_params']['k'] = np.load(pca_files[11])
        models['pca_params']['P_t'] = np.load(pca_files[12])
        models['pca_params']['X'] = np.load(pca_files[13])
        models['pca_params']['data_nor'] = np.load(pca_files[14])
    
    return models

#----------------------------------------å•æ ·æœ¬å¤„ç†å‡½æ•°------------------------------
def process_single_sample(sample_id, models, model_type):
    """å¤„ç†å•ä¸ªæµ‹è¯•æ ·æœ¬"""
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
    
    # æ•°æ®é¢„å¤„ç†
    if len(vin1_data.shape) == 2:
        vin1_data = vin1_data.unsqueeze(1)
    vin1_data = vin1_data.to(torch.float32).to(device)

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è€Œä¸æ˜¯é‡æ–°è®¡ç®—
    pca_params = models['pca_params']
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
    
    # MC-AEæ¨ç†
    models['net'].eval()
    models['netx'].eval()
    
    with torch.no_grad():
        models['net'] = models['net'].double()
        models['netx'] = models['netx'].double()
        
        recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
        reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    AA = recon_imtest[0].cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU

    BB = reconx_imtest[0].cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX

    # è¯Šæ–­ç‰¹å¾æå–
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—
    time = np.arange(df_data.shape[0])
    
    lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
        df_data.values, 
        pca_params['data_mean'], 
        pca_params['data_std'], 
        pca_params['v'].reshape(len(pca_params['v']), 1), 
        pca_params['p_k'], 
        pca_params['v_I'], 
        pca_params['T_99_limit'], 
        pca_params['SPE_99_limit'], 
        pca_params['X'], 
        time
    )
    
    # è®¡ç®—é˜ˆå€¼ - ä¸æºä»£ç ä¿æŒä¸€è‡´
    nm = 3000  # å›ºå®šå€¼ï¼Œä¸æºä»£ç ä¸€è‡´
    mm = len(fai)  # æ•°æ®æ€»é•¿åº¦
    
    # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
    if mm > nm:
        # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
        threshold1 = np.mean(fai[nm:mm]) + 3*np.std(fai[nm:mm])
        threshold2 = np.mean(fai[nm:mm]) + 4.5*np.std(fai[nm:mm])
        threshold3 = np.mean(fai[nm:mm]) + 6*np.std(fai[nm:mm])
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        print(f"   âš ï¸ æ ·æœ¬{sample_id}æ•°æ®é•¿åº¦({mm})ä¸è¶³3000ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—é˜ˆå€¼")
        threshold1 = np.mean(fai) + 3*np.std(fai)
        threshold2 = np.mean(fai) + 4.5*np.std(fai)
        threshold3 = np.mean(fai) + 6*np.std(fai)
    
    # ä¸‰çª—å£æ•…éšœæ£€æµ‹
    fault_labels, detection_info = three_window_fault_detection(fai, threshold1, sample_id)
    
    # æ„å»ºç»“æœ
    sample_result = {
        'sample_id': sample_id,
        'model_type': model_type,
        'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
        'df_data': df_data.values,
        'fai': fai,
        'T_squared': t_total,
        'SPE': q_total,
        'thresholds': {
            'threshold1': threshold1,
            'threshold2': threshold2, 
            'threshold3': threshold3
        },
        'fault_labels': fault_labels,
        'detection_info': detection_info,
        'performance_metrics': {
            'fai_mean': np.mean(fai),
            'fai_std': np.std(fai),
            'fai_max': np.max(fai),
            'fai_min': np.min(fai),
            'anomaly_count': np.sum(fai > threshold1),
            'anomaly_ratio': np.sum(fai > threshold1) / len(fai)
        }
    }
    
    return sample_result

#----------------------------------------ä¸»æµ‹è¯•æµç¨‹------------------------------
def main_test_process():
    """ä¸»è¦æµ‹è¯•æµç¨‹"""
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    test_results = {
        "BILSTM": [],
        "TRANSFORMER": [],
        "metadata": {
            "test_samples": TEST_SAMPLES,
            "window_config": WINDOW_CONFIG,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    # åŒæ¨¡å‹æµ‹è¯•å¾ªç¯
    total_operations = len(ALL_TEST_SAMPLES) * 2  # 4ä¸ªæ ·æœ¬ Ã— 2ä¸ªæ¨¡å‹
    
    print(f"\nğŸš€ å¼€å§‹åŒæ¨¡å‹æ¯”å¯¹æµ‹è¯•...")
    print(f"æ€»å…±éœ€è¦å¤„ç†: {total_operations} ä¸ªä»»åŠ¡")
    
    with tqdm(total=total_operations, desc="åŒæ¨¡å‹æµ‹è¯•è¿›åº¦", 
              bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}]') as pbar:
        
        for model_type in ["BILSTM", "TRANSFORMER"]:
            print(f"\n{'='*20} æµ‹è¯• {model_type} æ¨¡å‹ {'='*20}")
            
            # åŠ è½½æ¨¡å‹
            pbar.set_description(f"åŠ è½½{model_type}æ¨¡å‹")
            models = load_models(model_type)
            print(f"âœ… {model_type} æ¨¡å‹åŠ è½½å®Œæˆ")
            
            for sample_id in ALL_TEST_SAMPLES:
                pbar.set_description(f"{model_type}-æ ·æœ¬{sample_id}")
                
                try:
                    # å¤„ç†å•ä¸ªæ ·æœ¬
                    sample_result = process_single_sample(sample_id, models, model_type)
                    test_results[model_type].append(sample_result)
                    
                    # è¾“å‡ºç®€è¦ç»“æœ
                    metrics = sample_result['performance_metrics']
                    print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics['fai_mean']:.6f}, "
                          f"å¼‚å¸¸ç‡={metrics['anomaly_ratio']:.2%}, "
                          f"ä¸‰çª—å£æ£€æµ‹={sample_result['detection_info']['window_stats']['fault_ratio']:.2%}")
                    
                except Exception as e:
                    print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                    continue
                
                pbar.update(1)
                time.sleep(0.1)  # é¿å…è¿›åº¦æ¡æ›´æ–°è¿‡å¿«
    
    print(f"\nâœ… åŒæ¨¡å‹æµ‹è¯•å®Œæˆ!")
    print(f"   BiLSTM: æˆåŠŸå¤„ç† {len(test_results['BILSTM'])} ä¸ªæ ·æœ¬")
    print(f"   Transformer: æˆåŠŸå¤„ç† {len(test_results['TRANSFORMER'])} ä¸ªæ ·æœ¬")
    
    return test_results

# æ‰§è¡Œä¸»æµ‹è¯•æµç¨‹
test_results = main_test_process()

#----------------------------------------æ€§èƒ½åˆ†æå‡½æ•°------------------------------
def calculate_performance_metrics(test_results):
    """è®¡ç®—åŒæ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ”¬ è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    
    performance_metrics = {}
    
    for model_type in ["BILSTM", "TRANSFORMER"]:
        model_results = test_results[model_type]
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
        all_true_labels = []
        all_fai_values = []
        all_fault_predictions = []
        
        for result in model_results:
            true_label = result['label']
            fai_values = result['fai']
            fault_labels = result['fault_labels']
            threshold1 = result['thresholds']['threshold1']
            
            # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
            for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
                all_true_labels.append(true_label)
                all_fai_values.append(fai_val)
                
                # æ ¹æ®æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ROCé€»è¾‘ï¼š
                if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                    # æ­£å¸¸æ ·æœ¬ä¸­ï¼šfai > threshold å°±æ˜¯FPï¼Œfai <= threshold å°±æ˜¯TN
                    prediction = 1 if fai_val > threshold1 else 0
                else:  # æ•…éšœæ ·æœ¬
                    # æ•…éšœæ ·æœ¬ä¸­ï¼šéœ€è¦fai > threshold ä¸” ä¸‰çª—å£ç¡®è®¤ä¸ºæ•…éšœ æ‰æ˜¯TP
                    if fai_val > threshold1 and fault_pred == 1:
                        prediction = 1  # TP
                    else:
                        prediction = 0  # FN
                
                all_fault_predictions.append(prediction)
        
        # è®¡ç®—ROCæŒ‡æ ‡
        all_true_labels = np.array(all_true_labels)
        all_fai_values = np.array(all_fai_values)
        all_fault_predictions = np.array(all_fault_predictions)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
        fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
        fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
        tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        tpr = recall  # True Positive Rate = Recall
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
        
        # æ ·æœ¬çº§ç»Ÿè®¡
        sample_metrics = {
            'total_samples': len(model_results),
            'normal_samples': len([r for r in model_results if r['label'] == 0]),
            'fault_samples': len([r for r in model_results if r['label'] == 1]),
            'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                     for r in model_results if r['label'] == 0]),
            'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                    for r in model_results if r['label'] == 1]),
            'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                               for r in model_results if r['label'] == 0]),
            'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                              for r in model_results if r['label'] == 1])
        }
        
        performance_metrics[model_type] = {
            'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
            'classification_metrics': {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1_score),
                'specificity': float(specificity),
                'tpr': float(tpr),
                'fpr': float(fpr)
            },
            'sample_metrics': sample_metrics,
            'roc_data': {
                'true_labels': all_true_labels.tolist(),
                'fai_values': all_fai_values.tolist(),
                'predictions': all_fault_predictions.tolist()
            }
        }
    
    return performance_metrics

#----------------------------------------ROCæ›²çº¿å¯¹æ¯”------------------------------
def create_roc_comparison(test_results, performance_metrics, save_path):
    """ç”ŸæˆROCæ›²çº¿å¯¹æ¯”å›¾"""
    print("   ğŸ“ˆ ç”ŸæˆROCæ›²çº¿å¯¹æ¯”...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"])
    
    # === å­å›¾1: è¿ç»­é˜ˆå€¼ROCæ›²çº¿ ===
    ax1.set_title('(a) ROCæ›²çº¿å¯¹æ¯”\n(è¿ç»­é˜ˆå€¼æ‰«æ)')
    
    colors = {'BILSTM': 'green', 'TRANSFORMER': 'blue'}
    
    for model_type in ["BILSTM", "TRANSFORMER"]:
        model_results = test_results[model_type]
        
        # æ”¶é›†æ‰€æœ‰faiå€¼å’ŒçœŸå®æ ‡ç­¾
        all_fai = []
        all_labels = []
        
        for result in model_results:
            all_fai.extend(result['fai'])
            all_labels.extend([result['label']] * len(result['fai']))
        
        all_fai = np.array(all_fai)
        all_labels = np.array(all_labels)
        
        # ç”Ÿæˆè¿ç»­é˜ˆå€¼èŒƒå›´
        thresholds = np.linspace(np.min(all_fai), np.max(all_fai), 100)
        
        tpr_list = []
        fpr_list = []
        
        for threshold in thresholds:
            tp = fp = tn = fn = 0
            
            for i, (fai_val, true_label) in enumerate(zip(all_fai, all_labels)):
                if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                    if fai_val > threshold:
                        fp += 1
                    else:
                        tn += 1
                else:  # æ•…éšœæ ·æœ¬
                    # ç®€åŒ–ï¼šè¿™é‡Œç”¨faié˜ˆå€¼ä»£æ›¿ä¸‰çª—å£ç¡®è®¤
                    if fai_val > threshold:
                        tp += 1
                    else:
                        fn += 1
            
            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            tpr_list.append(tpr)
            fpr_list.append(fpr)
        
        # è®¡ç®—AUC
        from sklearn.metrics import auc
        auc_score = auc(fpr_list, tpr_list)
        
        # ç»˜åˆ¶ROCæ›²çº¿
        ax1.plot(fpr_list, tpr_list, color=colors[model_type], linewidth=2,
                label=f'{model_type} (AUC={auc_score:.3f})')
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºåˆ†ç±»å™¨')
    ax1.set_xlabel('å‡æ­£ä¾‹ç‡')
    ax1.set_ylabel('çœŸæ­£ä¾‹ç‡')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: å›ºå®šé˜ˆå€¼å·¥ä½œç‚¹ ===
    ax2.set_title('(b) è®ºæ–‡å·¥ä½œç‚¹\n(ä¸‰çº§æŠ¥è­¦é˜ˆå€¼)')
    
    for model_type in ["BILSTM", "TRANSFORMER"]:
        metrics = performance_metrics[model_type]['classification_metrics']
        ax2.scatter(metrics['fpr'], metrics['tpr'], 
                   s=200, color=colors[model_type], 
                   label=f'{model_type}\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
                   marker='o', edgecolors='black', linewidth=2)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('å‡æ­£ä¾‹ç‡')
    ax2.set_ylabel('çœŸæ­£ä¾‹ç‡')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” ===
    ax3.set_title('(c) åˆ†ç±»æŒ‡æ ‡å¯¹æ¯”')
    
    metrics_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'ç‰¹å¼‚æ€§']
    metric_mapping = {'å‡†ç¡®ç‡': 'accuracy', 'ç²¾ç¡®ç‡': 'precision', 'å¬å›ç‡': 'recall', 'F1åˆ†æ•°': 'f1_score', 'ç‰¹å¼‚æ€§': 'specificity'}
    bilstm_values = [performance_metrics['BILSTM']['classification_metrics'][metric_mapping[m]] 
                    for m in metrics_names]
    transformer_values = [performance_metrics['TRANSFORMER']['classification_metrics'][metric_mapping[m]] 
                         for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    ax3.bar(x - width/2, bilstm_values, width, label='BiLSTM', color='green', alpha=0.7)
    ax3.bar(x + width/2, transformer_values, width, label='Transformer', color='blue', alpha=0.7)
    
    ax3.set_xlabel('æŒ‡æ ‡')
    ax3.set_ylabel('åˆ†æ•°')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics_names, rotation=45)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # === å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å¯¹æ¯” ===
    ax4.set_title('(d) æ ·æœ¬çº§æ€§èƒ½å¯¹æ¯”')
    
    sample_metrics = ['å¹³å‡Ï†(æ­£å¸¸)', 'å¹³å‡Ï†(æ•…éšœ)', 'å¼‚å¸¸ç‡(æ­£å¸¸)', 'å¼‚å¸¸ç‡(æ•…éšœ)']
    bilstm_sample_values = [
        performance_metrics['BILSTM']['sample_metrics']['avg_fai_normal'],
        performance_metrics['BILSTM']['sample_metrics']['avg_fai_fault'],
        performance_metrics['BILSTM']['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics['BILSTM']['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    transformer_sample_values = [
        performance_metrics['TRANSFORMER']['sample_metrics']['avg_fai_normal'],
        performance_metrics['TRANSFORMER']['sample_metrics']['avg_fai_fault'],
        performance_metrics['TRANSFORMER']['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics['TRANSFORMER']['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    
    x = np.arange(len(sample_metrics))
    ax4.bar(x - width/2, bilstm_sample_values, width, label='BiLSTM', color='green', alpha=0.7)
    ax4.bar(x + width/2, transformer_sample_values, width, label='Transformer', color='blue', alpha=0.7)
    
    ax4.set_xlabel('æ ·æœ¬æŒ‡æ ‡')
    ax4.set_ylabel('æ•°å€¼')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_metrics, rotation=45, ha='right')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… ROCå¯¹æ¯”å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºå›¾------------------------------
def create_fault_detection_timeline(test_results, save_path):
    """ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    print("   ğŸ“Š ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    fault_sample_id = TEST_SAMPLES['fault'][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
    
    fig, axes = plt.subplots(5, 1, figsize=(15, 12), sharex=True)
    
    colors = {'BILSTM': 'green', 'TRANSFORMER': 'blue'}
    
    for i, model_type in enumerate(["BILSTM", "TRANSFORMER"]):
        # æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„ç»“æœ
        sample_result = next(r for r in test_results[model_type] if r['sample_id'] == fault_sample_id)
        
        fai_values = sample_result['fai']
        fault_labels = sample_result['fault_labels']
        thresholds = sample_result['thresholds']
        time_axis = np.arange(len(fai_values))
        
        # å­å›¾1&2: ç»¼åˆè¯Šæ–­æŒ‡æ ‡æ—¶åº
        ax = axes[i]
        ax.plot(time_axis, fai_values, color=colors[model_type], linewidth=1, alpha=0.8,
               label=f'{model_type} FAI')
        ax.axhline(y=thresholds['threshold1'], color='orange', linestyle='--', alpha=0.7,
                  label='ä¸€çº§é˜ˆå€¼')
        ax.axhline(y=thresholds['threshold2'], color='red', linestyle='--', alpha=0.7,
                  label='äºŒçº§é˜ˆå€¼')
        ax.axhline(y=thresholds['threshold3'], color='darkred', linestyle='--', alpha=0.7,
                  label='ä¸‰çº§é˜ˆå€¼')
        
        ax.set_ylabel(f'{model_type}\nç»¼åˆè¯Šæ–­æŒ‡æ ‡Ï†')
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        ax.set_title(f'{model_type} - æ ·æœ¬ {fault_sample_id} (æ•…éšœæ ·æœ¬)')
    
    # å­å›¾3: æ•…éšœæ£€æµ‹ç»“æœå¯¹æ¯”
    ax3 = axes[2]
    
    for model_type in ["BILSTM", "TRANSFORMER"]:
        sample_result = next(r for r in test_results[model_type] if r['sample_id'] == fault_sample_id)
        fault_labels = sample_result['fault_labels']
        time_axis = np.arange(len(fault_labels))
        
        # å°†æ•…éšœæ ‡ç­¾è½¬æ¢ä¸ºå¯è§†åŒ–åŒºåŸŸ
        fault_regions = np.where(fault_labels == 1, 0.8 if model_type == 'BILSTM' else 0.4, 0)
        ax3.fill_between(time_axis, fault_regions, 
                        alpha=0.6, color=colors[model_type],
                        label=f'{model_type} Fault Detection')
    
    ax3.set_ylabel('æ•…éšœæ£€æµ‹ç»“æœ')
    ax3.set_ylim(0, 1)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_title('æ•…éšœæ£€æµ‹ç»“æœå¯¹æ¯”')
    
    # å­å›¾4: ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹ï¼ˆä»¥TRANSFORMERä¸ºä¾‹ï¼‰
    ax4 = axes[3]
    transformer_result = next(r for r in test_results['TRANSFORMER'] if r['sample_id'] == fault_sample_id)
    detection_info = transformer_result['detection_info']
    
    time_axis = np.arange(len(transformer_result['fai']))
    ax4.plot(time_axis, transformer_result['fai'], 'b-', alpha=0.5, label='Ï†æŒ‡æ ‡å€¼')
    
    # æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        ax4.scatter(detection_info['candidate_points'], 
                   [transformer_result['fai'][i] for i in detection_info['candidate_points']],
                   color='orange', s=30, label='å€™é€‰ç‚¹', alpha=0.8)
    
    # æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax4.scatter(verified_indices,
                   [transformer_result['fai'][i] for i in verified_indices],
                   color='red', s=50, label='éªŒè¯ç‚¹', marker='^')
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    for region in detection_info['marked_regions']:
        start, end = region['range']
        ax4.axvspan(start, end, alpha=0.2, color='red', label='æ ‡è®°æ•…éšœåŒºåŸŸ')
    
    ax4.set_ylabel('ä¸‰çª—å£\næ£€æµ‹è¿‡ç¨‹')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_title('ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹ (Transformer)')
    
    # å­å›¾5: æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
    ax5 = axes[4]
    
    models = ['BiLSTM', 'Transformer']
    fault_ratios = []
    anomaly_ratios = []
    
    for model_type in ["BILSTM", "TRANSFORMER"]:
        sample_result = next(r for r in test_results[model_type] if r['sample_id'] == fault_sample_id)
        fault_ratios.append(sample_result['detection_info']['window_stats']['fault_ratio'])
        anomaly_ratios.append(sample_result['performance_metrics']['anomaly_ratio'])
    
    x = np.arange(len(models))
    width = 0.35
    
    ax5.bar(x - width/2, fault_ratios, width, label='ä¸‰çª—å£æ•…éšœæ¯”ç‡', alpha=0.7)
    ax5.bar(x + width/2, anomaly_ratios, width, label='é˜ˆå€¼å¼‚å¸¸æ¯”ç‡', alpha=0.7)
    
    ax5.set_xlabel('æ¨¡å‹ç±»å‹')
    ax5.set_ylabel('æ£€æµ‹æ¯”ç‡')
    ax5.set_xticks(x)
    ax5.set_xticklabels(models)
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    ax5.set_title('æ£€æµ‹æ€§èƒ½ç»Ÿè®¡')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… æ—¶åºå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾------------------------------
def create_performance_radar(performance_metrics, save_path):
    """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    print("   ğŸ•¸ï¸ ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'å‡†ç¡®ç‡': 'accuracy',
        'ç²¾ç¡®ç‡': 'precision', 
        'å¬å›ç‡': 'recall',
        'F1åˆ†æ•°': 'f1_score',
        'ç‰¹å¼‚æ€§': 'specificity',
        'æ—©æœŸé¢„è­¦': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'è¯¯æŠ¥æ§åˆ¶': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'æ£€æµ‹ç¨³å®šæ€§': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    # æ•°æ®é¢„å¤„ç†ï¼šFPRéœ€è¦è½¬æ¢ä¸ºæ§åˆ¶èƒ½åŠ› (1-FPR)
    bilstm_values = []
    transformer_values = []
    
    for metric_name, metric_key in radar_metrics.items():
        bilstm_val = performance_metrics['BILSTM']['classification_metrics'][metric_key]
        transformer_val = performance_metrics['TRANSFORMER']['classification_metrics'][metric_key]
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
        if metric_name == 'è¯¯æŠ¥æ§åˆ¶':
            bilstm_val = 1 - bilstm_val
            transformer_val = 1 - transformer_val
            
        bilstm_values.append(bilstm_val)
        transformer_values.append(transformer_val)
    
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    bilstm_values += bilstm_values[:1]  # é—­åˆ
    transformer_values += transformer_values[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize_medium"], subplot_kw=dict(projection='polar'))
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles, bilstm_values, 'o-', linewidth=2, label='BiLSTM', color='green')
    ax.fill(angles, bilstm_values, alpha=0.25, color='green')
    
    ax.plot(angles, transformer_values, 'o-', linewidth=2, label='Transformer', color='blue')
    ax.fill(angles, transformer_values, alpha=0.25, color='blue')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(list(radar_metrics.keys()))
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾\nBiLSTM vs Transformer å¯¹æ¯”', 
              pad=20, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # æ·»åŠ æ€§èƒ½æ€»ç»“
    bilstm_avg = np.mean(bilstm_values[:-1])
    transformer_avg = np.mean(transformer_values[:-1])
    
    plt.figtext(0.02, 0.02, f'ç»¼åˆæ€§èƒ½:\nBiLSTM: {bilstm_avg:.3f}\nTransformer: {transformer_avg:.3f}', 
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… é›·è¾¾å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–------------------------------
def create_three_window_visualization(test_results, save_path):
    """ç”Ÿæˆä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–"""
    print("   ğŸ” ç”Ÿæˆä¸‰çª—å£è¿‡ç¨‹å¯è§†åŒ–...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    fault_sample_id = TEST_SAMPLES['fault'][0]
    
    fig = plt.figure(figsize=(16, 10))
    
    # ä½¿ç”¨GridSpecè¿›è¡Œå¤æ‚å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1])
    
    # === ä¸»å›¾ï¼šä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹æ—¶åºå›¾ ===
    ax_main = fig.add_subplot(gs[0, :])
    
    # é€‰æ‹©Transformerç»“æœè¿›è¡Œå¯è§†åŒ–
    transformer_result = next(r for r in test_results['TRANSFORMER'] if r['sample_id'] == fault_sample_id)
    fai_values = transformer_result['fai']
    detection_info = transformer_result['detection_info']
    threshold1 = transformer_result['thresholds']['threshold1']
    
    time_axis = np.arange(len(fai_values))
    
    # ç»˜åˆ¶FAIæ—¶åº
    ax_main.plot(time_axis, fai_values, 'b-', linewidth=1.5, alpha=0.8, label='ç»¼åˆè¯Šæ–­æŒ‡æ ‡ Ï†(FAI)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.7, label='ä¸€çº§é˜ˆå€¼')
    
    # é˜¶æ®µ1ï¼šæ£€æµ‹çª—å£ - æ ‡è®°å€™é€‰ç‚¹
    if detection_info['candidate_points']:
        candidate_points = detection_info['candidate_points']
        ax_main.scatter(candidate_points, [fai_values[i] for i in candidate_points],
                       color='orange', s=40, alpha=0.8, label=f'æ£€æµ‹: {len(candidate_points)} ä¸ªå€™é€‰ç‚¹',
                       marker='o', zorder=5)
    
    # é˜¶æ®µ2ï¼šéªŒè¯çª—å£ - æ ‡è®°éªŒè¯é€šè¿‡çš„ç‚¹
    if detection_info['verified_points']:
        verified_indices = [v['point'] for v in detection_info['verified_points']]
        ax_main.scatter(verified_indices, [fai_values[i] for i in verified_indices],
                       color='red', s=60, alpha=0.9, label=f'éªŒè¯: {len(verified_indices)} ä¸ªç¡®è®¤ç‚¹',
                       marker='^', zorder=6)
        
        # æ˜¾ç¤ºéªŒè¯çª—å£èŒƒå›´
        for v_point in detection_info['verified_points']:
            verify_start, verify_end = v_point['verify_range']
            ax_main.axvspan(verify_start, verify_end, alpha=0.1, color='yellow')
    
    # é˜¶æ®µ3ï¼šæ ‡è®°çª—å£ - æ•…éšœåŒºåŸŸ
    fault_regions_plotted = set()  # é¿å…é‡å¤ç»˜åˆ¶å›¾ä¾‹
    for i, region in enumerate(detection_info['marked_regions']):
        start, end = region['range']
        label = 'æ ‡è®°: æ•…éšœåŒºåŸŸ' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('æ—¶é—´æ­¥é•¿')
    ax_main.set_ylabel('ç»¼åˆè¯Šæ–­æŒ‡æ ‡ Ï†')
    ax_main.set_title(f'ä¸‰çª—å£æ•…éšœæ£€æµ‹è¿‡ç¨‹ - æ ·æœ¬ {fault_sample_id} (Transformer)', 
                     fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left')
    ax_main.grid(True, alpha=0.3)
    
    # === å­å›¾1ï¼šæ£€æµ‹çª—å£ç»Ÿè®¡ ===
    ax1 = fig.add_subplot(gs[1, 0])
    
    window_stats = detection_info['window_stats']
    detection_data = [
        window_stats['total_candidates'],
        window_stats['verified_candidates'], 
        window_stats['total_fault_points']
    ]
    detection_labels = ['å€™é€‰ç‚¹', 'éªŒè¯ç‚¹', 'æ•…éšœç‚¹']
    colors1 = ['orange', 'red', 'darkred']
    
    bars1 = ax1.bar(detection_labels, detection_data, color=colors1, alpha=0.7)
    ax1.set_title('æ£€æµ‹ç»Ÿè®¡')
    ax1.set_ylabel('æ•°é‡')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, detection_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(value), ha='center', va='bottom')
    
    # === å­å›¾2ï¼šçª—å£å‚æ•°é…ç½® ===
    ax2 = fig.add_subplot(gs[1, 1])
    
    window_params = [
        WINDOW_CONFIG['detection_window'],
        WINDOW_CONFIG['verification_window'],
        WINDOW_CONFIG['marking_window']
    ]
    window_labels = ['æ£€æµ‹çª—å£\n(100)', 'éªŒè¯çª—å£\n(50)', 'æ ‡è®°çª—å£\n(50)']
    colors2 = ['lightblue', 'lightgreen', 'lightcoral']
    
    wedges, texts, autotexts = ax2.pie(window_params, labels=window_labels, colors=colors2,
                                      autopct='%1.0f', startangle=90)
    ax2.set_title('çª—å£å¤§å°\n(é‡‡æ ·ç‚¹æ•°)')
    
    # === å­å›¾3ï¼šéªŒè¯çª—å£è¯¦æƒ… ===
    ax3 = fig.add_subplot(gs[1, 2])
    
    if detection_info['verified_points']:
        continuous_ratios = [v['continuous_ratio'] for v in detection_info['verified_points']]
        verify_points = [v['point'] for v in detection_info['verified_points']]
        
        bars3 = ax3.bar(range(len(continuous_ratios)), continuous_ratios, 
                       color='green', alpha=0.7)
        ax3.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='é˜ˆå€¼ (30%)')
        ax3.set_title('éªŒè¯æ¯”ç‡')
        ax3.set_xlabel('éªŒè¯ç‚¹')
        ax3.set_ylabel('è¿ç»­æ¯”ç‡')
        ax3.set_xticks(range(len(continuous_ratios)))
        ax3.set_xticklabels([f'P{i+1}' for i in range(len(continuous_ratios))])
        ax3.legend()
    else:
        ax3.text(0.5, 0.5, 'æ— éªŒè¯ç‚¹', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('éªŒè¯æ¯”ç‡')
    
    # === å­å›¾4ï¼šæ¨¡å‹å¯¹æ¯” ===
    ax4 = fig.add_subplot(gs[1, 3])
    
    model_comparison = []
    model_labels = []
    
    for model_type in ['BILSTM', 'TRANSFORMER']:
        sample_result = next(r for r in test_results[model_type] if r['sample_id'] == fault_sample_id)
        model_comparison.append(sample_result['detection_info']['window_stats']['fault_ratio'])
        model_labels.append(model_type)
    
    colors4 = ['green', 'blue']
    bars4 = ax4.bar(model_labels, model_comparison, color=colors4, alpha=0.7)
    ax4.set_title('æ¨¡å‹å¯¹æ¯”\n(æ•…éšœæ£€æµ‹æ¯”ç‡)')
    ax4.set_ylabel('æ•…éšœæ¯”ç‡')
    
    for bar, value in zip(bars4, model_comparison):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === åº•éƒ¨ï¼šè¿‡ç¨‹è¯´æ˜ ===
    process_text = """
    ä¸‰çª—å£æ£€æµ‹è¿‡ç¨‹:
    
    1. æ£€æµ‹çª—å£ (100ç‚¹): æ‰«æå€™é€‰æ•…éšœç‚¹ï¼Œæ¡ä»¶ï¼šÏ†(FAI) > é˜ˆå€¼
    2. éªŒè¯çª—å£ (50ç‚¹): éªŒè¯å€™é€‰ç‚¹ï¼Œæ£€æŸ¥è¿ç»­æ€§ (â‰¥30% è¶…é˜ˆå€¼)
    3. æ ‡è®°çª—å£ (Â±50ç‚¹): æ ‡è®°ç¡®è®¤çš„æ•…éšœåŒºåŸŸ
    
    ä¼˜åŠ¿: åœ¨ä¿æŒé«˜æ•æ„Ÿæ€§çš„åŒæ—¶å‡å°‘è¯¯æŠ¥
    """
    
    fig.text(0.02, 0.02, process_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"])
    plt.close()
    
    print(f"   âœ… ä¸‰çª—å£è¿‡ç¨‹å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»“æœä¿å­˜å‡½æ•°------------------------------
def save_test_results(test_results, performance_metrics):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = f"test_results_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(f"{result_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{result_dir}/detailed_results", exist_ok=True)
    
    # 1. ä¿å­˜æ€§èƒ½æŒ‡æ ‡JSON
    performance_file = f"{result_dir}/performance_metrics.json"
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
    print(f"   âœ… æ€§èƒ½æŒ‡æ ‡ä¿å­˜è‡³: {performance_file}")
    
    # 2. ä¿å­˜è¯¦ç»†ç»“æœ
    for model_type in ["BILSTM", "TRANSFORMER"]:
        detail_file = f"{result_dir}/detailed_results/{model_type.lower()}_detailed_results.pkl"
        with open(detail_file, 'wb') as f:
            pickle.dump(test_results[model_type], f)
        print(f"   âœ… {model_type}è¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
    
    # 3. ä¿å­˜å…ƒæ•°æ®
    metadata_file = f"{result_dir}/detailed_results/test_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(test_results['metadata'], f, indent=2, ensure_ascii=False)
    print(f"   âœ… æµ‹è¯•å…ƒæ•°æ®ä¿å­˜è‡³: {metadata_file}")
    
    # 4. åˆ›å»ºExcelæ€»ç»“æŠ¥å‘Š
    summary_file = f"{result_dir}/detailed_results/comparison_summary.xlsx"
    
    with pd.ExcelWriter(summary_file) as writer:
        # æ€§èƒ½å¯¹æ¯”è¡¨
        comparison_data = []
        for model_type in ["BILSTM", "TRANSFORMER"]:
            metrics = performance_metrics[model_type]['classification_metrics']
            confusion = performance_metrics[model_type]['confusion_matrix']
            sample_metrics = performance_metrics[model_type]['sample_metrics']
            
            comparison_data.append({
                'Model': model_type,
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1_score'],
                'Specificity': metrics['specificity'],
                'TPR': metrics['tpr'],
                'FPR': metrics['fpr'],
                'TP': confusion['TP'],
                'FP': confusion['FP'],
                'TN': confusion['TN'],
                'FN': confusion['FN'],
                'Avg_FAI_Normal': sample_metrics['avg_fai_normal'],
                'Avg_FAI_Fault': sample_metrics['avg_fai_fault']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_excel(writer, sheet_name='Performance_Comparison', index=False)
        
        # æ ·æœ¬è¯¦æƒ…è¡¨
        sample_details = []
        for model_type in ["BILSTM", "TRANSFORMER"]:
            for result in test_results[model_type]:
                sample_details.append({
                    'Model': model_type,
                    'Sample_ID': result['sample_id'],
                    'True_Label': 'Fault' if result['label'] == 1 else 'Normal',
                    'FAI_Mean': result['performance_metrics']['fai_mean'],
                    'FAI_Std': result['performance_metrics']['fai_std'],
                    'FAI_Max': result['performance_metrics']['fai_max'],
                    'Anomaly_Ratio': result['performance_metrics']['anomaly_ratio'],
                    'Fault_Detection_Ratio': result['detection_info']['window_stats']['fault_ratio'],
                    'Candidates_Found': result['detection_info']['window_stats']['total_candidates'],
                    'Verified_Points': result['detection_info']['window_stats']['verified_candidates']
                })
        
        sample_df = pd.DataFrame(sample_details)
        sample_df.to_excel(writer, sheet_name='Sample_Details', index=False)
    
    print(f"   âœ… Excelæ€»ç»“æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")
    
    return result_dir

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
performance_metrics = calculate_performance_metrics(test_results)

# ä¿å­˜æµ‹è¯•ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
result_dir = save_test_results(test_results, performance_metrics)

#----------------------------------------æœ€ç»ˆæ€»ç»“------------------------------
print("\n" + "="*80)
print("ğŸ‰ åŒæ¨¡å‹æ¯”å¯¹æµ‹è¯•å®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(ALL_TEST_SAMPLES)} ä¸ª (æ­£å¸¸: {len(TEST_SAMPLES['normal'])}, æ•…éšœ: {len(TEST_SAMPLES['fault'])})")
print(f"   â€¢ æ¨¡å‹å¯¹æ¯”: BiLSTM vs Transformer")
print(f"   â€¢ ä¸‰çª—å£æ£€æµ‹: æ£€æµ‹({WINDOW_CONFIG['detection_window']}) â†’ éªŒè¯({WINDOW_CONFIG['verification_window']}) â†’ æ ‡è®°({WINDOW_CONFIG['marking_window']})")

print(f"\nğŸ”¬ æ€§èƒ½å¯¹æ¯”:")
for model_type in ["BILSTM", "TRANSFORMER"]:
    metrics = performance_metrics[model_type]['classification_metrics']
    print(f"   {model_type}:")
    print(f"     å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
    print(f"     ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
    print(f"     å¬å›ç‡: {metrics['recall']:.3f}")
    print(f"     F1åˆ†æ•°: {metrics['f1_score']:.3f}")
    print(f"     TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   â€¢ ç»“æœç›®å½•: {result_dir}")
print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {result_dir}/visualizations")
print(f"     - ROCæ›²çº¿å¯¹æ¯”: roc_comparison.png")
print(f"     - æ•…éšœæ£€æµ‹æ—¶åºå›¾: fault_detection_timeline.png") 
print(f"     - æ€§èƒ½é›·è¾¾å›¾: performance_radar.png")
print(f"     - ä¸‰çª—å£è¿‡ç¨‹å›¾: three_window_process.png")
print(f"   â€¢ æ€§èƒ½æŒ‡æ ‡: performance_metrics.json")
print(f"   â€¢ ExcelæŠ¥å‘Š: comparison_summary.xlsx")

# æœ€ç»ˆæ¨è
bilstm_score = np.mean(list(performance_metrics['BILSTM']['classification_metrics'].values()))
transformer_score = np.mean(list(performance_metrics['TRANSFORMER']['classification_metrics'].values()))

print(f"\nğŸ† ç»¼åˆæ€§èƒ½è¯„ä¼°:")
print(f"   BiLSTM ç»¼åˆå¾—åˆ†: {bilstm_score:.3f}")
print(f"   Transformer ç»¼åˆå¾—åˆ†: {transformer_score:.3f}")

if transformer_score > bilstm_score:
    print(f"   âœ… æ¨è: Transformer æ¨¡å‹æ€§èƒ½æ›´ä¼˜ (+{transformer_score - bilstm_score:.3f})")
else:
    print(f"   âœ… æ¨è: BiLSTM æ¨¡å‹æ€§èƒ½æ›´ä¼˜ (+{bilstm_score - transformer_score:.3f})")

print("\n" + "="*80)
print("æµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
print("="*80)