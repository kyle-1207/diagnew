#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸‰æ¨¡å‹å¯¹æ¯”åˆ†æè„šæœ¬
ç›´æ¥è¯»å–å·²ä¿å­˜çš„æµ‹è¯•ç»“æœè¿›è¡Œå¯¹æ¯”å¯è§†åŒ–
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
import matplotlib.font_manager as fm

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_fonts():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    # å°è¯•ç³»ç»Ÿå­—ä½“
    font_candidates = [
        'SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'Arial Unicode MS',
        'WenQuanYi Micro Hei', 'Source Han Sans CN'
    ]
    
    chosen = None
    for font in font_candidates:
        try:
            if any(font.lower() in f.name.lower() for f in fm.fontManager.ttflist):
                chosen = font
                break
        except:
            continue
    
    if chosen:
        rcParams['font.sans-serif'] = [chosen]
        rcParams['axes.unicode_minus'] = False
        print(f"âœ… ä½¿ç”¨å­—ä½“: {chosen}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        rcParams['font.sans-serif'] = ['DejaVu Sans']

# æ‰§è¡Œå­—ä½“é…ç½®
setup_chinese_fonts()

class ThreeModelComparator:
    def __init__(self, base_path=None):
        """
        åˆå§‹åŒ–ä¸‰æ¨¡å‹å¯¹æ¯”å™¨
        
        Args:
            base_path: ä¸‰ä¸ªæ¨¡å‹æ•°æ®çš„åŸºç¡€è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
        """
        if base_path is None:
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
            possible_paths = [
                "Three_model",  # å½“å‰ç›®å½•ä¸‹
                "/mnt/bz25t/bzhy/datasave/Three_model",  # LinuxæœåŠ¡å™¨è·¯å¾„
                "../Three_model",  # ä¸Šçº§ç›®å½•
                "../../Three_model"  # å†ä¸Šçº§ç›®å½•
            ]
            
            self.base_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    self.base_path = path
                    print(f"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {path}")
                    break
            
            if self.base_path is None:
                print("âŒ æœªæ‰¾åˆ°Three_modelæ•°æ®ç›®å½•ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šè·¯å¾„")
                print("å¯èƒ½çš„è·¯å¾„ä½ç½®:")
                for path in possible_paths:
                    print(f"   - {path}")
        else:
            self.base_path = base_path
        self.model_configs = {
            'BiLSTM': {
                'folder': 'BILSTM',
                'performance_file': 'bilstm_performance_metrics.json',
                'detailed_file': 'bilstm_detailed_results.pkl',
                'color': '#FF6B6B',  # çº¢è‰²
                'marker': 'o'
            },
            'Transformer-PN': {
                'folder': 'transformer_PN',
                'performance_file': 'transformer_performance_metrics.json',
                'detailed_file': 'transformer_detailed_results.pkl',
                'color': '#4ECDC4',  # é’è‰²
                'marker': 's'
            },
            'Transformer-Positive': {
                'folder': 'transformer_positive',
                'performance_file': 'transformer_performance_metrics.json',
                'detailed_file': 'transformer_detailed_results.pkl',
                'color': '#45B7D1',  # è“è‰²
                'marker': '^'
            }
        }
        
        self.model_data = {}
        self.comparison_results = {}
    
    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹çš„æ•°æ®"""
        print("="*60)
        print("ğŸ”„ å¼€å§‹åŠ è½½ä¸‰æ¨¡å‹æ•°æ®...")
        print("="*60)
        
        if self.base_path is None:
            print("âŒ æœªæŒ‡å®šæ•°æ®ç›®å½•ï¼Œæ— æ³•åŠ è½½æ•°æ®")
            print("\nğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
            print("1. æ‰‹åŠ¨åˆ›å»º Three_model ç›®å½•ç»“æ„")
            print("2. æˆ–è€…è¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ•°æ®")
            print("3. æˆ–è€…æŒ‡å®šå…·ä½“çš„æ•°æ®æ–‡ä»¶è·¯å¾„")
            return False
        
        for model_name, config in self.model_configs.items():
            print(f"\nğŸ“‚ åŠ è½½ {model_name} æ•°æ®...")
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            folder_path = os.path.join(self.base_path, config['folder'])
            performance_path = os.path.join(folder_path, config['performance_file'])
            detailed_path = os.path.join(folder_path, config['detailed_file'])
            
            print(f"   ğŸ” æŸ¥æ‰¾è·¯å¾„: {folder_path}")
            print(f"   ğŸ“Š æ€§èƒ½æ–‡ä»¶: {performance_path}")
            print(f"   ğŸ“‹ è¯¦ç»†æ–‡ä»¶: {detailed_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(performance_path):
                print(f"âŒ æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {performance_path}")
                continue
            if not os.path.exists(detailed_path):
                print(f"âŒ è¯¦ç»†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {detailed_path}")
                continue
            
            try:
                # åŠ è½½æ€§èƒ½æŒ‡æ ‡
                with open(performance_path, 'r', encoding='utf-8') as f:
                    performance_data = json.load(f)
                
                # åŠ è½½è¯¦ç»†ç»“æœ
                with open(detailed_path, 'rb') as f:
                    detailed_data = pickle.load(f)
                
                # ä¿å­˜æ•°æ®
                self.model_data[model_name] = {
                    'performance': performance_data,
                    'detailed': detailed_data,
                    'config': config
                }
                
                print(f"âœ… {model_name} æ•°æ®åŠ è½½æˆåŠŸ")
                print(f"   - æ€§èƒ½æŒ‡æ ‡: {len(performance_data)} é¡¹")
                print(f"   - è¯¦ç»†ç»“æœ: {len(detailed_data)} ä¸ªæ ·æœ¬")
                
            except Exception as e:
                print(f"âŒ {model_name} æ•°æ®åŠ è½½å¤±è´¥: {e}")
        
        print(f"\nâœ… å…±åŠ è½½äº† {len(self.model_data)} ä¸ªæ¨¡å‹çš„æ•°æ®")
        
        if len(self.model_data) == 0:
            print("\nğŸ’¡ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ•°æ®ï¼Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. è¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ•°æ®:")
            print("   - Linux/Train_BILSTM.py")
            print("   - Linux/Train_Transformer_HybridFeedback.py") 
            print("   - Linux/Train_Transformer_PN_HybridFeedback.py")
            print("2. æˆ–è€…æ‰‹åŠ¨åˆ›å»ºThree_modelç›®å½•ç»“æ„")
            print("3. æˆ–è€…ä½¿ç”¨create_sample_data()åˆ›å»ºç¤ºä¾‹æ•°æ®")
        
        return len(self.model_data) > 0
    
    def create_sample_data(self, save_path="Three_model"):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ç»“æ„ä¾›æµ‹è¯•ä½¿ç”¨"""
        print(f"\nğŸ”§ åˆ›å»ºç¤ºä¾‹æ•°æ®ç»“æ„: {save_path}")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        for model_name, config in self.model_configs.items():
            folder_path = os.path.join(save_path, config['folder'])
            os.makedirs(folder_path, exist_ok=True)
            
            # åˆ›å»ºç¤ºä¾‹æ€§èƒ½æŒ‡æ ‡
            sample_performance = {
                'accuracy': 0.85 + np.random.random() * 0.1,
                'precision': 0.80 + np.random.random() * 0.15,
                'recall': 0.75 + np.random.random() * 0.2,
                'f1_score': 0.82 + np.random.random() * 0.12,
                'auc': 0.88 + np.random.random() * 0.1,
                'specificity': 0.83 + np.random.random() * 0.12,
                'false_positive_rate': 0.05 + np.random.random() * 0.1,
                'early_warning_rate': 0.8 + np.random.random() * 0.15,
                'detection_stability': 0.85 + np.random.random() * 0.1
            }
            
            # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            performance_path = os.path.join(folder_path, config['performance_file'])
            with open(performance_path, 'w', encoding='utf-8') as f:
                json.dump(sample_performance, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºç¤ºä¾‹è¯¦ç»†ç»“æœ
            sample_detailed = []
            for i in range(100):  # 100ä¸ªæµ‹è¯•æ ·æœ¬
                sample_result = {
                    'sample_id': f'sample_{i}',
                    'true_labels': [0, 0, 0, 1, 1] if i % 2 == 0 else [1, 1, 0, 0, 1],
                    'probabilities': [0.1 + np.random.random() * 0.8 for _ in range(5)],
                    'predictions': [1 if p > 0.5 else 0 for p in [0.1 + np.random.random() * 0.8 for _ in range(5)]]
                }
                sample_detailed.append(sample_result)
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_path = os.path.join(folder_path, config['detailed_file'])
            with open(detailed_path, 'wb') as f:
                pickle.dump(sample_detailed, f)
            
            print(f"   âœ… {model_name} ç¤ºä¾‹æ•°æ®å·²åˆ›å»º")
        
        print(f"\nğŸ‰ ç¤ºä¾‹æ•°æ®ç»“æ„åˆ›å»ºå®Œæˆ: {save_path}")
        print("ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œå¯¹æ¯”åˆ†æäº†ï¼")
    
    def generate_roc_comparison(self, save_path="Three_model/comparison_roc_curves.png"):
        """ç”Ÿæˆä¸‰æ¨¡å‹ROCæ›²çº¿å¯¹æ¯”å›¾"""
        print("\nğŸ¯ ç”ŸæˆROCæ›²çº¿å¯¹æ¯”å›¾...")
        
        plt.figure(figsize=(10, 8))
        
        for model_name, data in self.model_data.items():
            try:
                # ä»è¯¦ç»†ç»“æœä¸­æå–ROCæ•°æ®
                detailed = data['detailed']
                config = data['config']
                
                # æå–æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾
                all_probs = []
                all_labels = []
                
                for sample_result in detailed:
                    if 'probabilities' in sample_result and 'true_labels' in sample_result:
                        all_probs.extend(sample_result['probabilities'])
                        all_labels.extend(sample_result['true_labels'])
                
                if len(all_probs) > 0:
                    # è®¡ç®—ROCæ›²çº¿
                    from sklearn.metrics import roc_curve, auc
                    fpr, tpr, _ = roc_curve(all_labels, all_probs)
                    roc_auc = auc(fpr, tpr)
                    
                    # ç»˜åˆ¶ROCæ›²çº¿
                    plt.plot(fpr, tpr, 
                            color=config['color'], 
                            marker=config['marker'],
                            markersize=4,
                            markevery=20,
                            linewidth=2.5,
                            label=f'{model_name} (AUC = {roc_auc:.3f})')
                else:
                    print(f"âš ï¸ {model_name} ç¼ºå°‘ROCæ•°æ®")
                    
            except Exception as e:
                print(f"âŒ {model_name} ROCæ•°æ®å¤„ç†å¤±è´¥: {e}")
        
        # ç»˜åˆ¶å¯¹è§’çº¿
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1)
        
        # è®¾ç½®å›¾è¡¨å±æ€§
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ä¸‰æ¨¡å‹ROCæ›²çº¿å¯¹æ¯”åˆ†æ', fontsize=14, fontweight='bold', pad=20)
        plt.legend(loc="lower right", fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ROCå¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    def generate_performance_radar(self, save_path="Three_model/comparison_performance_radar.png"):
        """ç”Ÿæˆä¸‰æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”"""
        print("\nğŸ¯ ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”...")
        
        # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
        radar_metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 
                        'Specificity', 'Early Warning', 'Detection Stability', 'False Alarm Control']
        
        # æ”¶é›†æ•°æ®
        radar_data = {}
        for model_name, data in self.model_data.items():
            performance = data['performance']
            
            # æå–æŒ‡æ ‡å€¼
            values = []
            try:
                values.append(performance.get('accuracy', 0))
                values.append(performance.get('precision', 0))
                values.append(performance.get('recall', 0))
                values.append(performance.get('f1_score', 0))
                values.append(performance.get('specificity', 0))
                values.append(performance.get('early_warning_rate', 0.8))  # é»˜è®¤å€¼
                values.append(performance.get('detection_stability', 0.85))  # é»˜è®¤å€¼
                values.append(1 - performance.get('false_positive_rate', 0.1))  # è½¬æ¢ä¸ºæ§åˆ¶ç‡
                
                radar_data[model_name] = values
            except Exception as e:
                print(f"âŒ {model_name} é›·è¾¾å›¾æ•°æ®æå–å¤±è´¥: {e}")
        
        if not radar_data:
            print("âŒ æ— å¯ç”¨çš„é›·è¾¾å›¾æ•°æ®")
            return
        
        # åˆ›å»ºé›·è¾¾å›¾
        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
        
        # è®¡ç®—è§’åº¦
        angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # é—­åˆ
        
        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹
        for model_name, values in radar_data.items():
            config = self.model_data[model_name]['config']
            
            # é—­åˆæ•°æ®
            values_closed = values + [values[0]]
            
            # ç»˜åˆ¶é›·è¾¾å›¾
            ax.plot(angles, values_closed, 
                   color=config['color'], 
                   linewidth=3, 
                   label=model_name,
                   marker=config['marker'],
                   markersize=8)
            
            # å¡«å……åŒºåŸŸ
            ax.fill(angles, values_closed, 
                   color=config['color'], 
                   alpha=0.15)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(radar_metrics, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # è®¾ç½®æ ‡é¢˜å’Œå›¾ä¾‹
        plt.title('ä¸‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾å¯¹æ¯”', fontsize=16, fontweight='bold', pad=30)
        plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0), fontsize=12)
        
        # ä¿å­˜å›¾ç‰‡
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ€§èƒ½é›·è¾¾å›¾å·²ä¿å­˜: {save_path}")
    
    def generate_metrics_comparison(self, save_path="Three_model/comparison_metrics_bar.png"):
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾å¯¹æ¯”"""
        print("\nğŸ¯ ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾å¯¹æ¯”...")
        
        # æ”¶é›†å…³é”®æŒ‡æ ‡
        metrics_data = []
        for model_name, data in self.model_data.items():
            performance = data['performance']
            
            metrics_data.append({
                'Model': model_name,
                'Accuracy': performance.get('accuracy', 0),
                'Precision': performance.get('precision', 0),
                'Recall': performance.get('recall', 0),
                'F1 Score': performance.get('f1_score', 0),
                'AUC': performance.get('auc', 0),
                'Specificity': performance.get('specificity', 0)
            })
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(metrics_data)
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'Specificity']
        colors = [self.model_data[model]['config']['color'] for model in df['Model']]
        
        for i, metric in enumerate(metrics):
            ax = axes[i]
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars = ax.bar(df['Model'], df[metric], color=colors, alpha=0.8, edgecolor='black', linewidth=1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_title(metric, fontsize=14, fontweight='bold')
            ax.set_ylim(0, 1.1)
            ax.set_ylabel('Score', fontsize=12)
            ax.grid(True, alpha=0.3, axis='y')
            
            # æ—‹è½¬xè½´æ ‡ç­¾
            ax.tick_params(axis='x', rotation=45)
        
        # æ€»æ ‡é¢˜
        fig.suptitle('ä¸‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”', fontsize=18, fontweight='bold', y=0.98)
        
        # ä¿å­˜å›¾ç‰‡
        plt.tight_layout()
        plt.subplots_adjust(top=0.93)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    def generate_summary_report(self, save_path="Three_model/comparison_summary.txt"):
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”æ€»ç»“æŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("="*80)
        report_lines.append("ğŸ”¬ ä¸‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†ææŠ¥å‘Š")
        report_lines.append("="*80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # æ¨¡å‹åŸºæœ¬ä¿¡æ¯
        report_lines.append("ğŸ“‹ æ¨¡å‹åŸºæœ¬ä¿¡æ¯:")
        for model_name, data in self.model_data.items():
            performance = data['performance']
            detailed = data['detailed']
            
            report_lines.append(f"\nğŸ”¸ {model_name}:")
            report_lines.append(f"   - æµ‹è¯•æ ·æœ¬æ•°: {len(detailed)}")
            report_lines.append(f"   - æ•´ä½“å‡†ç¡®ç‡: {performance.get('accuracy', 0):.4f}")
            report_lines.append(f"   - AUCå€¼: {performance.get('auc', 0):.4f}")
            report_lines.append(f"   - F1åˆ†æ•°: {performance.get('f1_score', 0):.4f}")
        
        # æ€§èƒ½æ’å
        report_lines.append("\n" + "="*50)
        report_lines.append("ğŸ† æ€§èƒ½æŒ‡æ ‡æ’å:")
        
        metrics_for_ranking = ['accuracy', 'precision', 'recall', 'f1_score', 'auc', 'specificity']
        
        for metric in metrics_for_ranking:
            metric_values = []
            for model_name, data in self.model_data.items():
                value = data['performance'].get(metric, 0)
                metric_values.append((model_name, value))
            
            # æ’åº
            metric_values.sort(key=lambda x: x[1], reverse=True)
            
            report_lines.append(f"\nğŸ“Š {metric.title()}æ’å:")
            for rank, (model, value) in enumerate(metric_values, 1):
                medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
                report_lines.append(f"   {medal} {rank}. {model}: {value:.4f}")
        
        # ç»¼åˆè¯„ä¼°
        report_lines.append("\n" + "="*50)
        report_lines.append("ğŸ“ˆ ç»¼åˆè¯„ä¼°å»ºè®®:")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_scores = {}
        for model_name, data in self.model_data.items():
            performance = data['performance']
            score = (
                performance.get('accuracy', 0) * 0.2 +
                performance.get('precision', 0) * 0.2 +
                performance.get('recall', 0) * 0.2 +
                performance.get('f1_score', 0) * 0.2 +
                performance.get('auc', 0) * 0.2
            )
            overall_scores[model_name] = score
        
        # æ’åºå¹¶ç»™å‡ºå»ºè®®
        ranked_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
        
        report_lines.append(f"\nğŸ¯ ç»¼åˆæ€§èƒ½æ’å:")
        for rank, (model, score) in enumerate(ranked_models, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
            report_lines.append(f"   {medal} {rank}. {model}: {score:.4f}")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
        for line in report_lines:
            print(line)
        
        print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
    
    def run_full_comparison(self):
        """è¿è¡Œå®Œæ•´çš„ä¸‰æ¨¡å‹å¯¹æ¯”åˆ†æ"""
        print("ğŸš€ å¼€å§‹ä¸‰æ¨¡å‹å®Œæ•´å¯¹æ¯”åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_all_data():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ")
            return False
        
        # 2. ç”ŸæˆROCå¯¹æ¯”å›¾
        self.generate_roc_comparison()
        
        # 3. ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
        self.generate_performance_radar()
        
        # 4. ç”ŸæˆæŒ‡æ ‡æŸ±çŠ¶å›¾
        self.generate_metrics_comparison()
        
        # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        print("\n" + "="*60)
        print("ğŸ‰ ä¸‰æ¨¡å‹å¯¹æ¯”åˆ†æå®Œæˆï¼")
        print("="*60)
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("ğŸ“Š Three_model/comparison_roc_curves.png - ROCæ›²çº¿å¯¹æ¯”")
        print("ğŸ¯ Three_model/comparison_performance_radar.png - æ€§èƒ½é›·è¾¾å›¾")
        print("ğŸ“ˆ Three_model/comparison_metrics_bar.png - æŒ‡æ ‡æŸ±çŠ¶å›¾")
        print("ğŸ“‹ Three_model/comparison_summary.txt - å¯¹æ¯”æŠ¥å‘Š")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ ä¸‰æ¨¡å‹å¯¹æ¯”åˆ†æç³»ç»Ÿ")
    print("ç›´æ¥è¯»å–å·²ä¿å­˜çš„æµ‹è¯•ç»“æœè¿›è¡Œå¯¹æ¯”")
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = ThreeModelComparator()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œæä¾›åˆ›å»ºç¤ºä¾‹æ•°æ®çš„é€‰é¡¹
    if comparator.base_path is None:
        print("\nâ“ æ˜¯å¦åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Ÿ")
        print("è¿™å°†åˆ›å»ºThree_modelç›®å½•ç»“æ„å’Œç¤ºä¾‹æ•°æ®æ–‡ä»¶")
        
        # è‡ªåŠ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        print("\nğŸ”§ è‡ªåŠ¨åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        comparator.create_sample_data()
        
        # é‡æ–°åˆå§‹åŒ–å¯¹æ¯”å™¨
        comparator = ThreeModelComparator()
    
    # è¿è¡Œå®Œæ•´å¯¹æ¯”
    success = comparator.run_full_comparison()
    
    if success:
        print("\nâœ… å¯¹æ¯”åˆ†ææˆåŠŸå®Œæˆï¼")
    else:
        print("\nâŒ å¯¹æ¯”åˆ†æå¤±è´¥ï¼")
        print("ğŸ’¡ å¦‚æœéœ€è¦é‡æ–°åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼Œè¯·åˆ é™¤Three_modelæ–‡ä»¶å¤¹åé‡æ–°è¿è¡Œ")

if __name__ == "__main__":
    main()
