# å¯¼å…¥å¿…è¦çš„åº“
import os
import warnings
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import pickle
import time
from datetime import datetime

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_loader_transformer import TransformerBatteryDataset
from distributed_utils import (
    setup_distributed, cleanup_distributed, to_distributed,
    create_distributed_loader, is_main_process, barrier, reduce_value
)

# è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
is_distributed, local_rank, world_size = setup_distributed()
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# æ‰“å°GPUä¿¡æ¯
if is_main_process():
    print(f"\nğŸ–¥ï¸ åˆ†å¸ƒå¼è®­ç»ƒé…ç½®:")
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"   å½“å‰è¿›ç¨‹: {local_rank}")
    print(f"   æ€»è¿›ç¨‹æ•°: {world_size}")
    if torch.cuda.is_available():
        print(f"   GPUå‹å·: {torch.cuda.get_device_name(local_rank)}")
        print(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(local_rank).total_memory/1024**3:.1f}GB")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒé…ç½®
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK SC']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

class TransformerPredictor(nn.Module):
    """æ—¶åºé¢„æµ‹Transformeræ¨¡å‹ - ç›´æ¥é¢„æµ‹çœŸå®ç‰©ç†å€¼"""
    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2):
        super(TransformerPredictor, self).__init__()
        self.input_size = input_size
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model//2, output_size)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, x):
        # x: [batch, input_size]
        x = x.unsqueeze(1)  # [batch, 1, input_size]
        
        # æŠ•å½±åˆ°d_modelç»´åº¦
        x = self.input_projection(x)  # [batch, 1, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:x.size(1)].unsqueeze(0)
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # [batch, 1, d_model]
        
        # è¾“å‡ºå±‚
        x = x.squeeze(1)  # [batch, d_model]
        out = self.output_layer(x)  # [batch, output_size]
        
        return out

def main():
    """ä¸»å‡½æ•° - åˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬"""
    try:
        # ç¡®ä¿åˆ†å¸ƒå¼ç¯å¢ƒæ­£ç¡®åˆå§‹åŒ–
        if not is_distributed and torch.cuda.device_count() > 1:
            print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å¤šä¸ªGPUä½†æœªå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·ä½¿ç”¨torchrunå¯åŠ¨è„šæœ¬")
            return
            
        # åŠ è½½è®­ç»ƒæ ·æœ¬ï¼ˆ0-200å·ï¼‰
        train_samples = list(range(201))
        if is_main_process():
            print(f"ğŸ“‹ è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-200")
            print(f"   æ ·æœ¬æ•°é‡: {len(train_samples)}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = TransformerBatteryDataset(
            data_path='/mnt/bz25t/bzhy/zhanglikang/project/QAS',
            sample_ids=train_samples
        )
        
        if len(dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½ï¼")
            return
            
        if is_main_process():
            print(f"\nğŸ“¥ åŠ è½½é¢„è®¡ç®—æ•°æ®...")
            print(f"ğŸ“Š åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªè®­ç»ƒæ•°æ®å¯¹")
        
        # åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
        batch_size = 1000 * world_size  # æ€»æ‰¹æ¬¡å¤§å°
        train_loader, train_sampler = create_distributed_loader(
            dataset, 
            batch_size=batch_size // world_size,  # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°
            num_workers=4
        )
        
        if is_main_process():
            print(f"ğŸ“¦ åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
            print(f"   æ€»æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   æ¯GPUæ‰¹æ¬¡å¤§å°: {batch_size // world_size}")
        
        # åˆå§‹åŒ–æ¨¡å‹å¹¶è½¬æ¢ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
        model = TransformerPredictor()
        model = to_distributed(model, local_rank)
        
        if is_main_process():
            print(f"ğŸ§  åˆ†å¸ƒå¼Transformeræ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # è®­ç»ƒå‚æ•°
        num_epochs = 30
        learning_rate = 0.001
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)
        criterion = nn.MSELoss()
        
        if is_main_process():
            print("\nâš™ï¸  è®­ç»ƒå‚æ•°:")
            print(f"   å­¦ä¹ ç‡: {learning_rate}")
            print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            print("\n" + "="*60)
            print("ğŸ¯ å¼€å§‹Transformerè®­ç»ƒ")
            print("="*60)
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        best_loss = float('inf')
        training_start_time = time.time()
        
        for epoch in range(num_epochs):
            model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            # è®¾ç½®é‡‡æ ·å™¨çš„epoch
            if train_sampler:
                train_sampler.set_epoch(epoch)
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                pbar = tqdm(train_loader, desc=f"Epoch {epoch:3d}")
            else:
                pbar = train_loader
            
            for batch in pbar:
                inputs, targets = batch
                inputs = inputs.to(local_rank if is_distributed else 'cuda')
                targets = targets.to(local_rank if is_distributed else 'cuda')
                
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
                
                # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±
                reduced_loss = reduce_value(loss.detach(), average=True)
                epoch_loss += reduced_loss.item()
                batch_count += 1
                
                if is_main_process():
                    pbar.set_postfix({'loss': f"{reduced_loss.item():.6f}"})
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = epoch_loss / batch_count
            train_losses.append(avg_loss)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = scheduler.get_last_lr()[0]
            scheduler.step()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and avg_loss < best_loss:
                best_loss = avg_loss
                torch.save(model.module.state_dict() if is_distributed else model.state_dict(),
                         '/mnt/bz25t/bzhy/zhanglikang/project/models/transformer_model_best.pth')
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and epoch % 5 == 0:
                print(f"Epoch: {epoch:3d} | Loss: {avg_loss:.6f} | LR: {current_lr:.6f}")
        
        # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒå†å²
        if is_main_process():
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            torch.save(model.module.state_dict() if is_distributed else model.state_dict(),
                     '/mnt/bz25t/bzhy/zhanglikang/project/models/transformer_model.pth')
            
            # ä¿å­˜è®­ç»ƒå†å²
            history = {
                'losses': train_losses,
                'best_loss': best_loss,
                'training_time': time.time() - training_start_time,
                'final_lr': scheduler.get_last_lr()[0],
                'batch_size': batch_size,
                'world_size': world_size
            }
            
            with open('/mnt/bz25t/bzhy/zhanglikang/project/models/transformer_training_history.pkl', 'wb') as f:
                pickle.dump(history, f)
            
            print("\nâœ… Transformerè®­ç»ƒå®Œæˆ!")
            print(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
            print(f"ğŸ“ˆ æŸå¤±æ”¹å–„: {((train_losses[0] - train_losses[-1])/train_losses[0]*100):.2f}% (ä» {train_losses[0]:.6f} åˆ° {train_losses[-1]:.6f})")
            print(f"â±ï¸  è®­ç»ƒç”¨æ—¶: {time.time() - training_start_time:.2f}ç§’")
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            plt.figure(figsize=(10, 6))
            plt.plot(train_losses, label='Training Loss')
            plt.title('Transformer Training Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.grid(True)
            plt.legend()
            plt.savefig('/mnt/bz25t/bzhy/zhanglikang/project/models/transformer_training_results.png')
            plt.close()
            
            print("\nğŸ’¾ ä¿å­˜ç»“æœ:")
            print("   âœ“ æ¨¡å‹æƒé‡: transformer_model.pth")
            print("   âœ“ æœ€ä½³æ¨¡å‹: transformer_model_best.pth")
            print("   âœ“ è®­ç»ƒå†å²: transformer_training_history.pkl")
            print("   âœ“ è®­ç»ƒæ›²çº¿: transformer_training_results.png")
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
        raise e
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()

if __name__ == "__main__":
    main()