# BiLSTMæ¨¡å‹ä¸“é—¨è®­ç»ƒè„šæœ¬
# åŠŸèƒ½ï¼šåªè®­ç»ƒBiLSTMæ¨¡å‹å¹¶ä¿å­˜ï¼Œä¾›åç»­MC-AEä½¿ç”¨

import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
import psutil  # ç³»ç»Ÿå†…å­˜ç›‘æ§

# GPUè®¾å¤‡é…ç½® - A100ç¯å¢ƒ
import os
# ä½¿ç”¨æŒ‡å®šçš„GPUè®¾å¤‡ï¼ˆA100ç¯å¢ƒï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # åªä½¿ç”¨GPU2

# CUDAè°ƒè¯•åŠŸèƒ½ï¼ˆåœ¨A100ç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´åˆå§‹åŒ–é—®é¢˜ï¼Œæš‚æ—¶ç¦ç”¨ï¼‰
print("ğŸ”§ CUDAè°ƒè¯•æ¨¡å¼å·²ç¦ç”¨ï¼ˆé¿å…A100åˆå§‹åŒ–å†²çªï¼‰")

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print("\nğŸ–¥ï¸ A100 GPUé…ç½®ä¿¡æ¯:")
    print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\n   GPU {i} ({props.name}):")
        print(f"      æ€»æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
    print(f"\n   å½“å‰ä½¿ç”¨: ä»…GPU2 (A100 80GBä¼˜åŒ–)")
    print(f"   ä¸»GPUè®¾å¤‡: cuda:0 (ç‰©ç†GPU2)")
    print(f"   å¤‡æ³¨: å•å¡A100ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨80GBæ˜¾å­˜")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒmatplotlibé…ç½®
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# Linuxç¯å¢ƒå­—ä½“è®¾ç½®
import matplotlib.font_manager as fm

font_options = [
    'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC',
    'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS'
]

available_fonts = []
for font in font_options:
    try:
        fm.findfont(font)
        available_fonts.append(font)
    except:
        continue

if available_fonts:
    plt.rcParams['font.sans-serif'] = available_fonts
    print(f"âœ… Linuxå­—ä½“é…ç½®å®Œæˆï¼Œä½¿ç”¨å­—ä½“: {available_fonts[0]}")
else:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

print("="*60)
print("BiLSTMä¸“é—¨è®­ç»ƒæ¨¡å¼")
print("åªè®­ç»ƒBiLSTMæ¨¡å‹å¹¶ä¿å­˜ï¼Œä¾›åç»­MC-AEä½¿ç”¨")
print("å¯ç”¨A100 80GBä¼˜åŒ–å’Œæ··åˆç²¾åº¦è®­ç»ƒ")
print("="*60)

#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_train_samples():
    """ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬ID"""
    try:
        import pandas as pd
        # Linuxè·¯å¾„æ ¼å¼
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–0-100èŒƒå›´çš„æ ·æœ¬
        all_samples = df['Num'].tolist()
        train_samples = [i for i in all_samples if 0 <= i <= 100]
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬:")
        print(f"   è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-100")
        print(f"   å®é™…å¯ç”¨æ ·æœ¬: {len(train_samples)} ä¸ª")
        print(f"   æ ·æœ¬ID: {train_samples[:10]}..." if len(train_samples) > 10 else f"   æ ·æœ¬ID: {train_samples}")
        
        return train_samples
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æ ·æœ¬èŒƒå›´ 0-20")
        return list(range(21))

# ç»Ÿä¸€çš„è·¯å¾„é…ç½® - Linuxç¯å¢ƒ
data_dir = '/mnt/bz25t/bzhy/zhanglikang/project/QAS'  # æ•°æ®ç›®å½•
save_dir = '/mnt/bz25t/bzhy/datasave/BILSTM_train'  # æ¨¡å‹ä¿å­˜ç›®å½•

# åˆ›å»ºä¿å­˜ç›®å½•
if not os.path.exists(save_dir):
    os.makedirs(save_dir, exist_ok=True)
    print(f"âœ… åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}")
else:
    print(f"âœ… æ¨¡å‹ä¿å­˜ç›®å½•å·²å­˜åœ¨: {save_dir}")

# CUDAè®¾å¤‡æ£€æŸ¥å’Œåˆå§‹åŒ–
def init_cuda_device():
    """å®‰å…¨çš„CUDAè®¾å¤‡åˆå§‹åŒ–"""
    try:
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return torch.device('cpu'), False
        
        # æ£€æŸ¥CUDAè®¾å¤‡æ•°é‡
        device_count = torch.cuda.device_count()
        print(f"ğŸš€ æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡")
        
        # è®¾ç½®é»˜è®¤è®¾å¤‡
        torch.cuda.set_device(0)
        device = torch.device('cuda:0')
        
        # æµ‹è¯•CUDAåˆå§‹åŒ–
        test_tensor = torch.zeros(1).cuda()
        del test_tensor
        torch.cuda.empty_cache()
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        props = torch.cuda.get_device_properties(0)
        memory_gb = props.total_memory / 1024**3
        print(f"ğŸ–¥ï¸  GPU: {props.name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {memory_gb:.1f}GB")
        
        return device, True
        
    except Exception as e:
        print(f"âŒ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
        return torch.device('cpu'), False

def get_dataloader_config(device, cuda_available):
    """è·å–å®‰å…¨çš„DataLoaderé…ç½®"""
    dataloader_workers = 0  # å¼ºåˆ¶ä½¿ç”¨ä¸»è¿›ç¨‹
    pin_memory_enabled = False  # ç¦ç”¨pin_memoryé¿å…CUDAå†…å­˜é—®é¢˜
    use_persistent = False  # ç¦ç”¨æŒä¹…worker
    
    print("ğŸš¨ å®‰å…¨æ¨¡å¼ï¼šå®Œå…¨ç¦ç”¨DataLoaderå¤šè¿›ç¨‹")
    print("   - Workers: 0 (ä¸»è¿›ç¨‹åŠ è½½)")
    print("   - Pin Memory: False (é¿å…CUDAå†…å­˜å†²çª)")
    
    return dataloader_workers, pin_memory_enabled, use_persistent

# åˆå§‹åŒ–CUDAè®¾å¤‡
device, cuda_available = init_cuda_device()

# è·å–å®‰å…¨çš„DataLoaderé…ç½®
dataloader_workers, pin_memory_enabled, use_persistent = get_dataloader_config(device, cuda_available)

# BiLSTMè®­ç»ƒå‚æ•°ï¼ˆä¸Transformerä¿æŒä¸€è‡´ï¼‰
# å‚ç…§æºä»£ç Train_.pyçš„è®­ç»ƒå‚æ•°è®¾ç½®
BILSTM_LR = 1e-4     # æºä»£ç ä¸­çš„å­¦ä¹ ç‡
BILSTM_EPOCH = 100   # æºä»£ç ä¸­çš„è®­ç»ƒè½®æ•°
BILSTM_BATCH_SIZE = 100  # æºä»£ç ä¸­çš„æ‰¹æ¬¡å¤§å°

# æ•°æ®å¤„ç†å‚æ•°ï¼ˆå‚ç…§æºä»£ç Train_.pyï¼‰
TIME_STEP = 1    # æºä»£ç ä¸­ä½¿ç”¨çš„æ—¶é—´æ­¥é•¿
INPUT_SIZE = 7   # æºä»£ç ä¸­ä½¿ç”¨çš„è¾“å…¥ç‰¹å¾æ•°é‡

print(f"ğŸ”§ BiLSTMè®­ç»ƒå‚æ•°ï¼ˆå‚ç…§æºä»£ç Train_.pyï¼‰:")
print(f"   å­¦ä¹ ç‡: {BILSTM_LR}")
print(f"   è®­ç»ƒè½®æ•°: {BILSTM_EPOCH}")
print(f"   æ‰¹æ¬¡å¤§å°: {BILSTM_BATCH_SIZE}")
print(f"   æ—¶é—´æ­¥é•¿: {TIME_STEP}")
print(f"   è¾“å…¥ç‰¹å¾æ•°: {INPUT_SIZE}")

# åŠ è½½è®­ç»ƒæ ·æœ¬
train_samples = load_train_samples()
print(f"ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")

#----------------------------------------BiLSTMè®­ç»ƒæ ¸å¿ƒä»£ç ------------------------------
# å†…å­˜ç›‘æ§å‡½æ•°
def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        allocated_gb = torch.cuda.memory_allocated() / 1024**3
        reserved_gb = torch.cuda.memory_reserved() / 1024**3
        total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return allocated_gb, reserved_gb, total_gb
    return 0, 0, 0

def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
    memory = psutil.virtual_memory()
    used_gb = memory.used / 1024**3
    total_gb = memory.total / 1024**3
    return used_gb, total_gb

# å†…å­˜ç›‘æ§è£…é¥°å™¨
def memory_monitor(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        # è®­ç»ƒå‰æ£€æŸ¥
        gpu_alloc, gpu_reserved, gpu_total = get_gpu_memory_info()
        sys_used, sys_total = get_system_memory_info()
        
        print(f"\nğŸ“Š è®­ç»ƒå‰å†…å­˜çŠ¶æ€:")
        print(f"   GPUæ˜¾å­˜: {gpu_alloc:.1f}GB / {gpu_total:.1f}GB ({gpu_alloc/gpu_total*100:.1f}%)")
        print(f"   ç³»ç»Ÿå†…å­˜: {sys_used:.1f}GB / {sys_total:.1f}GB ({sys_used/sys_total*100:.1f}%)")
        
        # å®‰å…¨æ£€æŸ¥
        if gpu_alloc/gpu_total > 0.85:
            print("âš ï¸  è­¦å‘Š: GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†ç¼“å­˜")
            torch.cuda.empty_cache()
        
        try:
            result = func(*args, **kwargs)
            return result
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("âŒ æ˜¾å­˜ä¸è¶³ï¼Œæ­£åœ¨æ¸…ç†ç¼“å­˜...")
                torch.cuda.empty_cache()
                raise e
            else:
                raise e
        finally:
            # è®­ç»ƒåçŠ¶æ€
            gpu_alloc_after, _, _ = get_gpu_memory_info()
            sys_used_after, _ = get_system_memory_info()
            print(f"\nğŸ“Š è®­ç»ƒåå†…å­˜çŠ¶æ€:")
            print(f"   GPUæ˜¾å­˜: {gpu_alloc_after:.1f}GB / {gpu_total:.1f}GB")
    
    return wrapper

# åŠ è½½æ‰€æœ‰æ ·æœ¬çš„vin_1æ•°æ®è¿›è¡ŒBILSTMè®­ç»ƒ
print(f"\nğŸ”„ åŠ è½½æ‰€æœ‰{len(train_samples)}ä¸ªæ ·æœ¬çš„vin_1æ•°æ®è¿›è¡ŒBILSTMè®­ç»ƒ")

# æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
all_train_X = []
all_train_y = []
valid_samples = []

for idx, sample_id in enumerate(train_samples):
    vin_1_file = os.path.join(data_dir, f'{sample_id}', 'vin_1.pkl')
    if os.path.exists(vin_1_file):
        try:
            with open(vin_1_file, 'rb') as file:
                sample_data = pickle.load(file)
            
            # ä½¿ç”¨æºä»£ç ä¸­çš„åŸå§‹æ•°æ®é¢„å¤„ç†å‡½æ•°
            sample_train_X, sample_train_y = prepare_training_data(sample_data, INPUT_SIZE, TIME_STEP, device)
            
            all_train_X.append(sample_train_X)
            all_train_y.append(sample_train_y)
            valid_samples.append(sample_id)
            
            if (idx + 1) % 10 == 0:
                print(f"   âœ“ å·²åŠ è½½ {idx + 1}/{len(train_samples)} ä¸ªæ ·æœ¬")
                
        except Exception as e:
            print(f"   âŒ æ ·æœ¬ {sample_id} åŠ è½½å¤±è´¥: {e}")
            continue
    else:
        print(f"   âš ï¸  æ ·æœ¬ {sample_id} çš„vin_1.pklæ–‡ä»¶ä¸å­˜åœ¨")

if len(all_train_X) > 0:
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
    train_X = torch.cat(all_train_X, dim=0)
    train_y = torch.cat(all_train_y, dim=0)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_samples)} ä¸ªæ ·æœ¬çš„æ•°æ®")
    print(f"   æœ‰æ•ˆæ ·æœ¬ID: {valid_samples}")
    print(f"   åˆå¹¶åè®­ç»ƒæ•°æ®å½¢çŠ¶:")
    print(f"   è¾“å…¥å½¢çŠ¶ (train_X): {train_X.shape}")
    print(f"   ç›®æ ‡å½¢çŠ¶ (train_y): {train_y.shape}")
    print(f"   é¢„æµ‹ç›®æ ‡: ä¸‹ä¸€æ—¶åˆ»ç´¢å¼•5å’Œ6çš„æ•°æ®")
else:
    print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    print("è·³è¿‡BILSTMè®­ç»ƒæ­¥éª¤")

if len(all_train_X) > 0:
    
    # åˆ›å»ºBILSTMæ¨¡å‹ï¼ˆç”¨äºæ‰¹æ¬¡å¤§å°è®¡ç®—ï¼‰
    try:
        bilstm_model = LSTM()
        if cuda_available:
            bilstm_model = bilstm_model.to(device)
        bilstm_model = bilstm_model.double()
        print(f"âœ… BILSTMæ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    except RuntimeError as e:
        if "CUDA" in str(e):
            print(f"âŒ CUDAè®¾å¤‡é”™è¯¯: {e}")
            print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
            cuda_available = False
            bilstm_model = LSTM().to(device).double()
        else:
            raise e
    
    # åº”ç”¨ä¸“é—¨çš„å¤§æ¨¡å‹æƒé‡åˆå§‹åŒ–
    def initialize_bilstm_weights(model):
        """å¤§è§„æ¨¡BILSTMä¸“ç”¨æƒé‡åˆå§‹åŒ–"""
        for name, param in model.named_parameters():
            if 'weight_ih' in name or 'weight_hh' in name:
                # LSTMæƒé‡ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œé™ä½æ–¹å·®
                nn.init.xavier_uniform_(param.data, gain=0.5)
            elif 'bias' in name:
                # åç½®åˆå§‹åŒ–ä¸º0ï¼Œforget gateåç½®è®¾ä¸º1
                nn.init.zeros_(param.data)
                if 'bias_hh' in name:
                    # forget gateåç½®è®¾ä¸º1ï¼Œå¸®åŠ©è®°å¿†
                    n = param.size(0)
                    param.data[n//4:n//2].fill_(1.0)
            elif 'weight' in name and 'fc' in name:
                # å…¨è¿æ¥å±‚æƒé‡
                nn.init.xavier_uniform_(param.data, gain=0.3)
            elif 'bias' in name and 'fc' in name:
                nn.init.zeros_(param.data)
        print("âœ… åº”ç”¨å¤§è§„æ¨¡BILSTMä¸“ç”¨æƒé‡åˆå§‹åŒ–")
    
    initialize_bilstm_weights(bilstm_model)
    
    # ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡
    bilstm_params = bilstm_model.count_parameters()
    print(f"\nğŸ“Š BILSTMæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°é‡: {bilstm_params:,}")
    print(f"   æ¨¡å‹è§„æ¨¡: {bilstm_params/1e6:.2f}M å‚æ•°")
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print(f"   æ¶æ„: BiLSTM(input=7, hidden=128, layers=3) + FC(256â†’128â†’64â†’2)")
    
    # ä½¿ç”¨æºä»£ç ä¸­çš„æ‰¹æ¬¡å¤§å°è®¾ç½®
    safe_batch_size = BILSTM_BATCH_SIZE
    print(f"\nğŸ¯ ä½¿ç”¨æ‰¹æ¬¡å¤§å°ï¼ˆå‚ç…§æºä»£ç ï¼‰: {safe_batch_size}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = MyDataset(train_X, train_y)
    
    bilstm_train_loader = DataLoader(
        train_dataset, 
        batch_size=safe_batch_size, 
        shuffle=True,
        num_workers=dataloader_workers,
        pin_memory=pin_memory_enabled,
        persistent_workers=use_persistent
    )
    
    # ä¼˜åŒ–å™¨é…ç½®ï¼ˆå¤§æ¨¡å‹é€‚é…ç‰ˆï¼‰
    bilstm_optimizer = torch.optim.AdamW(bilstm_model.parameters(), 
                                        lr=BILSTM_LR, 
                                        weight_decay=1e-5,
                                        betas=(0.9, 0.999),
                                        eps=1e-8)
    bilstm_loss_func = nn.MSELoss()
    
    # ç®€åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå‚ç…§æºä»£ç Train_.pyï¼‰
    def get_lr_with_decay(epoch):
        # ä½¿ç”¨æºä»£ç ä¸­çš„ç®€å•å­¦ä¹ ç‡è¡°å‡
        lr_decay_freq = 25  # æºä»£ç ä¸­çš„è¡°å‡é¢‘ç‡
        decay_factor = 0.9
        return BILSTM_LR * (decay_factor ** (epoch // lr_decay_freq))
    
    # æ‰‹åŠ¨å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå‚ç…§æºä»£ç ï¼‰
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        bilstm_optimizer, 
        lr_lambda=lambda epoch: get_lr_with_decay(epoch) / BILSTM_LR
    )
    
    print(f"\nğŸš€ A100å¤§è§„æ¨¡BILSTMè®­ç»ƒé…ç½®:")
    print(f"   æ¨¡å‹è§„æ¨¡: hidden_size=128, num_layers=3")
    print(f"   æ‰¹æ¬¡å¤§å°: {safe_batch_size}")
    print(f"   å­¦ä¹ ç‡: {BILSTM_LR:.6f}")
    print(f"   è®­ç»ƒè½®æ•°: {BILSTM_EPOCH}")
    print(f"   é¢„çƒ­è½®æ•°: {WARMUP_EPOCHS}")
    print(f"   ä¼˜åŒ–å™¨: AdamW")
    print(f"   å­¦ä¹ ç‡è°ƒåº¦: é•¿é¢„çƒ­ + CosineAnnealing")
    
    # å†…å­˜ç›‘æ§çš„BILSTMè®­ç»ƒå‡½æ•°
    @memory_monitor
    def bilstm_training_loop(train_loader):
        print(f"\nğŸ‹ï¸ å¼€å§‹BILSTMè®­ç»ƒ (A100ä¼˜åŒ–ç‰ˆæœ¬)...")
        
        # è®­ç»ƒå‰CUDAçŠ¶æ€æ£€æŸ¥
        if device.type == 'cuda':
            try:
                torch.cuda.synchronize()
                print(f"âœ… CUDAè®¾å¤‡çŠ¶æ€æ­£å¸¸: {torch.cuda.get_device_name()}")
                gpu_alloc, _, gpu_total = get_gpu_memory_info()
                print(f"ğŸ”‹ å½“å‰GPUæ˜¾å­˜: {gpu_alloc/gpu_total*100:.1f}%")
            except Exception as e:
                print(f"âš ï¸ CUDAçŠ¶æ€æ£€æŸ¥è­¦å‘Š: {e}")
        
        loss_train_100 = []
        bilstm_model.train()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(BILSTM_EPOCH):
            epoch_losses = []
            
            # æ¯ä¸ªepochå¼€å§‹å‰æ£€æŸ¥å†…å­˜
            if epoch % 50 == 0:
                gpu_alloc, _, gpu_total = get_gpu_memory_info()
                print(f"   Epoch {epoch}: GPUæ˜¾å­˜ä½¿ç”¨ {gpu_alloc/gpu_total*100:.1f}%")
            
            # DataLoaderæšä¸¾ä¿æŠ¤
            try:
                for step, (b_x, b_y) in enumerate(train_loader):
                    try:
                        # å‰å‘ä¼ æ’­
                        output = bilstm_model(b_x)
                        loss = bilstm_loss_func(b_y, output)
                        
                        # åå‘ä¼ æ’­
                        bilstm_optimizer.zero_grad()
                        loss.backward()
                        
                        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                        torch.nn.utils.clip_grad_norm_(bilstm_model.parameters(), max_norm=0.5)
                        
                        bilstm_optimizer.step()
                        
                        # è®°å½•æŸå¤±
                        if step % 20 == 0:
                            loss_train_100.append(loss.cpu().detach().numpy())
                            epoch_losses.append(loss.item())
                        
                        # å®šæœŸæ¸…ç†ç¼“å­˜
                        if step % 100 == 0:
                            torch.cuda.empty_cache()
                            
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            print(f"   âš ï¸  Epoch {epoch}, Step {step}: æ˜¾å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜åç»§ç»­")
                            torch.cuda.empty_cache()
                            continue
                        else:
                            raise e
            except RuntimeError as e:
                print(f"ğŸš¨ DataLoaderé”™è¯¯ (Epoch {epoch}): {e}")
                if "initialization error" in str(e).lower():
                    print("   ğŸ”„ å°è¯•é‡æ–°åˆ›å»ºDataLoader...")
                    # å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼é‡æ–°åˆ›å»ºDataLoader
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=safe_batch_size, 
                        shuffle=True,
                        num_workers=0,
                        pin_memory=False,
                        persistent_workers=False
                    )
                    print("   âœ… DataLoaderé‡æ–°åˆ›å»ºå®Œæˆï¼Œç»§ç»­è®­ç»ƒ")
                    continue
                else:
                    raise e
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # å®šæœŸè¾“å‡ºè®­ç»ƒçŠ¶æ€
            if epoch % 20 == 0:
                avg_loss = np.mean(epoch_losses) if epoch_losses else 0
                current_lr = scheduler.get_last_lr()[0]
                print(f"   Epoch {epoch:3d}/{BILSTM_EPOCH}: Loss={avg_loss:.6f}, LR={current_lr:.6f}")
        
        print(f"âœ… BILSTMè®­ç»ƒå®Œæˆï¼Œå…±è®°å½• {len(loss_train_100)} ä¸ªLosså€¼")
        return loss_train_100
    
    # æ‰§è¡Œè®­ç»ƒ
    loss_train_100 = bilstm_training_loop(bilstm_train_loader)
    
    #----------------------------------------æ¨¡å‹ä¿å­˜åŠŸèƒ½------------------------------
    print(f"\nğŸ’¾ ä¿å­˜BiLSTMæ¨¡å‹å’Œè®­ç»ƒç»“æœ...")
    
    # ä¿å­˜BILSTMæ¨¡å‹å’ŒLossè®°å½•ï¼ˆåŸºäºæ‰€æœ‰æ ·æœ¬è®­ç»ƒï¼‰
    bilstm_model_path = os.path.join(save_dir, 'bilstm_model_all_samples.pth')
    bilstm_loss_path = os.path.join(save_dir, 'bilstm_loss_record_all_samples.pkl')
    
    # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
    torch.save(bilstm_model.state_dict(), bilstm_model_path)
    
    # ä¿å­˜æŸå¤±è®°å½•
    with open(bilstm_loss_path, 'wb') as f:
        pickle.dump(loss_train_100, f)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        'valid_samples': valid_samples,
        'total_samples': len(valid_samples),
        'train_data_shape': (train_X.shape, train_y.shape),
        'training_epochs': BILSTM_EPOCH,
        'learning_rate': BILSTM_LR,
        'batch_size': safe_batch_size,
        'model_parameters': bilstm_params,
        'device_used': str(device),
        'final_loss': loss_train_100[-1] if loss_train_100 else None,
        'warmup_epochs': WARMUP_EPOCHS,
        'optimizer': 'AdamW',
        'scheduler': 'LambdaLR with warmup + cosine annealing'
    }
    
    training_info_path = os.path.join(save_dir, 'bilstm_training_info.pkl')
    with open(training_info_path, 'wb') as f:
        pickle.dump(training_info, f)
    
    # ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯ï¼ˆç”¨äºé‡å»ºæ¨¡å‹ï¼‰
    model_architecture = {
        'model_class': 'LSTM',
        'input_size': INPUT_SIZE,
        'time_step': TIME_STEP,
        'hidden_size': 128,
        'num_layers': 3,
        'output_size': 2,
        'model_type': 'BiLSTM',
        'note': 'ä¸“é—¨ä¸ºMC-AEå‡†å¤‡çš„BiLSTMæ¨¡å‹'
    }
    
    architecture_path = os.path.join(save_dir, 'bilstm_architecture.pkl')
    with open(architecture_path, 'wb') as f:
        pickle.dump(model_architecture, f)
    
    # æ‰“å°ä¿å­˜ä¿¡æ¯
    print(f"âœ… BiLSTMæ¨¡å‹å·²ä¿å­˜: {bilstm_model_path}")
    print(f"âœ… Lossè®°å½•å·²ä¿å­˜: {bilstm_loss_path}")
    print(f"âœ… è®­ç»ƒä¿¡æ¯å·²ä¿å­˜: {training_info_path}")
    print(f"âœ… æ¨¡å‹æ¶æ„å·²ä¿å­˜: {architecture_path}")
    print(f"âœ… æ¨¡å‹åŸºäº {len(valid_samples)} ä¸ªæ ·æœ¬è®­ç»ƒå®Œæˆ")
    
    # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
    print(f"\nğŸ“Š BiLSTMè®­ç»ƒæ€»ç»“:")
    print(f"   æ¨¡å‹å‚æ•°é‡: {bilstm_params:,}")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(valid_samples)}")
    print(f"   è®­ç»ƒè½®æ•°: {BILSTM_EPOCH}")
    print(f"   æ‰¹æ¬¡å¤§å°: {safe_batch_size}")
    print(f"   å­¦ä¹ ç‡: {BILSTM_LR:.6f}")
    print(f"   æœ€ç»ˆæŸå¤±: {loss_train_100[-1]:.6f}" if loss_train_100 else "   æœ€ç»ˆæŸå¤±: æœªè®°å½•")
    print(f"   æ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}")

else:
    print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    print("è·³è¿‡BILSTMè®­ç»ƒæ­¥éª¤")

print("\n" + "="*60)
print("BiLSTMä¸“é—¨è®­ç»ƒæ¨¡å¼å®Œæˆ")
print("æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä¾›MC-AEä½¿ç”¨")
print("="*60)
