# ä¸­æ–‡æ³¨é‡Šï¼šå¯¼å…¥å¸¸ç”¨åº“å’Œè‡ªå®šä¹‰æ¨¡å—
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle

# GPUè®¾å¤‡é…ç½®
import os
# ä½¿ç”¨æŒ‡å®šçš„GPUè®¾å¤‡
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€å¼ GPU
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"   å½“å‰GPU: {torch.cuda.current_device()}")
    print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# ä¸­æ–‡æ³¨é‡Šï¼šå¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

#----------------------------------------BiLSTMåŸºå‡†è®­ç»ƒé…ç½®------------------------------
print("="*50)
print("BiLSTMåŸºå‡†è®­ç»ƒæ¨¡å¼")
print("ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("è·³è¿‡Transformerè®­ç»ƒï¼Œç›´æ¥è¿›è¡ŒMC-AEè®­ç»ƒ")
print("="*50)

#----------------------------------------æ•°æ®åŠ è½½------------------------------
# ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬IDï¼ˆ0-200å·ï¼‰
def load_train_samples():
    """ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬ID"""
    try:
        import pandas as pd
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–0-200èŒƒå›´çš„æ ·æœ¬
        all_samples = df['Num'].tolist()
        train_samples = [i for i in all_samples if 0 <= i <= 200]
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬:")
        print(f"   è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-200")
        print(f"   å®é™…å¯ç”¨æ ·æœ¬: {len(train_samples)} ä¸ª")
        print(f"   æ ·æœ¬ID: {train_samples[:10]}..." if len(train_samples) > 10 else f"   æ ·æœ¬ID: {train_samples}")
        
        return train_samples
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æ ·æœ¬èŒƒå›´ 0-20")
        return list(range(21))

train_samples = load_train_samples()
print(f"ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")

#----------------------------------------MC-AEè®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼‰------------------------
print("="*50)
print("é˜¶æ®µ1: å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# ä¸­æ–‡æ³¨é‡Šï¼šåŠ è½½MC-AEæ¨¡å‹è¾“å…¥ç‰¹å¾ï¼ˆvin_2.pklå’Œvin_3.pklï¼‰
# åˆå¹¶æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„vin_2å’Œvin_3æ•°æ®
all_vin2_data = []
all_vin3_data = []

for sample_id in train_samples:
    vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
    vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
    
    # åŠ è½½åŸå§‹vin_2å’Œvin_3æ•°æ®
    with open(vin2_path, 'rb') as file:
        vin2_data = pickle.load(file)
        print(f"åŸå§‹æ ·æœ¬ {sample_id} vin_2: {vin2_data.shape}")
    
    with open(vin3_path, 'rb') as file:
        vin3_data = pickle.load(file)
        print(f"åŸå§‹æ ·æœ¬ {sample_id} vin_3: {vin3_data.shape}")
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•æ›¿æ¢
    print(f"æ ·æœ¬ {sample_id}: ä½¿ç”¨åŸå§‹BiLSTMè¾“å‡ºæ•°æ®")
    print(f"  vin_2å½¢çŠ¶: {vin2_data.shape}")
    print(f"  vin_3å½¢çŠ¶: {vin3_data.shape}")
    print(f"  åŸå§‹vin_2[x[0]]èŒƒå›´: [{vin2_data[:, 0].min():.3f}, {vin2_data[:, 0].max():.3f}]")
    print(f"  åŸå§‹vin_3[x[0]]èŒƒå›´: [{vin3_data[:, 0].min():.3f}, {vin3_data[:, 0].max():.3f}]")
    
    all_vin2_data.append(vin2_data)
    all_vin3_data.append(vin3_data)

# åˆå¹¶æ•°æ®
combined_tensor = torch.cat(all_vin2_data, dim=0)
combined_tensorx = torch.cat(all_vin3_data, dim=0)

print(f"åˆå¹¶åvin_2æ•°æ®å½¢çŠ¶: {combined_tensor.shape}")
print(f"åˆå¹¶åvin_3æ•°æ®å½¢çŠ¶: {combined_tensorx.shape}")

#----------------------------------------MC-AEå¤šé€šé“è‡ªç¼–ç å™¨è®­ç»ƒ--------------------------
print("="*50)
print("é˜¶æ®µ2: è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# ä¸­æ–‡æ³¨é‡Šï¼šå®šä¹‰ç‰¹å¾åˆ‡ç‰‡ç»´åº¦
# vin_2.pkl
dim_x = 2
dim_y = 110
dim_z = 110
dim_q = 3

# ä¸­æ–‡æ³¨é‡Šï¼šåˆ†å‰²ç‰¹å¾å¼ é‡
x_recovered = combined_tensor[:, :dim_x]
y_recovered = combined_tensor[:, dim_x:dim_x + dim_y]
z_recovered = combined_tensor[:, dim_x + dim_y: dim_x + dim_y + dim_z]
q_recovered = combined_tensor[:, dim_x + dim_y + dim_z:]

# vin_3.pkl
dim_x2 = 2
dim_y2 = 110
dim_z2 = 110
dim_q2= 4

x_recovered2 = combined_tensorx[:, :dim_x2]
y_recovered2 = combined_tensorx[:, dim_x2:dim_x2 + dim_y2]
z_recovered2 = combined_tensorx[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
q_recovered2 = combined_tensorx[:, dim_x2 + dim_y2 + dim_z2:]

# è®­ç»ƒè¶…å‚æ•°é…ç½®
EPOCH = 300
LR = 5e-4
BATCHSIZE = 1000  # å¢å¤§æ‰¹æ¬¡å¤§å°ä»¥æé«˜GPUåˆ©ç”¨ç‡

# ç”¨äºè®°å½•è®­ç»ƒæŸå¤±
train_losses_mcae1 = []
train_losses_mcae2 = []

# ä¸­æ–‡æ³¨é‡Šï¼šè‡ªå®šä¹‰å¤šè¾“å…¥æ•°æ®é›†ç±»ï¼ˆæœ¬åœ°å®šä¹‰ï¼ŒéClass_.pyä¸­çš„Datasetï¼‰
class Dataset(Dataset):
    def __init__(self, x, y, z, q):
        self.x = x.to(torch.double)
        self.y = y.to(torch.double)
        self.z = z.to(torch.double)
        self.q = q.to(torch.double)
    def __len__(self):
        return len(self.x)
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.z[idx], self.q[idx]

# ä¸­æ–‡æ³¨é‡Šï¼šç”¨DataLoaderæ‰¹é‡åŠ è½½å¤šé€šé“ç‰¹å¾æ•°æ®
train_loader_u = DataLoader(Dataset(x_recovered, y_recovered, z_recovered, q_recovered), batch_size=BATCHSIZE, shuffle=False)

# ä¸­æ–‡æ³¨é‡Šï¼šåˆå§‹åŒ–MC-AEæ¨¡å‹
net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, activation_fn=custom_activation, use_dx_in_forward=True).to(device)
netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)

optimizer = torch.optim.Adam(net.parameters(), lr=LR)
l1_lambda = 0.01
loss_f = nn.MSELoss()
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    for iteration, (x, y, z, q) in enumerate(train_loader_u):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        net = net.double()
        recon_im , recon_p = net(x,z,q)
        loss_u = loss_f(y,recon_im)
        total_loss += loss_u.item()
        num_batches += 1
        optimizer.zero_grad()
        loss_u.backward()
        optimizer.step()
    avg_loss = total_loss / num_batches
    train_losses_mcae1.append(avg_loss)
    if epoch % 50 == 0:
        print('MC-AE1 Epoch: {:2d} | Average Loss: {:.6f}'.format(epoch, avg_loss))

# ä¸­æ–‡æ³¨é‡Šï¼šå…¨é‡æ¨ç†ï¼Œè·å¾—é‡æ„è¯¯å·®
train_loader2 = DataLoader(Dataset(x_recovered, y_recovered, z_recovered, q_recovered), batch_size=len(x_recovered), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loader2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    net = net.double()
    recon_imtest, recon = net(x, z, q)
AA = recon_imtest.cpu().detach().numpy()
yTrainU = y_recovered.cpu().detach().numpy()
ERRORU = AA - yTrainU

# ä¸­æ–‡æ³¨é‡Šï¼šç¬¬äºŒç»„ç‰¹å¾çš„MC-AEè®­ç»ƒ
train_loader_soc = DataLoader(Dataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), batch_size=BATCHSIZE, shuffle=False)
optimizer = torch.optim.Adam(netx.parameters(), lr=LR)
loss_f = nn.MSELoss()
avg_loss_list_x = []
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    for iteration, (x, y, z, q) in enumerate(train_loader_soc):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        netx = netx.double()
        recon_im , z  = netx(x,z,q)
        loss_x = loss_f(y,recon_im)
        total_loss += loss_x.item()
        num_batches += 1
        optimizer.zero_grad()
        loss_x.backward()
        optimizer.step()
    avg_loss = total_loss / num_batches
    avg_loss_list_x.append(avg_loss)
    train_losses_mcae2.append(avg_loss)
    if epoch % 50 == 0:
        print('MC-AE2 Epoch: {:2d} | Average Loss: {:.6f}'.format(epoch, avg_loss))

train_loaderx2 = DataLoader(Dataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), batch_size=len(x_recovered2), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loaderx2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    netx = netx.double()
    recon_imtestx, z = netx(x, z, q)

BB = recon_imtestx.cpu().detach().numpy()
yTrainX = y_recovered2.cpu().detach().numpy()
ERRORX = BB - yTrainX

# ä¸­æ–‡æ³¨é‡Šï¼šè¯Šæ–­ç‰¹å¾æå–ä¸PCAåˆ†æ
df_data = DiagnosisFeature(ERRORU,ERRORX)

v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = PCA(df_data,0.95,0.95)

# è®­ç»ƒç»“æŸåè‡ªåŠ¨ä¿å­˜æ¨¡å‹å’Œåˆ†æç»“æœ
print("="*50)
print("ä¿å­˜BiLSTMåŸºå‡†è®­ç»ƒç»“æœ")
print("="*50)

# ç»˜åˆ¶è®­ç»ƒç»“æœ
print("ğŸ“ˆ ç»˜åˆ¶BiLSTMè®­ç»ƒæ›²çº¿...")

# Linuxç¯å¢ƒå­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK SC', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False

# Linuxç¯å¢ƒmatplotlibé…ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

# åˆ›å»ºå›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# å­å›¾1: MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿
ax1 = axes[0, 0]
epochs = range(1, len(train_losses_mcae1) + 1)
ax1.plot(epochs, train_losses_mcae1, 'b-', linewidth=2, label='MC-AE1 Training Loss')
ax1.set_xlabel('è®­ç»ƒè½®æ•°')
ax1.set_ylabel('MSEæŸå¤±')
ax1.set_title('MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿')
ax1.grid(True, alpha=0.3)
ax1.legend()
ax1.set_yscale('log')

# å­å›¾2: MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿ 
ax2 = axes[0, 1]
ax2.plot(epochs, train_losses_mcae2, 'r-', linewidth=2, label='MC-AE2 Training Loss')
ax2.set_xlabel('è®­ç»ƒè½®æ•°')
ax2.set_ylabel('MSEæŸå¤±')
ax2.set_title('MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿')
ax2.grid(True, alpha=0.3)
ax2.legend()
ax2.set_yscale('log')

# å­å›¾3: MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ
ax3 = axes[1, 0]
reconstruction_errors_1 = ERRORU.flatten()
ax3.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
         label=f'MC-AE1é‡æ„è¯¯å·® (å‡å€¼: {np.mean(np.abs(reconstruction_errors_1)):.4f})')
ax3.set_xlabel('ç»å¯¹é‡æ„è¯¯å·®')
ax3.set_ylabel('é¢‘æ•°')
ax3.set_title('MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax3.legend()
ax3.grid(True, alpha=0.3)

# å­å›¾4: MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ
ax4 = axes[1, 1]
reconstruction_errors_2 = ERRORX.flatten()
ax4.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
         label=f'MC-AE2é‡æ„è¯¯å·® (å‡å€¼: {np.mean(np.abs(reconstruction_errors_2)):.4f})')
ax4.set_xlabel('ç»å¯¹é‡æ„è¯¯å·®')
ax4.set_ylabel('é¢‘æ•°')
ax4.set_title('MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f'{result_dir}/bilstm_training_results.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"âœ… BiLSTMè®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {result_dir}/bilstm_training_results.png")

# 1. åˆ›å»ºç»“æœç›®å½•
result_dir = '/mnt/bz25t/bzhy/zhanglikang/project/models'
if not os.path.exists(result_dir):
    os.makedirs(result_dir)

# 2. ä¿å­˜è¯Šæ–­ç‰¹å¾DataFrame
df_data.to_excel(f'{result_dir}/diagnosis_feature_bilstm_baseline.xlsx', index=False)
df_data.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline.csv', index=False)
print(f"âœ“ ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline.xlsx/csv")

# 3. ä¿å­˜PCAåˆ†æä¸»è¦ç»“æœ
np.save(f'{result_dir}/v_I_bilstm_baseline.npy', v_I)
np.save(f'{result_dir}/v_bilstm_baseline.npy', v)
np.save(f'{result_dir}/v_ratio_bilstm_baseline.npy', v_ratio)
np.save(f'{result_dir}/p_k_bilstm_baseline.npy', p_k)
np.save(f'{result_dir}/data_mean_bilstm_baseline.npy', data_mean)
np.save(f'{result_dir}/data_std_bilstm_baseline.npy', data_std)
np.save(f'{result_dir}/T_95_limit_bilstm_baseline.npy', T_95_limit)
np.save(f'{result_dir}/T_99_limit_bilstm_baseline.npy', T_99_limit)
np.save(f'{result_dir}/SPE_95_limit_bilstm_baseline.npy', SPE_95_limit)
np.save(f'{result_dir}/SPE_99_limit_bilstm_baseline.npy', SPE_99_limit)
np.save(f'{result_dir}/P_bilstm_baseline.npy', P)
np.save(f'{result_dir}/k_bilstm_baseline.npy', k)
np.save(f'{result_dir}/P_t_bilstm_baseline.npy', P_t)
np.save(f'{result_dir}/X_bilstm_baseline.npy', X)
np.save(f'{result_dir}/data_nor_bilstm_baseline.npy', data_nor)
print(f"âœ“ ä¿å­˜PCAåˆ†æç»“æœ: {result_dir}/*_bilstm_baseline.npy")

# 4. ä¿å­˜CombinedAEæ¨¡å‹å‚æ•°
torch.save(net.state_dict(), f'{result_dir}/net_model_bilstm_baseline.pth')
torch.save(netx.state_dict(), f'{result_dir}/netx_model_bilstm_baseline.pth')
print(f"âœ“ ä¿å­˜MC-AEæ¨¡å‹: {result_dir}/net_model_bilstm_baseline.pth, {result_dir}/netx_model_bilstm_baseline.pth")

# 5. ä¿å­˜è®­ç»ƒå†å²
training_history = {
    'mcae1_losses': train_losses_mcae1,
    'mcae2_losses': train_losses_mcae2,
    'final_mcae1_loss': train_losses_mcae1[-1],
    'final_mcae2_loss': train_losses_mcae2[-1],
    'mcae1_reconstruction_error_mean': np.mean(np.abs(ERRORU)),
    'mcae1_reconstruction_error_std': np.std(np.abs(ERRORU)),
    'mcae2_reconstruction_error_mean': np.mean(np.abs(ERRORX)),
    'mcae2_reconstruction_error_std': np.std(np.abs(ERRORX)),
    'training_samples': len(train_samples),
    'epochs': EPOCH,
    'learning_rate': LR,
    'batch_size': BATCHSIZE
}

import pickle
with open(f'{result_dir}/bilstm_training_history.pkl', 'wb') as f:
    pickle.dump(training_history, f)
print(f"âœ“ ä¿å­˜è®­ç»ƒå†å²: {result_dir}/bilstm_training_history.pkl")

print("="*50)
print("ğŸ‰ BiLSTMåŸºå‡†è®­ç»ƒå®Œæˆï¼")
print("="*50)
print("BiLSTMåŸºå‡†æ¨¡å¼æ€»ç»“ï¼š")
print("1. âœ… è·³è¿‡Transformerè®­ç»ƒé˜¶æ®µ")
print("2. âœ… ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("3. âœ… ä¿æŒPack Modelingè¾“å‡ºvin_2[x[1]]å’Œvin_3[x[1]]ä¸å˜")
print("4. âœ… MC-AEä½¿ç”¨åŸå§‹BiLSTMæ•°æ®è¿›è¡Œè®­ç»ƒ")
print("5. âœ… æ‰€æœ‰æ¨¡å‹å’Œç»“æœæ–‡ä»¶æ·»åŠ '_bilstm_baseline'åç¼€")
print("")
print("ğŸ“Š æ¯”å¯¹è¯´æ˜ï¼š")
print("   - æ­¤æ¨¡å¼å»ºç«‹BiLSTMåŸºå‡†æ€§èƒ½")
print("   - å¯ä¸Transformeræ¨¡å¼è¿›è¡Œå…¬å¹³å¯¹æ¯”")
print("   - ä¾¿äºè¯„ä¼°Transformeræ›¿æ¢çš„æ•ˆæœ")
print("   - è®­ç»ƒæ—¶é—´æ›´çŸ­ï¼Œé€‚åˆå¿«é€ŸéªŒè¯") 