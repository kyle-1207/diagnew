# ä¸­æ–‡æ³¨é‡Šï¼šå¯¼å…¥å¸¸ç”¨åº“å’Œè‡ªå®šä¹‰æ¨¡å—
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
import psutil  # ç³»ç»Ÿå†…å­˜ç›‘æ§

# GPUè®¾å¤‡é…ç½® - A100ç¯å¢ƒ
import os
# ä½¿ç”¨æŒ‡å®šçš„GPUè®¾å¤‡ï¼ˆA100ç¯å¢ƒï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # åªä½¿ç”¨GPU2

# CUDAè°ƒè¯•åŠŸèƒ½ï¼ˆåœ¨A100ç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´åˆå§‹åŒ–é—®é¢˜ï¼Œæš‚æ—¶ç¦ç”¨ï¼‰
# os.environ['TORCH_USE_CUDA_DSA'] = '1'  # å¯èƒ½å¯¼è‡´A100åˆå§‹åŒ–é”™è¯¯
# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # ä»…åœ¨éœ€è¦è¯¦ç»†è°ƒè¯•æ—¶å¯ç”¨
print("ğŸ”§ CUDAè°ƒè¯•æ¨¡å¼å·²ç¦ç”¨ï¼ˆé¿å…A100åˆå§‹åŒ–å†²çªï¼‰")

# deviceå°†åœ¨init_cuda_device()å‡½æ•°ä¸­åˆå§‹åŒ–

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print("\nğŸ–¥ï¸ A100 GPUé…ç½®ä¿¡æ¯:")
    print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\n   GPU {i} ({props.name}):")
        print(f"      æ€»æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
    print(f"\n   å½“å‰ä½¿ç”¨: ä»…GPU2 (A100 80GBä¼˜åŒ–)")
    print(f"   ä¸»GPUè®¾å¤‡: cuda:0 (ç‰©ç†GPU2)")
    print(f"   å¤‡æ³¨: å•å¡A100ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨80GBæ˜¾å­˜")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# ä¸­æ–‡æ³¨é‡Šï¼šå¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒmatplotlibé…ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# Linuxç¯å¢ƒå­—ä½“è®¾ç½® - ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
import matplotlib.font_manager as fm

# å°è¯•å¤šç§å­—ä½“æ–¹æ¡ˆ
font_options = [
    'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC',
    'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS'
]

# æ£€æŸ¥å¯ç”¨å­—ä½“
available_fonts = []
for font in font_options:
    try:
        fm.findfont(font)
        available_fonts.append(font)
    except:
        continue

# è®¾ç½®å­—ä½“
if available_fonts:
    plt.rcParams['font.sans-serif'] = available_fonts
    print(f"âœ… Linuxå­—ä½“é…ç½®å®Œæˆï¼Œä½¿ç”¨å­—ä½“: {available_fonts[0]}")
else:
    # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

plt.rcParams['axes.unicode_minus'] = False

plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

#----------------------------------------BiLSTM A100ä¼˜åŒ–è®­ç»ƒé…ç½®------------------------------
print("="*50)
print("BiLSTM A100ä¼˜åŒ–è®­ç»ƒæ¨¡å¼")
print("ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("è·³è¿‡Transformerè®­ç»ƒï¼Œç›´æ¥è¿›è¡ŒMC-AEè®­ç»ƒ")
print("å¯ç”¨A100 80GBä¼˜åŒ–å’Œæ··åˆç²¾åº¦è®­ç»ƒ")
print("="*50)

#----------------------------------------æ•°æ®åŠ è½½------------------------------
# ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬IDï¼ˆ0-200å·ï¼‰
def load_train_samples():
    """ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬ID"""
    try:
        import pandas as pd
        # Linuxè·¯å¾„æ ¼å¼
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–0-100èŒƒå›´çš„æ ·æœ¬
        all_samples = df['Num'].tolist()
        train_samples = [i for i in all_samples if 0 <= i <= 100]
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬:")
        print(f"   è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-100")
        print(f"   å®é™…å¯ç”¨æ ·æœ¬: {len(train_samples)} ä¸ª")
        print(f"   æ ·æœ¬ID: {train_samples[:10]}..." if len(train_samples) > 10 else f"   æ ·æœ¬ID: {train_samples}")
        
        return train_samples
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æ ·æœ¬èŒƒå›´ 0-20")
        return list(range(21))

train_samples = load_train_samples()
print(f"ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")

# ç»Ÿä¸€çš„è·¯å¾„é…ç½® - Linuxç¯å¢ƒ
data_dir = '/mnt/bz25t/bzhy/zhanglikang/project/QAS'  # æ•°æ®ç›®å½•
save_dir = '/mnt/bz25t/bzhy/datasave/BILSTM_train'  # æ¨¡å‹ä¿å­˜ç›®å½•

# åˆ›å»ºä¿å­˜ç›®å½•
if not os.path.exists(save_dir):
    os.makedirs(save_dir, exist_ok=True)
    print(f"âœ… åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}")
else:
    print(f"âœ… æ¨¡å‹ä¿å­˜ç›®å½•å·²å­˜åœ¨: {save_dir}")

# CUDAè®¾å¤‡æ£€æŸ¥å’Œåˆå§‹åŒ–
def init_cuda_device():
    """å®‰å…¨çš„CUDAè®¾å¤‡åˆå§‹åŒ–"""
    try:
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return torch.device('cpu'), False
        
        # æ£€æŸ¥CUDAè®¾å¤‡æ•°é‡
        device_count = torch.cuda.device_count()
        print(f"ğŸš€ æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡")
        
        # è®¾ç½®é»˜è®¤è®¾å¤‡
        torch.cuda.set_device(0)
        device = torch.device('cuda:0')
        
        # æµ‹è¯•CUDAåˆå§‹åŒ–
        test_tensor = torch.zeros(1).cuda()
        del test_tensor
        torch.cuda.empty_cache()
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        props = torch.cuda.get_device_properties(0)
        memory_gb = props.total_memory / 1024**3
        print(f"ğŸ–¥ï¸  GPU: {props.name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {memory_gb:.1f}GB")
        
        return device, True
        
    except Exception as e:
        print(f"âŒ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
        return torch.device('cpu'), False

def get_dataloader_config(device, cuda_available):
    """
    è·å–å®‰å…¨çš„DataLoaderé…ç½® - å®Œå…¨ç¦ç”¨å¤šè¿›ç¨‹é¿å…CUDAåˆå§‹åŒ–é—®é¢˜
    """
    # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šå®Œå…¨ç¦ç”¨å¤šè¿›ç¨‹DataLoader
    # è¿™æ˜¯æœ€å®‰å…¨çš„é…ç½®ï¼Œé¿å…æ‰€æœ‰CUDA workerè¿›ç¨‹é—®é¢˜
    dataloader_workers = 0  # å¼ºåˆ¶ä½¿ç”¨ä¸»è¿›ç¨‹ï¼Œæ— å¤šè¿›ç¨‹
    pin_memory_enabled = False  # ç¦ç”¨pin_memoryé¿å…CUDAå†…å­˜é—®é¢˜
    use_persistent = False  # ç¦ç”¨æŒä¹…worker
    
    print("ğŸš¨ ç´§æ€¥ä¿®å¤æ¨¡å¼ï¼šå®Œå…¨ç¦ç”¨DataLoaderå¤šè¿›ç¨‹")
    print("   - Workers: 0 (ä¸»è¿›ç¨‹åŠ è½½)")
    print("   - Pin Memory: False (é¿å…CUDAå†…å­˜å†²çª)")
    print("   - æ€§èƒ½å½±å“: è½»å¾®ï¼Œä½†ç¡®ä¿ç¨³å®šæ€§")
    
    return dataloader_workers, pin_memory_enabled, use_persistent

# åˆå§‹åŒ–CUDAè®¾å¤‡
device, cuda_available = init_cuda_device()

# è·å–å®‰å…¨çš„DataLoaderé…ç½®
dataloader_workers, pin_memory_enabled, use_persistent = get_dataloader_config(device, cuda_available)

# æ˜¾ç¤ºä¿®å¤çŠ¶æ€
print("ğŸ”§ CUDAé”™è¯¯ä¿®å¤çŠ¶æ€:")
print(f"   - CUDAè°ƒè¯•æ¨¡å¼: å·²ç¦ç”¨ï¼ˆé¿å…A100å†²çªï¼‰")
print(f"   - DataLoader workers: {dataloader_workers}")
print(f"   - Pin memory: {pin_memory_enabled}")
print(f"   - Persistent workers: {use_persistent}")
print("   - æ‰¹æ¬¡å¤§å°: å·²ä¼˜åŒ–ä¸ºå®‰å…¨çº§åˆ«")

# MC-AEè®­ç»ƒå‚æ•°ï¼ˆä¸Transformerä¿æŒä¸€è‡´ï¼‰
EPOCH = 500  # ä¸Transformerä¿æŒä¸€è‡´çš„MC-AEè®­ç»ƒè½®æ•°
INIT_LR = 2e-5  # ä¸Transformerä¿æŒä¸€è‡´çš„åˆå§‹å­¦ä¹ ç‡
MAX_LR = 1e-4   # ä¸Transformerä¿æŒä¸€è‡´çš„æœ€å¤§å­¦ä¹ ç‡

# æ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å° - A100ä¼˜åŒ–ï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰
if cuda_available:
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory_gb >= 80:  # A100 80GB
        BATCHSIZE = 2000  # ä¸Transformerä¿æŒä¸€è‡´çš„MC-AEæ‰¹æ¬¡å¤§å°
    elif gpu_memory_gb >= 40:  # A100 40GB
        BATCHSIZE = 1000
    elif gpu_memory_gb >= 24:  # A100 24GB
        BATCHSIZE = 500
    elif gpu_memory_gb >= 16:  # V100 16GB
        BATCHSIZE = 300
    else:  # å…¶ä»–GPU
        BATCHSIZE = 150
    print(f"ğŸ–¥ï¸  è®¾ç½®æ‰¹æ¬¡å¤§å°: {BATCHSIZE}")
else:
    BATCHSIZE = 50  # CPUæ¨¡å¼ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
    print("âš ï¸  CPUæ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: 50")

WARMUP_EPOCHS = 50  # ä¸Transformerä¿æŒä¸€è‡´çš„é¢„çƒ­è½®æ•°

# æ·»åŠ æ¢¯åº¦è£å‰ª
MAX_GRAD_NORM = 1.0  # è°ƒæ•´åˆ°æ›´åˆç†çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼
MIN_GRAD_NORM = 0.1  # æœ€å°æ¢¯åº¦èŒƒæ•°é˜ˆå€¼

# A100 80GBå†…å­˜ä¼˜åŒ–å‚æ•°
MEMORY_CHECK_INTERVAL = 25  # æ›´é¢‘ç¹æ£€æŸ¥å†…å­˜ï¼ˆæ¯25ä¸ªæ‰¹æ¬¡ï¼‰
CLEAR_CACHE_INTERVAL = 50   # æ›´é¢‘ç¹æ¸…ç†ç¼“å­˜ï¼ˆæ¯50ä¸ªæ‰¹æ¬¡ï¼‰
MAX_MEMORY_THRESHOLD = 0.85  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%æ—¶é‡‡å–æªæ–½ï¼ˆA100å•å¡ä¿å®ˆç­–ç•¥ï¼‰
EMERGENCY_MEMORY_THRESHOLD = 0.95  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡95%æ—¶ç´§æ€¥å¤„ç†

# å­¦ä¹ ç‡é¢„çƒ­å‡½æ•°
def get_lr(epoch):
    if epoch < WARMUP_EPOCHS:
        return INIT_LR + (MAX_LR - INIT_LR) * epoch / WARMUP_EPOCHS
    return MAX_LR * (0.9 ** (epoch // 50))  # æ¯50ä¸ªepochè¡°å‡åˆ°90%

# å†…å­˜ç›‘æ§å‡½æ•°
def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            usage_ratio = allocated / total
            print(f"   GPU {i}: {allocated:.1f}GB / {cached:.1f}GB / {total:.1f}GB (å·²ç”¨/ç¼“å­˜/æ€»è®¡) - {usage_ratio*100:.1f}%")
            return usage_ratio
    return 0.0

def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("   ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

# æ˜¾ç¤ºä¼˜åŒ–åçš„è®­ç»ƒå‚æ•°
print(f"\nâš™ï¸  BiLSTMè®­ç»ƒå‚æ•°ï¼ˆA100å•å¡ä¼˜åŒ–ç‰ˆæœ¬ï¼‰:")
print(f"   æ‰¹æ¬¡å¤§å°: {BATCHSIZE} (å……åˆ†åˆ©ç”¨A100 80GBæ˜¾å­˜)")
print(f"   è®­ç»ƒè½®æ•°: {EPOCH}")
print(f"   åˆå§‹å­¦ä¹ ç‡: {INIT_LR}")
print(f"   æœ€å¤§å­¦ä¹ ç‡: {MAX_LR}")
print(f"   GPUé…ç½®: å•å¡A100 80GB (GPU2)")
print(f"   æ··åˆç²¾åº¦: å¯ç”¨ (AMP)")
print(f"   å†…å­˜ç›‘æ§: å¯ç”¨ (æ¯{MEMORY_CHECK_INTERVAL}æ‰¹æ¬¡æ£€æŸ¥)")
print(f"   ç¼“å­˜æ¸…ç†: å¯ç”¨ (æ¯{CLEAR_CACHE_INTERVAL}æ‰¹æ¬¡æ¸…ç†)")
print(f"   å†…å­˜é˜ˆå€¼: {MAX_MEMORY_THRESHOLD*100:.0f}% (A100å•å¡ä¿å®ˆç­–ç•¥)")

#----------------------------------------BILSTMè®­ç»ƒï¼ˆè§‚å¯ŸLossä¸‹é™ï¼‰------------------------
print("="*50)
print("é˜¶æ®µ0: BILSTMè®­ç»ƒï¼ˆè§‚å¯ŸLossä¸‹é™æƒ…å†µï¼‰")
print("="*50)

# A100 GPUä¼˜åŒ–å‚æ•°é…ç½®ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
TIME_STEP = 1  # rnn time step
INPUT_SIZE = 7  # rnn input size

# æ˜¾å­˜å’Œå†…å­˜å®‰å…¨ç›‘æ§å‡½æ•°
def get_gpu_memory_info():
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        return allocated, reserved, total
    return 0, 0, 0

def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
    import psutil
    memory = psutil.virtual_memory()
    used_gb = memory.used / 1024**3
    total_gb = memory.total / 1024**3
    return used_gb, total_gb

def safe_batch_size_calculator(model, sample_data, max_batch_size=4096, safety_margin=0.2):
    """å®‰å…¨çš„æ‰¹æ¬¡å¤§å°è®¡ç®—å™¨"""
    print(f"\nğŸ” æ­£åœ¨è®¡ç®—å®‰å…¨çš„æ‰¹æ¬¡å¤§å°...")
    
    # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
    allocated_before, reserved_before, total_gpu = get_gpu_memory_info()
    print(f"   å½“å‰æ˜¾å­˜: {allocated_before:.1f}GB / {total_gpu:.1f}GB")
    
    # äºŒåˆ†æ³•æŸ¥æ‰¾æœ€å¤§å®‰å…¨æ‰¹æ¬¡å¤§å°
    min_batch = 32
    max_batch = max_batch_size
    safe_batch = min_batch
    
    while min_batch <= max_batch:
        test_batch = (min_batch + max_batch) // 2
        try:
            # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
            if len(sample_data.shape) == 3:  # (samples, time, features)
                test_input = sample_data[:test_batch].clone().to(device)
            else:  # å…¶ä»–å½¢çŠ¶
                test_input = sample_data[:test_batch].clone().to(device)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            with torch.no_grad():
                model.eval()
                _ = model(test_input)
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
            allocated_after, _, _ = get_gpu_memory_info()
            memory_usage = allocated_after / total_gpu
            
            if memory_usage < (1.0 - safety_margin):  # å®‰å…¨é˜ˆå€¼
                safe_batch = test_batch
                min_batch = test_batch + 1
                print(f"   âœ… æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜ä½¿ç”¨ {memory_usage*100:.1f}% - å®‰å…¨")
            else:
                max_batch = test_batch - 1
                print(f"   âš ï¸  æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜ä½¿ç”¨ {memory_usage*100:.1f}% - è¶…é™")
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            del test_input
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                max_batch = test_batch - 1
                print(f"   âŒ æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜æº¢å‡º")
                torch.cuda.empty_cache()
            else:
                raise e
        except Exception as e:
            print(f"   âš ï¸  æ‰¹æ¬¡ {test_batch}: æµ‹è¯•å¤±è´¥ - {str(e)}")
            max_batch = test_batch - 1
    
    return safe_batch

# å†…å­˜ç›‘æ§è£…é¥°å™¨
def memory_monitor(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        # è®­ç»ƒå‰æ£€æŸ¥
        gpu_alloc, gpu_reserved, gpu_total = get_gpu_memory_info()
        sys_used, sys_total = get_system_memory_info()
        
        print(f"\nğŸ“Š è®­ç»ƒå‰å†…å­˜çŠ¶æ€:")
        print(f"   GPUæ˜¾å­˜: {gpu_alloc:.1f}GB / {gpu_total:.1f}GB ({gpu_alloc/gpu_total*100:.1f}%)")
        print(f"   ç³»ç»Ÿå†…å­˜: {sys_used:.1f}GB / {sys_total:.1f}GB ({sys_used/sys_total*100:.1f}%)")
        
        # å®‰å…¨æ£€æŸ¥
        if gpu_alloc/gpu_total > 0.85:
            print("âš ï¸  è­¦å‘Š: GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†ç¼“å­˜")
            torch.cuda.empty_cache()
        
        if sys_used/sys_total > 0.90:
            print("âš ï¸  è­¦å‘Š: ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        
        try:
            result = func(*args, **kwargs)
            return result
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("âŒ æ˜¾å­˜ä¸è¶³ï¼Œæ­£åœ¨æ¸…ç†ç¼“å­˜...")
                torch.cuda.empty_cache()
                raise e
            else:
                raise e
        finally:
            # è®­ç»ƒåçŠ¶æ€
            gpu_alloc_after, _, _ = get_gpu_memory_info()
            sys_used_after, _ = get_system_memory_info()
            print(f"\nğŸ“Š è®­ç»ƒåå†…å­˜çŠ¶æ€:")
            print(f"   GPUæ˜¾å­˜: {gpu_alloc_after:.1f}GB / {gpu_total:.1f}GB")
            print(f"   ç³»ç»Ÿå†…å­˜: {sys_used_after:.1f}GB / {sys_total:.1f}GB")
    
    return wrapper

# A100ä¼˜åŒ–å‚æ•°ï¼ˆå¤§è§„æ¨¡BILSTMé€‚é…ç‰ˆ - ç¨³å®šè®­ç»ƒï¼‰
BILSTM_LR = 5e-5  # ä¸Transformerä¿æŒä¸€è‡´çš„å­¦ä¹ ç‡
BILSTM_EPOCH = 800  # ä¸Transformerä¿æŒä¸€è‡´çš„å®Œæ•´è®­ç»ƒè½®æ•°
BILSTM_BATCH_SIZE_TARGET = 512  # ä¸Transformerä¿æŒä¸€è‡´çš„æ‰¹æ¬¡å¤§å°

# å¤§æ¨¡å‹è®­ç»ƒçš„é¢å¤–å‚æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
WARMUP_EPOCHS = 5  # æµ‹è¯•é…ç½®ï¼šå¿«é€Ÿé¢„çƒ­ï¼ˆåŸå€¼ï¼š50ï¼‰
GRADIENT_CLIP = 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
WEIGHT_DECAY = 1e-5  # é™ä½æƒé‡è¡°å‡ï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–

print(f"A100å•å¡GPUä¼˜åŒ–é…ç½®ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰:")
print(f"   æ—¶é—´æ­¥é•¿: {TIME_STEP}")
print(f"   è¾“å…¥ç»´åº¦: {INPUT_SIZE}")
print(f"   å­¦ä¹ ç‡: {BILSTM_LR} (é’ˆå¯¹A100ä¼˜åŒ–)")
print(f"   è®­ç»ƒè½®æ•°: {BILSTM_EPOCH} (å……åˆ†è®­ç»ƒ)")
print(f"   ç›®æ ‡æ‰¹æ¬¡å¤§å°: {BILSTM_BATCH_SIZE_TARGET} (å°†åŠ¨æ€è°ƒæ•´)")

# åŠ è½½æ‰€æœ‰æ ·æœ¬çš„vin_1æ•°æ®è¿›è¡ŒBILSTMè®­ç»ƒ
print(f"\nğŸ”„ åŠ è½½æ‰€æœ‰{len(train_samples)}ä¸ªæ ·æœ¬çš„vin_1æ•°æ®è¿›è¡ŒBILSTMè®­ç»ƒ")

# æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
all_train_X = []
all_train_y = []
valid_samples = []

for idx, sample_id in enumerate(train_samples):
    vin_1_file = os.path.join(data_dir, f'{sample_id}', 'vin_1.pkl')
    if os.path.exists(vin_1_file):
        try:
            with open(vin_1_file, 'rb') as file:
                sample_data = pickle.load(file)
            
            # ä½¿ç”¨æºä»£ç çš„prepare_training_dataå‡½æ•°
            sample_train_X, sample_train_y = prepare_training_data(sample_data, INPUT_SIZE, TIME_STEP, device)
            
            all_train_X.append(sample_train_X)
            all_train_y.append(sample_train_y)
            valid_samples.append(sample_id)
            
            if (idx + 1) % 10 == 0:
                print(f"   âœ“ å·²åŠ è½½ {idx + 1}/{len(train_samples)} ä¸ªæ ·æœ¬")
                
        except Exception as e:
            print(f"   âŒ æ ·æœ¬ {sample_id} åŠ è½½å¤±è´¥: {e}")
            continue
    else:
        print(f"   âš ï¸  æ ·æœ¬ {sample_id} çš„vin_1.pklæ–‡ä»¶ä¸å­˜åœ¨")

if len(all_train_X) > 0:
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
    train_X = torch.cat(all_train_X, dim=0)
    train_y = torch.cat(all_train_y, dim=0)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_samples)} ä¸ªæ ·æœ¬çš„æ•°æ®")
    print(f"   æœ‰æ•ˆæ ·æœ¬ID: {valid_samples}")
    print(f"   åˆå¹¶åè®­ç»ƒæ•°æ®å½¢çŠ¶:")
    print(f"   è¾“å…¥å½¢çŠ¶ (train_X): {train_X.shape}")
    print(f"   ç›®æ ‡å½¢çŠ¶ (train_y): {train_y.shape}")
    print(f"   é¢„æµ‹ç›®æ ‡: ä¸‹ä¸€æ—¶åˆ»ç´¢å¼•5å’Œ6çš„æ•°æ®")
else:
    print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    print("è·³è¿‡BILSTMè®­ç»ƒæ­¥éª¤")

if len(all_train_X) > 0:
    
    # åˆ›å»ºBILSTMæ¨¡å‹ï¼ˆç”¨äºæ‰¹æ¬¡å¤§å°è®¡ç®—ï¼‰
    try:
        bilstm_model = LSTM()
        if cuda_available:
            bilstm_model = bilstm_model.to(device)
        bilstm_model = bilstm_model.double()
        print(f"âœ… BILSTMæ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    except RuntimeError as e:
        if "CUDA" in str(e):
            print(f"âŒ CUDAè®¾å¤‡é”™è¯¯: {e}")
            print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
            cuda_available = False
            bilstm_model = LSTM().to(device).double()
        else:
            raise e
    
    # åº”ç”¨ä¸“é—¨çš„å¤§æ¨¡å‹æƒé‡åˆå§‹åŒ–
    def initialize_bilstm_weights(model):
        """å¤§è§„æ¨¡BILSTMä¸“ç”¨æƒé‡åˆå§‹åŒ–"""
        for name, param in model.named_parameters():
            if 'weight_ih' in name or 'weight_hh' in name:
                # LSTMæƒé‡ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œé™ä½æ–¹å·®
                nn.init.xavier_uniform_(param.data, gain=0.5)
            elif 'bias' in name:
                # åç½®åˆå§‹åŒ–ä¸º0ï¼Œforget gateåç½®è®¾ä¸º1
                nn.init.zeros_(param.data)
                if 'bias_hh' in name:
                    # forget gateåç½®è®¾ä¸º1ï¼Œå¸®åŠ©è®°å¿†
                    n = param.size(0)
                    param.data[n//4:n//2].fill_(1.0)
            elif 'weight' in name and 'fc' in name:
                # å…¨è¿æ¥å±‚æƒé‡
                nn.init.xavier_uniform_(param.data, gain=0.3)
            elif 'bias' in name and 'fc' in name:
                nn.init.zeros_(param.data)
        print("âœ… åº”ç”¨å¤§è§„æ¨¡BILSTMä¸“ç”¨æƒé‡åˆå§‹åŒ–")
    
    initialize_bilstm_weights(bilstm_model)
    
    # ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡
    bilstm_params = bilstm_model.count_parameters()
    print(f"\nğŸ“Š BILSTMæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°é‡: {bilstm_params:,}")
    print(f"   æ¨¡å‹è§„æ¨¡: {bilstm_params/1e6:.2f}M å‚æ•°")
    print(f"   å¯¹æ¯”Transformer: çº¦ {bilstm_params/920000:.2f}x è§„æ¨¡")
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print(f"   æ¶æ„: BiLSTM(input=7, hidden=128, layers=3) + FC(256â†’128â†’64â†’2)")
    print(f"   åŒ¹é…ç›®æ ‡: Transformer(d_model=128, layers=3) â‰ˆ 0.92Må‚æ•°")
    
    # å®‰å…¨è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    safe_batch_size = safe_batch_size_calculator(bilstm_model, train_X, BILSTM_BATCH_SIZE_TARGET, safety_margin=0.2)
    print(f"\nğŸ¯ ç¡®å®šå®‰å…¨æ‰¹æ¬¡å¤§å°: {safe_batch_size}")
    print(f"   åŸç›®æ ‡: {BILSTM_BATCH_SIZE_TARGET}")
    print(f"   å®é™…ä½¿ç”¨: {safe_batch_size}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨å…¨å±€å®‰å…¨é…ç½®ï¼‰
    train_dataset = MyDataset(train_X, train_y)
    
    bilstm_train_loader = DataLoader(
        train_dataset, 
        batch_size=safe_batch_size, 
        shuffle=True,
        num_workers=dataloader_workers,
        pin_memory=pin_memory_enabled,
        persistent_workers=use_persistent,
        prefetch_factor=2 if dataloader_workers > 0 else None
    )
    
    # ä¼˜åŒ–å™¨é…ç½®ï¼ˆå¤§æ¨¡å‹é€‚é…ç‰ˆï¼‰
    # ä¸ºå¤§æ¨¡å‹ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡ç­–ç•¥
    actual_lr = BILSTM_LR  # ä¸å†çº¿æ€§ç¼©æ”¾ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡
    bilstm_optimizer = torch.optim.AdamW(bilstm_model.parameters(), 
                                        lr=actual_lr, 
                                        weight_decay=WEIGHT_DECAY,  # ä½¿ç”¨æ›´ä¿å®ˆçš„æƒé‡è¡°å‡
                                        betas=(0.9, 0.999),  # æ ‡å‡†betaå€¼
                                        eps=1e-8)  # æ•°å€¼ç¨³å®šæ€§
    bilstm_loss_func = nn.MSELoss()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ”¯æŒé¢„çƒ­ï¼‰
    def get_lr_with_warmup(epoch):
        if epoch < WARMUP_EPOCHS:
            # é¢„çƒ­é˜¶æ®µï¼šä»0çº¿æ€§å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡
            return actual_lr * (epoch + 1) / WARMUP_EPOCHS
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            cos_epoch = epoch - WARMUP_EPOCHS
            cos_max = BILSTM_EPOCH - WARMUP_EPOCHS
            return actual_lr * 0.5 * (1 + np.cos(np.pi * cos_epoch / cos_max))
    
    # æ‰‹åŠ¨å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ›´ç²¾ç¡®æ§åˆ¶ï¼‰
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        bilstm_optimizer, 
        lr_lambda=lambda epoch: get_lr_with_warmup(epoch) / actual_lr
    )
    
    print(f"\nğŸš€ A100å¤§è§„æ¨¡BILSTMè®­ç»ƒé…ç½® (ç¨³å®šç‰ˆ):")
    print(f"   æ¨¡å‹è§„æ¨¡: hidden_size=128, num_layers=3 (åŒ¹é…Transformer)")
    print(f"   æ‰¹æ¬¡å¤§å°: {safe_batch_size} (é™ä½ä»¥æé«˜ç¨³å®šæ€§)")
    print(f"   å­¦ä¹ ç‡: {actual_lr:.6f} (å¤§å¹…é™ä½ï¼Œç¡®ä¿ç¨³å®š)")
    print(f"   è®­ç»ƒè½®æ•°: {BILSTM_EPOCH} (å¢åŠ ä»¥è¡¥å¿ä½å­¦ä¹ ç‡)")
    print(f"   é¢„çƒ­è½®æ•°: {WARMUP_EPOCHS} (é•¿é¢„çƒ­æœŸ)")
    print(f"   æ¢¯åº¦è£å‰ª: {GRADIENT_CLIP} (ä¸¥æ ¼æ§åˆ¶)")
    print(f"   æƒé‡è¡°å‡: {WEIGHT_DECAY} (ä¿å®ˆæ­£åˆ™åŒ–)")
    print(f"   ä¼˜åŒ–å™¨: AdamW (æ•°å€¼ç¨³å®šé…ç½®)")
    print(f"   å­¦ä¹ ç‡è°ƒåº¦: é•¿é¢„çƒ­ + CosineAnnealing")
    print(f"   æƒé‡åˆå§‹åŒ–: ä¸“ç”¨å¤§æ¨¡å‹åˆå§‹åŒ–")
    print(f"   æ•°æ®åŠ è½½è¿›ç¨‹: 8")
    
    # å†…å­˜ç›‘æ§çš„BILSTMè®­ç»ƒå‡½æ•°
    @memory_monitor
    def bilstm_training_loop(train_loader):
        print(f"\nğŸ‹ï¸ å¼€å§‹BILSTMè®­ç»ƒ (A100ä¼˜åŒ–ç‰ˆæœ¬)...")
        
        # ğŸš¨ è®­ç»ƒå‰CUDAçŠ¶æ€æ£€æŸ¥
        if device.type == 'cuda':
            try:
                torch.cuda.synchronize()  # ç¡®ä¿CUDAæ“ä½œå®Œæˆ
                print(f"âœ… CUDAè®¾å¤‡çŠ¶æ€æ­£å¸¸: {torch.cuda.get_device_name()}")
                gpu_alloc, _, gpu_total = get_gpu_memory_info()
                print(f"ğŸ”‹ å½“å‰GPUæ˜¾å­˜: {gpu_alloc/gpu_total*100:.1f}%")
            except Exception as e:
                print(f"âš ï¸ CUDAçŠ¶æ€æ£€æŸ¥è­¦å‘Š: {e}")
                print("ğŸ”„ ç»§ç»­ä½¿ç”¨å½“å‰é…ç½®...")
        
        loss_train_100 = []
        bilstm_model.train()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(BILSTM_EPOCH):
            epoch_losses = []
            
            # æ¯ä¸ªepochå¼€å§‹å‰æ£€æŸ¥å†…å­˜
            if epoch % 50 == 0:
                gpu_alloc, _, gpu_total = get_gpu_memory_info()
                print(f"   Epoch {epoch}: GPUæ˜¾å­˜ä½¿ç”¨ {gpu_alloc/gpu_total*100:.1f}%")
            
            # ğŸš¨ DataLoaderæšä¸¾ä¿æŠ¤
            try:
                for step, (b_x, b_y) in enumerate(train_loader):
                    try:
                        # å‰å‘ä¼ æ’­
                        output = bilstm_model(b_x)
                        loss = bilstm_loss_func(b_y, output)
                        
                        # åå‘ä¼ æ’­
                        bilstm_optimizer.zero_grad()
                        loss.backward()
                        
                        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                        torch.nn.utils.clip_grad_norm_(bilstm_model.parameters(), max_norm=GRADIENT_CLIP)
                        
                        bilstm_optimizer.step()
                        
                        # è®°å½•æŸå¤±
                        if step % 20 == 0:  # æ›´é¢‘ç¹çš„è®°å½•
                            loss_train_100.append(loss.cpu().detach().numpy())
                            epoch_losses.append(loss.item())
                        
                        # å®šæœŸæ¸…ç†ç¼“å­˜
                        if step % 100 == 0:
                            torch.cuda.empty_cache()
                            
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            print(f"   âš ï¸  Epoch {epoch}, Step {step}: æ˜¾å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜åç»§ç»­")
                            torch.cuda.empty_cache()
                            continue
                        else:
                            raise e
            except RuntimeError as e:
                print(f"ğŸš¨ DataLoaderé”™è¯¯ (Epoch {epoch}): {e}")
                if "initialization error" in str(e).lower():
                    print("   åŸå› : CUDAåˆå§‹åŒ–å¤±è´¥")
                    print("   ğŸ”„ å°è¯•é‡æ–°åˆ›å»ºDataLoader...")
                    # å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼é‡æ–°åˆ›å»ºDataLoader
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=safe_batch_size, 
                        shuffle=True,
                        num_workers=0,  # å¼ºåˆ¶å•è¿›ç¨‹
                        pin_memory=False,
                        persistent_workers=False
                    )
                    print("   âœ… DataLoaderé‡æ–°åˆ›å»ºå®Œæˆï¼Œç»§ç»­è®­ç»ƒ")
                    # é‡æ–°å°è¯•è¿™ä¸ªepoch
                    epoch -= 1  # é‡æ–°å°è¯•å½“å‰epoch
                    continue
                else:
                    print(f"   âŒ æœªçŸ¥DataLoaderé”™è¯¯: {e}")
                    raise e
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # å®šæœŸè¾“å‡ºè®­ç»ƒçŠ¶æ€
            if epoch % 20 == 0:
                avg_loss = np.mean(epoch_losses) if epoch_losses else 0
                current_lr = scheduler.get_last_lr()[0]
                print(f"   Epoch {epoch:3d}/{BILSTM_EPOCH}: Loss={avg_loss:.6f}, LR={current_lr:.6f}")
        
        print(f"âœ… BILSTMè®­ç»ƒå®Œæˆï¼Œå…±è®°å½• {len(loss_train_100)} ä¸ªLosså€¼")
        return loss_train_100
    
    # æ‰§è¡Œè®­ç»ƒ
    loss_train_100 = bilstm_training_loop(bilstm_train_loader)
    
    # ä¿å­˜BILSTMæ¨¡å‹å’ŒLossè®°å½•ï¼ˆåŸºäºæ‰€æœ‰æ ·æœ¬è®­ç»ƒï¼‰
    bilstm_model_path = os.path.join(save_dir, 'bilstm_model_all_samples.pth')
    bilstm_loss_path = os.path.join(save_dir, 'bilstm_loss_record_all_samples.pkl')
    
    torch.save(bilstm_model.state_dict(), bilstm_model_path)
    with open(bilstm_loss_path, 'wb') as f:
        pickle.dump(loss_train_100, f)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        'valid_samples': valid_samples,
        'total_samples': len(valid_samples),
        'train_data_shape': (train_X.shape, train_y.shape),
        'training_epochs': BILSTM_EPOCH,
        'learning_rate': BILSTM_LR,
        'final_loss': loss_train_100[-1] if loss_train_100 else None
    }
    
    training_info_path = os.path.join(save_dir, 'bilstm_training_info.pkl')
    with open(training_info_path, 'wb') as f:
        pickle.dump(training_info, f)
    
    print(f"âœ… BILSTMæ¨¡å‹å·²ä¿å­˜: {bilstm_model_path}")
    print(f"âœ… Lossè®°å½•å·²ä¿å­˜: {bilstm_loss_path}")
    print(f"âœ… è®­ç»ƒä¿¡æ¯å·²ä¿å­˜: {training_info_path}")
    print(f"âœ… æ¨¡å‹åŸºäº {len(valid_samples)} ä¸ªæ ·æœ¬è®­ç»ƒå®Œæˆ")

else:
    print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    print("è·³è¿‡BILSTMè®­ç»ƒæ­¥éª¤")

print("\n" + "="*50)

#----------------------------------------MC-AEè®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼‰------------------------
print("="*50)
print("é˜¶æ®µ1: å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
def check_data_quality(data, name, sample_id=None):
    """è¯¦ç»†çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
    prefix = f"æ ·æœ¬ {sample_id} - " if sample_id else ""
    print(f"\nğŸ” {prefix}{name} æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # æ•°å€¼ç»Ÿè®¡
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    print(f"   æ•°å€¼èŒƒå›´: [{data_np.min():.6f}, {data_np.max():.6f}]")
    print(f"   å‡å€¼: {data_np.mean():.6f}")
    print(f"   æ ‡å‡†å·®: {data_np.std():.6f}")
    print(f"   ä¸­ä½æ•°: {np.median(data_np):.6f}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥
    nan_count = np.isnan(data_np).sum()
    inf_count = np.isinf(data_np).sum()
    zero_count = (data_np == 0).sum()
    negative_count = (data_np < 0).sum()
    
    print(f"   NaNæ•°é‡: {nan_count}")
    print(f"   Infæ•°é‡: {inf_count}")
    print(f"   é›¶å€¼æ•°é‡: {zero_count}")
    print(f"   è´Ÿå€¼æ•°é‡: {negative_count}")
    
    # å¼‚å¸¸å€¼æ¯”ä¾‹
    total_elements = data_np.size
    print(f"   NaNæ¯”ä¾‹: {nan_count/total_elements*100:.2f}%")
    print(f"   Infæ¯”ä¾‹: {inf_count/total_elements*100:.2f}%")
    print(f"   é›¶å€¼æ¯”ä¾‹: {zero_count/total_elements*100:.2f}%")
    print(f"   è´Ÿå€¼æ¯”ä¾‹: {negative_count/total_elements*100:.2f}%")
    
    # å¼‚å¸¸å€¼è­¦å‘Š
    if nan_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°NaNå€¼ï¼")
    if inf_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼")
    if data_np.min() < -1e6 or data_np.max() > 1e6:
        print(f"   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¤§å€¼ï¼èŒƒå›´: [{data_np.min():.2e}, {data_np.max():.2e}]")
    
    return {
        'has_nan': nan_count > 0,
        'has_inf': inf_count > 0,
        'has_extreme_values': data_np.min() < -1e6 or data_np.max() > 1e6,
        'data_type': data.dtype,
        'shape': data.shape
    }

def physics_based_data_processing(data, name, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆå‚è€ƒè®ºæ–‡æ–¹æ³•ï¼‰"""
    print(f"\nğŸ”§ åŸºäºç‰©ç†çº¦æŸå¤„ç† {name}...")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç±»å‹
    if isinstance(data, np.ndarray):
        print(f"   åŸå§‹ç±»å‹: numpy.ndarray, dtype={data.dtype}")
    elif isinstance(data, torch.Tensor):
        print(f"   åŸå§‹ç±»å‹: torch.Tensor, dtype={data.dtype}")
    else:
        print(f"   åŸå§‹ç±»å‹: {type(data)}")
    
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    print(f"   åŸå§‹æ•°æ®ç‚¹æ•°é‡: {original_data_points}")
    
    print("   æ‰§è¡ŒåŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†...")
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤1: å¤„ç†ç¼ºå¤±æ•°æ®...")
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        print(f"     æ£€æµ‹åˆ° {complete_nan_rows.sum()} è¡Œå®Œå…¨ç¼ºå¤±çš„æ•°æ®")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡ç¼ºå¤±æ•°æ®å¤„ç†")
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
                print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢å…¨NaNè¡Œ")
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
                print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢")
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    print("   æ­¥éª¤2: å¤„ç†å¼‚å¸¸æ•°æ®...")
    
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        print(f"     å¤„ç†vin_2æ•°æ®ï¼ˆ225åˆ—ï¼‰")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºç”µå‹èŒƒå›´[0,5]Vçš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
            else:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: ç”µå‹å€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹ç›¸å…³åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºç”µå‹èŒƒå›´[-5,5]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
            else:
                print(f"       ç”µå‹ç›¸å…³åˆ—{col}: ç”µå‹å€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        print(f"     å¤„ç†vin_3æ•°æ®ï¼ˆ226åˆ—ï¼‰ï¼Œç¬¬224åˆ—ä¸ºç‰¹æ®Šä¿ç•™åˆ—")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       SOCé¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
            else:
                print(f"       SOCé¢„æµ‹åˆ—{col}: SOCå€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       å•ä½“SOCåˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       SOCåå·®åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        special_col = 224
        print(f"       ç‰¹æ®Šä¿ç•™åˆ—{special_col}: ä¿æŒåŸå€¼ä¸å˜")
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
            
    elif feature_type == 'current':
        # ç”µæµç‰©ç†çº¦æŸï¼š-100Aåˆ°100A
        valid_mask = (data_np >= -100) & (data_np <= 100)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-100,100]Açš„å¼‚å¸¸å€¼")
            data_np[data_np < -100] = -100
            data_np[data_np > 100] = 100
            
    elif feature_type == 'temperature':
        # æ¸©åº¦ç‰©ç†çº¦æŸï¼š-40Â°Cåˆ°80Â°C
        valid_mask = (data_np >= -40) & (data_np <= 80)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np < -40] = -40
            data_np[data_np > 80] = 80
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤3: å¤„ç†é‡‡æ ·æ•…éšœ...")
    
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        print(f"     æ£€æµ‹åˆ° {fault_mask.sum()} ä¸ªé‡‡æ ·æ•…éšœç‚¹")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢æ•…éšœç‚¹ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡é‡‡æ ·æ•…éšœå¤„ç†")
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢ {col_fault_mask.sum()} ä¸ªæ•…éšœå€¼")
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢")
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    print("   æ­¥éª¤4: æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        print(f"     âš ï¸  ä»æœ‰ {final_nan_count} ä¸ªNaNå’Œ {final_inf_count} ä¸ªInfå€¼")
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    else:
        print("     âœ… æ‰€æœ‰å¼‚å¸¸å€¼å·²å¤„ç†å®Œæˆ")
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    # æ£€æŸ¥æ•°æ®ç‚¹æ•°é‡æ˜¯å¦ä¿æŒä¸€è‡´
    final_data_points = data_tensor.shape[0]
    if final_data_points == original_data_points:
        print(f"   å¤„ç†å®Œæˆ: {data_tensor.shape}, dtype={data_tensor.dtype}")
        print(f"   âœ… æ•°æ®ç‚¹æ•°é‡ä¿æŒä¸€è‡´: {original_data_points} -> {final_data_points}")
    else:
        print(f"   âš ï¸  æ•°æ®ç‚¹æ•°é‡å‘ç”Ÿå˜åŒ–: {original_data_points} -> {final_data_points}")
    
    return data_tensor

def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        # ä¸éœ€è¦å¤„ç†ï¼Œä¿æŒåŸå€¼
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    return data_tensor

# ä¸­æ–‡æ³¨é‡Šï¼šåŠ è½½MC-AEæ¨¡å‹è¾“å…¥ç‰¹å¾ï¼ˆvin_2.pklå’Œvin_3.pklï¼‰
# åˆå¹¶æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„vin_2å’Œvin_3æ•°æ®
all_vin2_data = []
all_vin3_data = []

# æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
sample_summary = {
    'total_samples': len(train_samples),
    'processed_samples': 0,
    'error_samples': 0,
    'total_vin2_issues_fixed': 0,
    'total_vin3_issues_fixed': 0
}

print("="*60)
print("ğŸ“¥ å¼€å§‹æ•°æ®åŠ è½½å’Œè´¨é‡æ£€æŸ¥")
print("="*60)

for sample_id in train_samples:
    vin2_path = os.path.join(data_dir, str(sample_id), 'vin_2.pkl')
    vin3_path = os.path.join(data_dir, str(sample_id), 'vin_3.pkl')
    
    # åŠ è½½åŸå§‹vin_2æ•°æ®
    try:
        with open(vin2_path, 'rb') as file:
            vin2_data = pickle.load(file)
        
        # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        vin2_tensor = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ ·æœ¬ {sample_id} çš„vin_2æ•°æ®å¤±è´¥: {e}")
        sample_summary['error_samples'] += 1
        continue
    
    # åŠ è½½åŸå§‹vin_3æ•°æ®
    try:
        with open(vin3_path, 'rb') as file:
            vin3_data = pickle.load(file)
        
        # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        vin3_tensor = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ ·æœ¬ {sample_id} çš„vin_3æ•°æ®å¤±è´¥: {e}")
        sample_summary['error_samples'] += 1
        continue
    
    # æ·»åŠ åˆ°åˆ—è¡¨
    all_vin2_data.append(vin2_tensor)
    all_vin3_data.append(vin3_tensor)
    sample_summary['processed_samples'] += 1
    
    # æ¯å¤„ç†10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
    if sample_summary['processed_samples'] % 10 == 0:
        print(f"ğŸ“Š å·²å¤„ç† {sample_summary['processed_samples']}/{sample_summary['total_samples']} ä¸ªæ ·æœ¬")

# è¾“å‡ºå¤„ç†æ±‡æ€»ä¿¡æ¯
print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
print(f"   æ€»æ ·æœ¬æ•°: {sample_summary['total_samples']}")
print(f"   æˆåŠŸå¤„ç†: {sample_summary['processed_samples']}")
print(f"   å¤„ç†å¤±è´¥: {sample_summary['error_samples']}")
print(f"   æˆåŠŸç‡: {sample_summary['processed_samples']/sample_summary['total_samples']*100:.1f}%")

# åˆå¹¶æ•°æ®
print("\n" + "="*60)
print("ğŸ”— åˆå¹¶æ‰€æœ‰æ ·æœ¬æ•°æ®")
print("="*60)

combined_tensor = torch.cat(all_vin2_data, dim=0)
combined_tensorx = torch.cat(all_vin3_data, dim=0)

print(f"åˆå¹¶åvin_2æ•°æ®å½¢çŠ¶: {combined_tensor.shape}")
print(f"åˆå¹¶åvin_3æ•°æ®å½¢çŠ¶: {combined_tensorx.shape}")

# ç®€è¦æ£€æŸ¥åˆå¹¶åçš„æ•°æ®è´¨é‡
print("\nğŸ” åˆå¹¶åæ•°æ®è´¨é‡ç®€è¦æ£€æŸ¥:")
vin2_nan_count = torch.isnan(combined_tensor).sum().item()
vin2_inf_count = torch.isinf(combined_tensor).sum().item()
vin3_nan_count = torch.isnan(combined_tensorx).sum().item()
vin3_inf_count = torch.isinf(combined_tensorx).sum().item()

print(f"   vin_2: NaN={vin2_nan_count}, Inf={vin2_inf_count}")
print(f"   vin_3: NaN={vin3_nan_count}, Inf={vin3_inf_count}")

# æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼éœ€è¦å¤„ç†
vin2_has_issues = (vin2_nan_count > 0 or vin2_inf_count > 0)
vin3_has_issues = (vin3_nan_count > 0 or vin3_inf_count > 0)

if vin2_has_issues or vin3_has_issues:
    print("âš ï¸  æ£€æµ‹åˆ°æ•°æ®é—®é¢˜ï¼Œè¿›è¡Œä¿®å¤...")
    
    # ä¿®å¤NaNå’ŒInfå€¼
    if vin2_has_issues:
        combined_tensor = torch.where(torch.isnan(combined_tensor) | torch.isinf(combined_tensor), 
                                     torch.zeros_like(combined_tensor), combined_tensor)
        print("   âœ… vin_2æ•°æ®ä¿®å¤å®Œæˆ")
    
    if vin3_has_issues:
        combined_tensorx = torch.where(torch.isnan(combined_tensorx) | torch.isinf(combined_tensorx), 
                                      torch.zeros_like(combined_tensorx), combined_tensorx)
        print("   âœ… vin_3æ•°æ®ä¿®å¤å®Œæˆ")
else:
    print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®å¤")

#----------------------------------------MC-AEå¤šé€šé“è‡ªç¼–ç å™¨è®­ç»ƒ--------------------------
print("="*50)
print("é˜¶æ®µ2: è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# ä¸­æ–‡æ³¨é‡Šï¼šå®šä¹‰ç‰¹å¾åˆ‡ç‰‡ç»´åº¦
# vin_2.pkl
dim_x = 2
dim_y = 110
dim_z = 110
dim_q = 3

# ä¸­æ–‡æ³¨é‡Šï¼šåˆ†å‰²ç‰¹å¾å¼ é‡
x_recovered = combined_tensor[:, :dim_x]
y_recovered = combined_tensor[:, dim_x:dim_x + dim_y]
z_recovered = combined_tensor[:, dim_x + dim_y: dim_x + dim_y + dim_z]
q_recovered = combined_tensor[:, dim_x + dim_y + dim_z:]

# vin_3.pkl
dim_x2 = 2
dim_y2 = 110
dim_z2 = 110
dim_q2= 4

x_recovered2 = combined_tensorx[:, :dim_x2]
y_recovered2 = combined_tensorx[:, dim_x2:dim_x2 + dim_y2]
z_recovered2 = combined_tensorx[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
q_recovered2 = combined_tensorx[:, dim_x2 + dim_y2 + dim_z2:]

# è®­ç»ƒè¶…å‚æ•°é…ç½®ï¼ˆå·²åœ¨å‰é¢å®šä¹‰ï¼‰

# ç”¨äºè®°å½•è®­ç»ƒæŸå¤±
train_losses_mcae1 = []
train_losses_mcae2 = []

# ä¸­æ–‡æ³¨é‡Šï¼šè‡ªå®šä¹‰å¤šè¾“å…¥æ•°æ®é›†ç±»ï¼ˆæœ¬åœ°å®šä¹‰ï¼ŒéClass_.pyä¸­çš„Datasetï¼‰
class Dataset(Dataset):
    def __init__(self, x, y, z, q):
        self.x = x.to(torch.float32)
        self.y = y.to(torch.float32)
        self.z = z.to(torch.float32)
        self.q = q.to(torch.float32)
    def __len__(self):
        return len(self.x)
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.z[idx], self.q[idx]

# A100å®‰å…¨æ‰¹æ¬¡å¤§å°è®¡ç®—ï¼ˆMC-AE1ï¼‰
print(f"\nğŸ” MC-AE1: è®¡ç®—å®‰å…¨æ‰¹æ¬¡å¤§å°...")
print(f"   åŸè®¾å®šæ‰¹æ¬¡å¤§å°: {BATCHSIZE}")

# åˆ›å»ºä¸´æ—¶æ¨¡å‹ç”¨äºæ‰¹æ¬¡å¤§å°æµ‹è¯•
temp_net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, activation_fn=custom_activation, use_dx_in_forward=True).to(device).to(torch.float32)

# åˆ›å»ºæµ‹è¯•æ•°æ®æ ·æœ¬
test_sample_size = min(BATCHSIZE, len(x_recovered))
sample_x = x_recovered[:test_sample_size]
sample_y = y_recovered[:test_sample_size] 
sample_z = z_recovered[:test_sample_size]
sample_q = q_recovered[:test_sample_size]

# ä½¿ç”¨å®‰å…¨æ‰¹æ¬¡å¤§å°è®¡ç®—å™¨ï¼ˆä¿®æ”¹ç‰ˆæœ¬é€‚é…MC-AEï¼‰
def safe_mcae_batch_calculator(model, x, y, z, q, max_batch_size, safety_margin=0.2):
    """MC-AEä¸“ç”¨å®‰å…¨æ‰¹æ¬¡å¤§å°è®¡ç®—å™¨"""
    print(f"   æ­£åœ¨æµ‹è¯•MC-AEæ‰¹æ¬¡å¤§å°...")
    
    allocated_before, _, total_gpu = get_gpu_memory_info()
    print(f"   å½“å‰æ˜¾å­˜: {allocated_before:.1f}GB / {total_gpu:.1f}GB")
    
    min_batch = 32
    max_batch = min(max_batch_size, len(x))
    safe_batch = min_batch
    
    while min_batch <= max_batch:
        test_batch = (min_batch + max_batch) // 2
        try:
            # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
            test_x = x[:test_batch].to(device)
            test_y = y[:test_batch].to(device)
            test_z = z[:test_batch].to(device)
            test_q = q[:test_batch].to(device)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            with torch.no_grad():
                model.eval()
                _, _ = model(test_x, test_z, test_q)
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
            allocated_after, _, _ = get_gpu_memory_info()
            memory_usage = allocated_after / total_gpu
            
            if memory_usage < (1.0 - safety_margin):
                safe_batch = test_batch
                min_batch = test_batch + 1
                print(f"   âœ… æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜ {memory_usage*100:.1f}% - å®‰å…¨")
            else:
                max_batch = test_batch - 1
                print(f"   âš ï¸  æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜ {memory_usage*100:.1f}% - è¶…é™")
            
            # æ¸…ç†
            del test_x, test_y, test_z, test_q
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                max_batch = test_batch - 1
                print(f"   âŒ æ‰¹æ¬¡ {test_batch}: æ˜¾å­˜æº¢å‡º")
                torch.cuda.empty_cache()
            else:
                raise e
    
    return safe_batch

# è®¡ç®—MC-AE1çš„å®‰å…¨æ‰¹æ¬¡å¤§å°
safe_mcae1_batch = safe_mcae_batch_calculator(temp_net, sample_x, sample_y, sample_z, sample_q, BATCHSIZE, safety_margin=0.25)
print(f"ğŸ¯ MC-AE1 å®‰å…¨æ‰¹æ¬¡å¤§å°: {safe_mcae1_batch}")

# æ¸…ç†ä¸´æ—¶æ¨¡å‹
del temp_net
torch.cuda.empty_cache()

# ä¸­æ–‡æ³¨é‡Šï¼šç”¨DataLoaderæ‰¹é‡åŠ è½½å¤šé€šé“ç‰¹å¾æ•°æ®ï¼ˆå®‰å…¨é…ç½®ï¼‰
train_loader_u = DataLoader(Dataset(x_recovered, y_recovered, z_recovered, q_recovered), 
                           batch_size=safe_mcae1_batch, shuffle=False, 
                           num_workers=dataloader_workers, pin_memory=pin_memory_enabled)

# ä¸­æ–‡æ³¨é‡Šï¼šåˆå§‹åŒ–MC-AEæ¨¡å‹ï¼ˆä½¿ç”¨float32ï¼‰
try:
    net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, activation_fn=custom_activation, use_dx_in_forward=True)
    if cuda_available:
        net = net.to(device)
    net = net.to(torch.float32)
    print(f"âœ… MC-AE1æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    
    netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, activation_fn=torch.sigmoid, use_dx_in_forward=True)
    if cuda_available:
        netx = netx.to(device)
    netx = netx.to(torch.float32)
    print(f"âœ… MC-AE2æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    
except RuntimeError as e:
    if "CUDA" in str(e):
        print(f"âŒ MC-AEæ¨¡å‹CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼")
        device = torch.device('cpu')
        cuda_available = False
        
        # é‡æ–°åˆ›å»ºæ¨¡å‹åˆ°CPU
        net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, activation_fn=custom_activation, use_dx_in_forward=True).to(device).to(torch.float32)
        netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device).to(torch.float32)
        print("âœ… MC-AEæ¨¡å‹å·²ç§»åŠ¨åˆ°CPUè®¾å¤‡")
    else:
        raise e

# ä½¿ç”¨æ›´ç¨³å®šçš„æƒé‡åˆå§‹åŒ–
def stable_weight_init(model):
    """ä½¿ç”¨æ›´ç¨³å®šçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
    for module in model.modules():
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œä½†é™åˆ¶æƒé‡èŒƒå›´
            nn.init.xavier_uniform_(module.weight, gain=0.3)  # é™ä½gainå€¼é¿å…æ¢¯åº¦çˆ†ç‚¸
            if module.bias is not None:
                nn.init.zeros_(module.bias)

# åº”ç”¨ç¨³å®šçš„æƒé‡åˆå§‹åŒ–
stable_weight_init(net)
stable_weight_init(netx)
print("âœ… åº”ç”¨ç¨³å®šçš„æƒé‡åˆå§‹åŒ–")

# A100å•å¡ä¼˜åŒ–é…ç½®
print("âœ… ä½¿ç”¨å•å¡A100ä¼˜åŒ–æ¨¡å¼")
print(f"   GPUè®¾å¤‡: {device}")
print(f"   æ˜¾å­˜ä¼˜åŒ–: é’ˆå¯¹80GBæ˜¾å­˜ç‰¹åˆ«ä¼˜åŒ–")

optimizer = torch.optim.Adam(net.parameters(), lr=INIT_LR)
l1_lambda = 0.01
loss_f = nn.MSELoss()

# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
scaler = torch.cuda.amp.GradScaler()
print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    
    # æ›´æ–°å­¦ä¹ ç‡
    current_lr = get_lr(epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr
    
    # æ¯ä¸ªepochå¼€å§‹æ—¶æ¸…ç†ç¼“å­˜
    clear_gpu_cache()
    
        for iteration, (x, y, z, q) in enumerate(train_loader_u):
            x = x.to(device)
            y = y.to(device)
            z = z.to(device)
            q = q.to(device)
            
            # å†…å­˜ç›‘æ§ - å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if iteration % MEMORY_CHECK_INTERVAL == 0:
                memory_usage = check_gpu_memory()
                if memory_usage > EMERGENCY_MEMORY_THRESHOLD:
                    print(f"ğŸš¨  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({memory_usage*100:.1f}%)ï¼Œç´§æ€¥æ¸…ç†ç¼“å­˜...")
                    clear_gpu_cache()
                    torch.cuda.synchronize()  # å¼ºåˆ¶åŒæ­¥
                    if memory_usage > 0.98:  # å¦‚æœä»ç„¶è¿‡é«˜ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡
                        print(f"ğŸš¨  å†…å­˜ä½¿ç”¨ç‡ä»ç„¶è¿‡é«˜ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                elif memory_usage > MAX_MEMORY_THRESHOLD:
                    print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({memory_usage*100:.1f}%)ï¼Œæ¸…ç†ç¼“å­˜...")
                    clear_gpu_cache()
            
            # å®šæœŸæ¸…ç†ç¼“å­˜
            if iteration % CLEAR_CACHE_INTERVAL == 0:
                clear_gpu_cache()
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´
            if torch.isnan(x).any() or torch.isinf(x).any() or torch.isnan(y).any() or torch.isinf(y).any():
                print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®åŒ…å«NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†
            if x.abs().max() > 1000 or y.abs().max() > 1000:
                print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
                print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
                continue
            
            # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¸¦CUDAé”™è¯¯å¤„ç†ï¼‰
            try:
                with torch.cuda.amp.autocast():
                    recon_im, recon_p = net(x, z, q)
                    loss_u = loss_f(y, recon_im)
            except RuntimeError as e:
                if "CUDA" in str(e):
                    print(f"ğŸš¨ CUDAè¿è¡Œæ—¶é”™è¯¯: {e}")
                    print("å°è¯•æ¸…ç†GPUç¼“å­˜å¹¶ç»§ç»­...")
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    continue
                else:
                    raise e
                    
            # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦ä¸ºNaN
            if torch.isnan(loss_u) or torch.isinf(loss_u):
                print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ£€æµ‹åˆ°NaN/InfæŸå¤±å€¼")
                print(f"è¾“å…¥èŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
                print(f"è¾“å‡ºèŒƒå›´: [{recon_im.min():.4f}, {recon_im.max():.4f}]")
                print(f"æŸå¤±å€¼: {loss_u.item()}")
                print("è·³è¿‡æ­¤æ‰¹æ¬¡ï¼Œä¸è¿›è¡Œåå‘ä¼ æ’­")
                continue
            
            total_loss += loss_u.item()
            num_batches += 1
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            scaler.scale(loss_u).backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºNaNæˆ–æ— ç©·å¤§
            grad_norm = 0
            has_grad_issue = False
            
            # å®‰å…¨åœ°å¤„ç†æ¢¯åº¦
            try:
                # åœ¨æ£€æŸ¥æ¢¯åº¦å‰unscale
                scaler.unscale_(optimizer)
                
                for name, param in net.named_parameters():
                    if param.grad is not None:
                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                            print(f"è­¦å‘Šï¼šå‚æ•° {name} çš„æ¢¯åº¦å‡ºç°NaNæˆ–æ— ç©·å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                            has_grad_issue = True
                            break
                        grad_norm += param.grad.data.norm(2).item() ** 2
                
                if has_grad_issue:
                    # é‡ç½®scalerçŠ¶æ€
                    scaler.update()
                    continue
                
                grad_norm = grad_norm ** 0.5
                
                # æ¸è¿›å¼æ¢¯åº¦è£å‰ª - åªæ˜¾ç¤ºå¼‚å¸¸æƒ…å†µ
                if grad_norm > MAX_GRAD_NORM:
                    torch.nn.utils.clip_grad_norm_(net.parameters(), MAX_GRAD_NORM)
                    print(f"âš ï¸  æ¢¯åº¦è£å‰ª: {grad_norm:.4f} -> {MAX_GRAD_NORM}")
                elif grad_norm < MIN_GRAD_NORM:
                    print(f"âš ï¸  æ¢¯åº¦è¿‡å°: {grad_norm:.4f} < {MIN_GRAD_NORM}")
                
                # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
                scaler.step(optimizer)
                scaler.update()
                
            except Exception as e:
                print(f"ä¼˜åŒ–å™¨æ­¥éª¤å¤±è´¥: {e}")
                print("è·³è¿‡æ­¤æ‰¹æ¬¡å¹¶é‡ç½®scalerçŠ¶æ€")
                # é‡ç½®scalerçŠ¶æ€
                scaler.update()
                continue
            
            # åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
            del x, y, z, q, recon_im, recon_p, loss_u
    
    avg_loss = total_loss / num_batches
    train_losses_mcae1.append(avg_loss)
    if epoch % 50 == 0:
        print('MC-AE1 Epoch: {:2d} | Average Loss: {:.6f}'.format(epoch, avg_loss))

# ä¸­æ–‡æ³¨é‡Šï¼šå…¨é‡æ¨ç†ï¼Œè·å¾—é‡æ„è¯¯å·®
train_loader2 = DataLoader(Dataset(x_recovered, y_recovered, z_recovered, q_recovered), batch_size=len(x_recovered), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loader2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    with torch.cuda.amp.autocast():
        recon_imtest, recon = net(x, z, q)
AA = recon_imtest.cpu().detach().numpy()
yTrainU = y_recovered.cpu().detach().numpy()
ERRORU = AA - yTrainU

# A100å®‰å…¨æ‰¹æ¬¡å¤§å°è®¡ç®—ï¼ˆMC-AE2ï¼‰
print(f"\nğŸ” MC-AE2: è®¡ç®—å®‰å…¨æ‰¹æ¬¡å¤§å°...")

# è®¡ç®—MC-AE2çš„å®‰å…¨æ‰¹æ¬¡å¤§å°
test_sample_size2 = min(BATCHSIZE, len(x_recovered2))
sample_x2 = x_recovered2[:test_sample_size2]
sample_y2 = y_recovered2[:test_sample_size2]
sample_z2 = z_recovered2[:test_sample_size2]
sample_q2 = q_recovered2[:test_sample_size2]

safe_mcae2_batch = safe_mcae_batch_calculator(netx, sample_x2, sample_y2, sample_z2, sample_q2, BATCHSIZE, safety_margin=0.25)
print(f"ğŸ¯ MC-AE2 å®‰å…¨æ‰¹æ¬¡å¤§å°: {safe_mcae2_batch}")

# ä¸­æ–‡æ³¨é‡Šï¼šç¬¬äºŒç»„ç‰¹å¾çš„MC-AEè®­ç»ƒï¼ˆå®‰å…¨é…ç½®ï¼‰
train_loader_soc = DataLoader(Dataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), 
                             batch_size=safe_mcae2_batch, shuffle=False,
                             num_workers=dataloader_workers, pin_memory=pin_memory_enabled)
optimizer = torch.optim.Adam(netx.parameters(), lr=INIT_LR)
loss_f = nn.MSELoss()

# ä¸ºç¬¬äºŒä¸ªæ¨¡å‹åˆ›å»ºæ–°çš„scaler
scaler2 = torch.cuda.amp.GradScaler()

avg_loss_list_x = []
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    
    # æ›´æ–°å­¦ä¹ ç‡
    current_lr = get_lr(epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr
    
    # æ¯ä¸ªepochå¼€å§‹æ—¶æ¸…ç†ç¼“å­˜
    clear_gpu_cache()
    
    for iteration, (x, y, z, q) in enumerate(train_loader_soc):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        
        # å†…å­˜ç›‘æ§ - å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        if iteration % MEMORY_CHECK_INTERVAL == 0:
            memory_usage = check_gpu_memory()
            if memory_usage > EMERGENCY_MEMORY_THRESHOLD:
                print(f"ğŸš¨  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({memory_usage*100:.1f}%)ï¼Œç´§æ€¥æ¸…ç†ç¼“å­˜...")
                clear_gpu_cache()
                torch.cuda.synchronize()  # å¼ºåˆ¶åŒæ­¥
                if memory_usage > 0.98:  # å¦‚æœä»ç„¶è¿‡é«˜ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡
                    print(f"ğŸš¨  å†…å­˜ä½¿ç”¨ç‡ä»ç„¶è¿‡é«˜ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
            elif memory_usage > MAX_MEMORY_THRESHOLD:
                print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({memory_usage*100:.1f}%)ï¼Œæ¸…ç†ç¼“å­˜...")
                clear_gpu_cache()
        
        # å®šæœŸæ¸…ç†ç¼“å­˜
        if iteration % CLEAR_CACHE_INTERVAL == 0:
            clear_gpu_cache()
        
        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        with torch.cuda.amp.autocast():
            recon_im, z = netx(x, z, q)
            loss_x = loss_f(y, recon_im)
            
        # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦ä¸ºNaN
        if torch.isnan(loss_x) or torch.isinf(loss_x):
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ£€æµ‹åˆ°NaN/InfæŸå¤±å€¼")
            print(f"è¾“å…¥èŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"è¾“å‡ºèŒƒå›´: [{recon_im.min():.4f}, {recon_im.max():.4f}]")
            print(f"æŸå¤±å€¼: {loss_x.item()}")
            print("è·³è¿‡æ­¤æ‰¹æ¬¡ï¼Œä¸è¿›è¡Œåå‘ä¼ æ’­")
            continue
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†
        if x.abs().max() > 1000 or y.abs().max() > 1000:
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
            continue
        
        total_loss += loss_x.item()
        num_batches += 1
        optimizer.zero_grad()
        scaler2.scale(loss_x).backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºNaNæˆ–æ— ç©·å¤§
        grad_norm = 0
        has_grad_issue = False
        
        # å®‰å…¨åœ°å¤„ç†æ¢¯åº¦
        try:
            # åœ¨æ£€æŸ¥æ¢¯åº¦å‰unscale
            scaler2.unscale_(optimizer)
            
            for name, param in netx.named_parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        print(f"è­¦å‘Šï¼šå‚æ•° {name} çš„æ¢¯åº¦å‡ºç°NaNæˆ–æ— ç©·å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        has_grad_issue = True
                        break
                    grad_norm += param.grad.data.norm(2).item() ** 2
            
            if has_grad_issue:
                # é‡ç½®scalerçŠ¶æ€
                scaler2.update()
                continue
            
            grad_norm = grad_norm ** 0.5
            
            # æ¸è¿›å¼æ¢¯åº¦è£å‰ª - åªæ˜¾ç¤ºå¼‚å¸¸æƒ…å†µ
            if grad_norm > MAX_GRAD_NORM:
                torch.nn.utils.clip_grad_norm_(netx.parameters(), MAX_GRAD_NORM)
                print(f"âš ï¸  æ¢¯åº¦è£å‰ª: {grad_norm:.4f} -> {MAX_GRAD_NORM}")
            elif grad_norm < MIN_GRAD_NORM:
                print(f"âš ï¸  æ¢¯åº¦è¿‡å°: {grad_norm:.4f} < {MIN_GRAD_NORM}")
            
            # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
            scaler2.step(optimizer)
            scaler2.update()
            
        except Exception as e:
            print(f"ä¼˜åŒ–å™¨æ­¥éª¤å¤±è´¥: {e}")
            print("è·³è¿‡æ­¤æ‰¹æ¬¡å¹¶é‡ç½®scalerçŠ¶æ€")
            # é‡ç½®scalerçŠ¶æ€
            scaler2.update()
            continue
        
        # åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
        del x, y, z, q, recon_im, loss_x
    
    avg_loss = total_loss / num_batches
    avg_loss_list_x.append(avg_loss)
    train_losses_mcae2.append(avg_loss)
    if epoch % 50 == 0:
        print('MC-AE2 Epoch: {:2d} | Average Loss: {:.6f}'.format(epoch, avg_loss))

train_loaderx2 = DataLoader(Dataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), batch_size=len(x_recovered2), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loaderx2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    with torch.cuda.amp.autocast():
        recon_imtestx, z = netx(x, z, q)

BB = recon_imtestx.cpu().detach().numpy()
yTrainX = y_recovered2.cpu().detach().numpy()
ERRORX = BB - yTrainX

# ä½¿ç”¨ç»Ÿä¸€çš„ä¿å­˜ç›®å½•
result_dir = save_dir
print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {result_dir}")

# ä¸­æ–‡æ³¨é‡Šï¼šè¯Šæ–­ç‰¹å¾æå–ä¸PCAåˆ†æ
df_data = DiagnosisFeature(ERRORU,ERRORX)

v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = PCA(df_data,0.95,0.95)

# è®­ç»ƒç»“æŸåè‡ªåŠ¨ä¿å­˜æ¨¡å‹å’Œåˆ†æç»“æœ
print("="*50)
print("ä¿å­˜BiLSTMåŸºå‡†è®­ç»ƒç»“æœ")
print("="*50)

# ç»˜åˆ¶è®­ç»ƒç»“æœ
print("ğŸ“ˆ ç»˜åˆ¶BiLSTMè®­ç»ƒæ›²çº¿...")

# åˆ›å»ºå›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# å­å›¾1: MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿
ax1 = axes[0, 0]
epochs = range(1, len(train_losses_mcae1) + 1)
ax1.plot(epochs, train_losses_mcae1, 'b-', linewidth=2, label='MC-AE1 Training Loss')
ax1.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
ax1.set_ylabel('MSE Loss / MSEæŸå¤±')
ax1.set_title('MC-AE1 Training Loss / MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿')
ax1.grid(True, alpha=0.3)
ax1.legend()
ax1.set_yscale('log')

# å­å›¾2: MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿ 
ax2 = axes[0, 1]
ax2.plot(epochs, train_losses_mcae2, 'r-', linewidth=2, label='MC-AE2 Training Loss')
ax2.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
ax2.set_ylabel('MSE Loss / MSEæŸå¤±')
ax2.set_title('MC-AE2 Training Loss / MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿')
ax2.grid(True, alpha=0.3)
ax2.legend()
ax2.set_yscale('log')

# å­å›¾3: MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ
ax3 = axes[1, 0]
reconstruction_errors_1 = ERRORU.flatten()
mean_error_1 = np.mean(np.abs(reconstruction_errors_1))
ax3.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
         label=f'MC-AE1 Reconstruction Error (Mean: {mean_error_1:.4f}) / MC-AE1é‡æ„è¯¯å·® (å‡å€¼: {mean_error_1:.4f})')
ax3.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
ax3.set_ylabel('Frequency / é¢‘æ•°')
ax3.set_title('MC-AE1 Reconstruction Error Distribution / MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax3.legend()
ax3.grid(True, alpha=0.3)

# å­å›¾4: MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ
ax4 = axes[1, 1]
reconstruction_errors_2 = ERRORX.flatten()
mean_error_2 = np.mean(np.abs(reconstruction_errors_2))
ax4.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
         label=f'MC-AE2 Reconstruction Error (Mean: {mean_error_2:.4f}) / MC-AE2é‡æ„è¯¯å·® (å‡å€¼: {mean_error_2:.4f})')
ax4.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
ax4.set_ylabel('Frequency / é¢‘æ•°')
ax4.set_title('MC-AE2 Reconstruction Error Distribution / MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f'{result_dir}/bilstm_training_results.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"âœ… BiLSTMè®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {result_dir}/bilstm_training_results.png")

# ç»“æœç›®å½•å·²åœ¨å‰é¢åˆ›å»ºï¼Œæ— éœ€é‡å¤æ£€æŸ¥

# 2. ä¿å­˜è¯Šæ–­ç‰¹å¾DataFrame
df_data.to_excel(f'{result_dir}/diagnosis_feature_bilstm_baseline.xlsx', index=False)
df_data.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline.csv', index=False)
print(f"âœ“ ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline.xlsx/csv")

# 3. ä¿å­˜PCAåˆ†æä¸»è¦ç»“æœ
np.save(f'{result_dir}/v_I_bilstm_baseline.npy', v_I)
np.save(f'{result_dir}/v_bilstm_baseline.npy', v)
np.save(f'{result_dir}/v_ratio_bilstm_baseline.npy', v_ratio)
np.save(f'{result_dir}/p_k_bilstm_baseline.npy', p_k)
np.save(f'{result_dir}/data_mean_bilstm_baseline.npy', data_mean)
np.save(f'{result_dir}/data_std_bilstm_baseline.npy', data_std)
np.save(f'{result_dir}/T_95_limit_bilstm_baseline.npy', T_95_limit)
np.save(f'{result_dir}/T_99_limit_bilstm_baseline.npy', T_99_limit)
np.save(f'{result_dir}/SPE_95_limit_bilstm_baseline.npy', SPE_95_limit)
np.save(f'{result_dir}/SPE_99_limit_bilstm_baseline.npy', SPE_99_limit)
np.save(f'{result_dir}/P_bilstm_baseline.npy', P)
np.save(f'{result_dir}/k_bilstm_baseline.npy', k)
np.save(f'{result_dir}/P_t_bilstm_baseline.npy', P_t)
np.save(f'{result_dir}/X_bilstm_baseline.npy', X)
np.save(f'{result_dir}/data_nor_bilstm_baseline.npy', data_nor)
print(f"âœ“ ä¿å­˜PCAåˆ†æç»“æœ: {result_dir}/*_bilstm_baseline.npy")

# 4. ä¿å­˜CombinedAEæ¨¡å‹å‚æ•°
torch.save(net.state_dict(), f'{result_dir}/net_model_bilstm_baseline.pth')
torch.save(netx.state_dict(), f'{result_dir}/netx_model_bilstm_baseline.pth')
print(f"âœ“ ä¿å­˜MC-AEæ¨¡å‹: {result_dir}/net_model_bilstm_baseline.pth, {result_dir}/netx_model_bilstm_baseline.pth")

# 5. ä¿å­˜è®­ç»ƒå†å²
training_history = {
    'mcae1_losses': train_losses_mcae1,
    'mcae2_losses': train_losses_mcae2,
    'final_mcae1_loss': train_losses_mcae1[-1],
    'final_mcae2_loss': train_losses_mcae2[-1],
    'mcae1_reconstruction_error_mean': np.mean(np.abs(ERRORU)),
    'mcae1_reconstruction_error_std': np.std(np.abs(ERRORU)),
    'mcae2_reconstruction_error_mean': np.mean(np.abs(ERRORX)),
    'mcae2_reconstruction_error_std': np.std(np.abs(ERRORX)),
    'training_samples': len(train_samples),
    'epochs': EPOCH,
    'learning_rate': INIT_LR, # Changed from LR to INIT_LR
    'batch_size': BATCHSIZE
}

import pickle
with open(f'{result_dir}/bilstm_training_history.pkl', 'wb') as f:
    pickle.dump(training_history, f)
print(f"âœ“ ä¿å­˜è®­ç»ƒå†å²: {result_dir}/bilstm_training_history.pkl")

print("="*50)
print("ğŸ‰ BiLSTMåŸºå‡†è®­ç»ƒå®Œæˆï¼")
print("="*50)
print("BiLSTMåŸºå‡†æ¨¡å¼æ€»ç»“ï¼š")
print("1. âœ… è·³è¿‡Transformerè®­ç»ƒé˜¶æ®µ")
print("2. âœ… ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("3. âœ… ä¿æŒPack Modelingè¾“å‡ºvin_2[x[1]]å’Œvin_3[x[1]]ä¸å˜")
print("4. âœ… MC-AEä½¿ç”¨åŸå§‹BiLSTMæ•°æ®è¿›è¡Œè®­ç»ƒ")
print("5. âœ… æ‰€æœ‰æ¨¡å‹å’Œç»“æœæ–‡ä»¶æ·»åŠ '_bilstm_baseline'åç¼€")
print("")
print("ğŸ“Š æ¯”å¯¹è¯´æ˜ï¼š")
print("   - æ­¤æ¨¡å¼å»ºç«‹BiLSTMåŸºå‡†æ€§èƒ½")
print("   - å¯ä¸Transformeræ¨¡å¼è¿›è¡Œå…¬å¹³å¯¹æ¯”")
print("   - ä¾¿äºè¯„ä¼°Transformeræ›¿æ¢çš„æ•ˆæœ")
print("   - è®­ç»ƒæ—¶é—´æ›´çŸ­ï¼Œé€‚åˆå¿«é€ŸéªŒè¯") 