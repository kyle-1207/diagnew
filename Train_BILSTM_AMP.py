# å¯¼å…¥å¿…è¦çš„åº“
import os
import warnings
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import pickle
import time
from datetime import datetime

# å¯¼å…¥æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·
from torch.cuda.amp import autocast, GradScaler

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from Function_ import *
from Class_ import CombinedAE
from distributed_utils import (
    setup_distributed, cleanup_distributed, to_distributed,
    create_distributed_loader, is_main_process, barrier, reduce_value
)

# è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
is_distributed, local_rank, world_size = setup_distributed()
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# æ‰“å°GPUä¿¡æ¯
if is_main_process():
    print(f"\nğŸ–¥ï¸ åˆ†å¸ƒå¼è®­ç»ƒé…ç½®:")
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"   å½“å‰è¿›ç¨‹: {local_rank}")
    print(f"   æ€»è¿›ç¨‹æ•°: {world_size}")
    if torch.cuda.is_available():
        print(f"   GPUå‹å·: {torch.cuda.get_device_name(local_rank)}")
        print(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(local_rank).total_memory/1024**3:.1f}GB")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒé…ç½®
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK SC']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

class MCDataset(Dataset):
    """MC-AEæ•°æ®é›†"""
    def __init__(self, x, y, z, q):
        self.x = torch.tensor(x, dtype=torch.float64)
        self.y = torch.tensor(y, dtype=torch.float64)
        self.z = torch.tensor(z, dtype=torch.float64)
        self.q = torch.tensor(q, dtype=torch.float64)
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.z[idx], self.q[idx]

def main():
    """ä¸»å‡½æ•° - åˆ†å¸ƒå¼è®­ç»ƒ + æ··åˆç²¾åº¦ç‰ˆæœ¬"""
    try:
        # ç¡®ä¿åˆ†å¸ƒå¼ç¯å¢ƒæ­£ç¡®åˆå§‹åŒ–
        if not is_distributed and torch.cuda.device_count() > 1:
            print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å¤šä¸ªGPUä½†æœªå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·ä½¿ç”¨torchrunå¯åŠ¨è„šæœ¬")
            return
        
        # åŠ è½½è®­ç»ƒæ ·æœ¬ï¼ˆ0-200å·ï¼‰
        train_samples = list(range(201))
        if is_main_process():
            print(f"ğŸ“‹ è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-200")
            print(f"   æ ·æœ¬æ•°é‡: {len(train_samples)}")
        
        # åŠ è½½æ•°æ®
        all_vin2_data = []
        all_vin3_data = []
        
        for sample_id in train_samples:
            vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
            vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
            
            with open(vin2_path, 'rb') as file:
                vin2_data = pickle.load(file)
                all_vin2_data.append(vin2_data)
            
            with open(vin3_path, 'rb') as file:
                vin3_data = pickle.load(file)
                all_vin3_data.append(vin3_data)
        
        # åˆå¹¶æ•°æ®
        all_vin2_data = torch.cat(all_vin2_data, dim=0)
        all_vin3_data = torch.cat(all_vin3_data, dim=0)
        
        if is_main_process():
            print(f"\nğŸ“¥ æ•°æ®åŠ è½½å®Œæˆ:")
            print(f"   vin_2æ•°æ®å½¢çŠ¶: {all_vin2_data.shape}")
            print(f"   vin_3æ•°æ®å½¢çŠ¶: {all_vin3_data.shape}")
        
        # å®šä¹‰ç»´åº¦
        dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
        dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
        
        # åˆ†ç¦»æ•°æ®
        x_recovered = all_vin2_data[:, :dim_x]
        y_recovered = all_vin2_data[:, dim_x:dim_x + dim_y]
        z_recovered = all_vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
        q_recovered = all_vin2_data[:, dim_x + dim_y + dim_z:]
        
        x_recovered2 = all_vin3_data[:, :dim_x2]
        y_recovered2 = all_vin3_data[:, dim_x2:dim_x2 + dim_y2]
        z_recovered2 = all_vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
        q_recovered2 = all_vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        batch_size = 2048 * world_size  # å¢å¤§æ€»æ‰¹æ¬¡å¤§å°
        
        # MC-AE1æ•°æ®é›†
        dataset1 = MCDataset(x_recovered, y_recovered, z_recovered, q_recovered)
        train_loader1, train_sampler1 = create_distributed_loader(
            dataset1,
            batch_size=batch_size // world_size,
            num_workers=4
        )
        
        # MC-AE2æ•°æ®é›†
        dataset2 = MCDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2)
        train_loader2, train_sampler2 = create_distributed_loader(
            dataset2,
            batch_size=batch_size // world_size,
            num_workers=4
        )
        
        if is_main_process():
            print(f"\nğŸ“¦ åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
            print(f"   æ€»æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   æ¯GPUæ‰¹æ¬¡å¤§å°: {batch_size // world_size}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                        activation_fn=custom_activation, use_dx_in_forward=True)
        netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110,
                         activation_fn=torch.sigmoid, use_dx_in_forward=True)
        
        # è½¬æ¢ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
        net = to_distributed(net, local_rank)
        netx = to_distributed(netx, local_rank)
        
        if is_main_process():
            print(f"\nğŸ§  åˆ†å¸ƒå¼MC-AEæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            print(f"   MC-AE1å‚æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
            print(f"   MC-AE2å‚æ•°é‡: {sum(p.numel() for p in netx.parameters()):,}")
        
        # è®­ç»ƒå‚æ•°
        num_epochs = 300
        learning_rate = 5e-4 * 4  # å› ä¸ºæ‰¹æ¬¡å¤§å°å¢åŠ 4å€ï¼Œå­¦ä¹ ç‡ä¹Ÿç›¸åº”å¢åŠ 
        
        # ä¼˜åŒ–å™¨
        optimizer1 = optim.Adam(net.parameters(), lr=learning_rate)
        optimizer2 = optim.Adam(netx.parameters(), lr=learning_rate)
        
        # æŸå¤±å‡½æ•°
        criterion = nn.MSELoss()
        
        # åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨
        scaler1 = GradScaler()
        scaler2 = GradScaler()
        
        if is_main_process():
            print("\nâš™ï¸  è®­ç»ƒå‚æ•°:")
            print(f"   å­¦ä¹ ç‡: {learning_rate}")
            print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   æ··åˆç²¾åº¦è®­ç»ƒ: å¯ç”¨")
            print("\n" + "="*60)
            print("ğŸ¯ å¼€å§‹MC-AEè®­ç»ƒ")
            print("="*60)
        
        # è®­ç»ƒMC-AE1
        train_losses_mcae1 = []
        best_loss_mcae1 = float('inf')
        
        if is_main_process():
            print("\nğŸ”§ è®­ç»ƒç¬¬ä¸€ç»„MC-AEæ¨¡å‹ï¼ˆvin_2ï¼‰...")
        
        for epoch in range(num_epochs):
            net.train()
            epoch_loss = 0.0
            batch_count = 0
            
            # è®¾ç½®é‡‡æ ·å™¨çš„epoch
            if train_sampler1:
                train_sampler1.set_epoch(epoch)
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                pbar = tqdm(train_loader1, desc=f"MC-AE1 Epoch {epoch:3d}")
            else:
                pbar = train_loader1
            
            for x, y, z, q in pbar:
                x = x.to(local_rank if is_distributed else 'cuda')
                y = y.to(local_rank if is_distributed else 'cuda')
                z = z.to(local_rank if is_distributed else 'cuda')
                q = q.to(local_rank if is_distributed else 'cuda')
                
                optimizer1.zero_grad()
                
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast():
                    outputs = net(x, z, q)
                    loss = criterion(outputs[0], y)
                
                # ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾å™¨
                scaler1.scale(loss).backward()
                scaler1.step(optimizer1)
                scaler1.update()
                
                # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±
                reduced_loss = reduce_value(loss.detach(), average=True)
                epoch_loss += reduced_loss.item()
                batch_count += 1
                
                if is_main_process():
                    pbar.set_postfix({'loss': f"{reduced_loss.item():.6f}"})
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = epoch_loss / batch_count
            train_losses_mcae1.append(avg_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and avg_loss < best_loss_mcae1:
                best_loss_mcae1 = avg_loss
                torch.save(net.module.state_dict() if is_distributed else net.state_dict(),
                         '/mnt/bz25t/bzhy/zhanglikang/project/models/net_model_bilstm_baseline_best.pth')
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and epoch % 50 == 0:
                print(f"MC-AE1 Epoch: {epoch:3d} | Average Loss: {avg_loss:.4f}")
        
        # è®­ç»ƒMC-AE2
        train_losses_mcae2 = []
        best_loss_mcae2 = float('inf')
        
        if is_main_process():
            print("\nğŸ”§ è®­ç»ƒç¬¬äºŒç»„MC-AEæ¨¡å‹ï¼ˆvin_3ï¼‰...")
        
        for epoch in range(num_epochs):
            netx.train()
            epoch_loss = 0.0
            batch_count = 0
            
            # è®¾ç½®é‡‡æ ·å™¨çš„epoch
            if train_sampler2:
                train_sampler2.set_epoch(epoch)
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                pbar = tqdm(train_loader2, desc=f"MC-AE2 Epoch {epoch:3d}")
            else:
                pbar = train_loader2
            
            for x, y, z, q in pbar:
                x = x.to(local_rank if is_distributed else 'cuda')
                y = y.to(local_rank if is_distributed else 'cuda')
                z = z.to(local_rank if is_distributed else 'cuda')
                q = q.to(local_rank if is_distributed else 'cuda')
                
                optimizer2.zero_grad()
                
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast():
                    outputs = netx(x, z, q)
                    loss = criterion(outputs[0], y)
                
                # ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾å™¨
                scaler2.scale(loss).backward()
                scaler2.step(optimizer2)
                scaler2.update()
                
                # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±
                reduced_loss = reduce_value(loss.detach(), average=True)
                epoch_loss += reduced_loss.item()
                batch_count += 1
                
                if is_main_process():
                    pbar.set_postfix({'loss': f"{reduced_loss.item():.6f}"})
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = epoch_loss / batch_count
            train_losses_mcae2.append(avg_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and avg_loss < best_loss_mcae2:
                best_loss_mcae2 = avg_loss
                torch.save(netx.module.state_dict() if is_distributed else netx.state_dict(),
                         '/mnt/bz25t/bzhy/zhanglikang/project/models/netx_model_bilstm_baseline_best.pth')
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and epoch % 50 == 0:
                print(f"MC-AE2 Epoch: {epoch:3d} | Average Loss: {avg_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            torch.save(net.module.state_dict() if is_distributed else net.state_dict(),
                     '/mnt/bz25t/bzhy/zhanglikang/project/models/net_model_bilstm_baseline.pth')
            torch.save(netx.module.state_dict() if is_distributed else netx.state_dict(),
                     '/mnt/bz25t/bzhy/zhanglikang/project/models/netx_model_bilstm_baseline.pth')
            
            # ä¿å­˜è®­ç»ƒå†å²
            history = {
                'mcae1_losses': train_losses_mcae1,
                'mcae2_losses': train_losses_mcae2,
                'mcae1_best_loss': best_loss_mcae1,
                'mcae2_best_loss': best_loss_mcae2,
                'batch_size': batch_size,
                'world_size': world_size,
                'amp_enabled': True
            }
            
            with open('/mnt/bz25t/bzhy/zhanglikang/project/models/bilstm_training_history.pkl', 'wb') as f:
                pickle.dump(history, f)
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            plt.figure(figsize=(15, 6))
            
            # MC-AE1æŸå¤±æ›²çº¿
            plt.subplot(1, 2, 1)
            plt.plot(train_losses_mcae1, label='MC-AE1 Loss')
            plt.title('MC-AE1 Training Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.yscale('log')
            plt.grid(True)
            plt.legend()
            
            # MC-AE2æŸå¤±æ›²çº¿
            plt.subplot(1, 2, 2)
            plt.plot(train_losses_mcae2, label='MC-AE2 Loss')
            plt.title('MC-AE2 Training Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.yscale('log')
            plt.grid(True)
            plt.legend()
            
            plt.tight_layout()
            plt.savefig('/mnt/bz25t/bzhy/zhanglikang/project/models/bilstm_training_results.png')
            plt.close()
            
            print("\nğŸ’¾ ä¿å­˜ç»“æœ:")
            print("   âœ“ MC-AE1æ¨¡å‹: net_model_bilstm_baseline.pth")
            print("   âœ“ MC-AE2æ¨¡å‹: netx_model_bilstm_baseline.pth")
            print("   âœ“ æœ€ä½³MC-AE1: net_model_bilstm_baseline_best.pth")
            print("   âœ“ æœ€ä½³MC-AE2: netx_model_bilstm_baseline_best.pth")
            print("   âœ“ è®­ç»ƒå†å²: bilstm_training_history.pkl")
            print("   âœ“ è®­ç»ƒæ›²çº¿: bilstm_training_results.png")
            
            print("\nğŸ“Š è®­ç»ƒç»“æœ:")
            print(f"   MC-AE1æœ€ç»ˆæŸå¤±: {train_losses_mcae1[-1]:.6f}")
            print(f"   MC-AE2æœ€ç»ˆæŸå¤±: {train_losses_mcae2[-1]:.6f}")
            print(f"   MC-AE1æœ€ä½³æŸå¤±: {best_loss_mcae1:.6f}")
            print(f"   MC-AE2æœ€ä½³æŸå¤±: {best_loss_mcae2:.6f}")
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
        raise e
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()

if __name__ == "__main__":
    main()