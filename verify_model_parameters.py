#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯BILSTMå’ŒTransformeræ¨¡å‹å‚æ•°é‡åŒ¹é…
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from Class_ import LSTM

class TransformerPredictor(nn.Module):
    """å¤åˆ¶Transformeræ¨¡å‹ç»“æ„ç”¨äºå‚æ•°é‡å¯¹æ¯”"""
    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2):
        super(TransformerPredictor, self).__init__()
        self.input_size = input_size
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model, dtype=torch.float32))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model//2, output_size)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def count_parameters(self):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

def main():
    print("ğŸ” éªŒè¯BILSTMå’ŒTransformeræ¨¡å‹å‚æ•°é‡åŒ¹é…")
    print("=" * 60)
    
    # åˆ›å»ºBILSTMæ¨¡å‹
    print("1. åˆ›å»ºBILSTMæ¨¡å‹...")
    bilstm_model = LSTM()
    bilstm_params = bilstm_model.count_parameters()
    
    # åˆ›å»ºTransformeræ¨¡å‹
    print("2. åˆ›å»ºTransformeræ¨¡å‹...")
    transformer_model = TransformerPredictor(
        input_size=7,
        d_model=128,
        nhead=8,
        num_layers=3,
        output_size=2
    )
    transformer_params = transformer_model.count_parameters()
    
    # å‚æ•°é‡å¯¹æ¯”
    print("\nğŸ“Š æ¨¡å‹å‚æ•°é‡å¯¹æ¯”:")
    print("-" * 40)
    print(f"BILSTMæ¨¡å‹å‚æ•°é‡:      {bilstm_params:,}")
    print(f"Transformeræ¨¡å‹å‚æ•°é‡: {transformer_params:,}")
    print(f"å‚æ•°é‡æ¯”ä¾‹:            {bilstm_params/transformer_params:.3f}")
    
    print(f"\nBILSTMæ¨¡å‹è§„æ¨¡:        {bilstm_params/1e6:.2f}M å‚æ•°")
    print(f"Transformeræ¨¡å‹è§„æ¨¡:   {transformer_params/1e6:.2f}M å‚æ•°")
    
    # åˆ¤æ–­æ˜¯å¦åŒ¹é…
    ratio = bilstm_params / transformer_params
    if 0.3 <= ratio <= 1.5:  # åœ¨åˆç†èŒƒå›´å†…
        print(f"\nâœ… å‚æ•°é‡åŒ¹é…è‰¯å¥½ï¼æ¯”ä¾‹ {ratio:.3f} åœ¨åˆç†èŒƒå›´å†… (0.3-1.5)")
    else:
        print(f"\nâš ï¸  å‚æ•°é‡å·®å¼‚è¾ƒå¤§ï¼æ¯”ä¾‹ {ratio:.3f} è¶…å‡ºåˆç†èŒƒå›´")
    
    # è¯¦ç»†ç»“æ„å¯¹æ¯”
    print("\nğŸ—ï¸  æ¨¡å‹ç»“æ„å¯¹æ¯”:")
    print("-" * 40)
    print("BILSTM:")
    print("  - BiLSTM: input_size=7, hidden_size=128, num_layers=3, bidirectional=True")
    print("  - FCç½‘ç»œ: 256 â†’ 128 â†’ 64 â†’ 2")
    print("  - Dropout: 0.1")
    
    print("\nTransformer:")
    print("  - è¾“å…¥æŠ•å½±: 7 â†’ 128")
    print("  - ä½ç½®ç¼–ç : 1000Ã—128")
    print("  - Transformerç¼–ç å™¨: d_model=128, nhead=8, num_layers=3")
    print("  - è¾“å‡ºç½‘ç»œ: 128 â†’ 64 â†’ 2")
    print("  - Dropout: 0.1")
    
    print("\n" + "=" * 60)
    print("éªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    main()
