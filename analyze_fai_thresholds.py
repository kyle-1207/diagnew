# FAIé˜ˆå€¼åˆ†æè„šæœ¬
# ä¸“é—¨ç”¨äºåˆ†ææ•…éšœæ ·æœ¬å’Œæ­£å¸¸æ ·æœ¬çš„FAIå€¼åˆ†å¸ƒæƒ…å†µ

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import torch
import os
import warnings
from Function_ import *
from Class_ import *
from Comprehensive_calculation import Comprehensive_calculation

warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

def load_sample_data(sample_id):
    """åŠ è½½æ ·æœ¬æ•°æ®"""
    base_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}'
    
    with open(f'{base_path}/vin_1.pkl', 'rb') as f:
        vin1_data = pickle.load(f)
    with open(f'{base_path}/vin_2.pkl', 'rb') as f:
        vin2_data = pickle.load(f) 
    with open(f'{base_path}/vin_3.pkl', 'rb') as f:
        vin3_data = pickle.load(f)
        
    return vin1_data, vin2_data, vin3_data

def load_models():
    """åŠ è½½æ¨¡å‹å’ŒPCAå‚æ•°"""
    models = {}
    
    # åŠ è½½MC-AEæ¨¡å‹
    models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                              activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                               activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    net_path = "/mnt/bz25t/bzhy/datasave/Three_model/BILSTM/net_model_bilstm_baseline.pth"
    netx_path = "/mnt/bz25t/bzhy/datasave/Three_model/BILSTM/netx_model_bilstm_baseline.pth"
    
    models['net'].load_state_dict(torch.load(net_path, map_location=device), strict=False)
    models['netx'].load_state_dict(torch.load(netx_path, map_location=device), strict=False)
    
    # åŠ è½½PCAå‚æ•°
    pca_params_path = "/mnt/bz25t/bzhy/datasave/Three_model/BILSTM/pca_params_bilstm_baseline.pkl"
    with open(pca_params_path, 'rb') as f:
        models['pca_params'] = pickle.load(f)
    
    return models

def compute_fai_for_sample(sample_id, models):
    """è®¡ç®—æ ·æœ¬çš„FAIå€¼"""
    print(f"\nğŸ”¬ å¤„ç†æ ·æœ¬ {sample_id}...")
    
    # åŠ è½½æ•°æ®
    vin1_data, vin2_data, vin3_data = load_sample_data(sample_id)
    
    # æ•°æ®é¢„å¤„ç†
    if len(vin1_data.shape) == 2:
        vin1_data = vin1_data.unsqueeze(1)
    vin1_data = vin1_data.to(torch.float32).to(device)

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
    
    # MC-AEæ¨ç†
    models['net'].eval()
    models['netx'].eval()
    
    with torch.no_grad():
        x_recovered = x_recovered.float()
        z_recovered = z_recovered.float()
        q_recovered = q_recovered.float()
        x_recovered2 = x_recovered2.float()
        z_recovered2 = z_recovered2.float()
        q_recovered2 = q_recovered2.float()
        
        recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
        reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    AA = recon_imtest[0].cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU

    BB = reconx_imtest[0].cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX

    # è¯Šæ–­ç‰¹å¾æå–
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    
    # ç»¼åˆè®¡ç®—è·å–FAI
    pca_params = models['pca_params']
    time = np.arange(df_data.shape[0])
    
    result = Comprehensive_calculation(
        df_data.values, 
        pca_params['data_mean'], 
        pca_params['data_std'], 
        pca_params['v'].reshape(len(pca_params['v']), 1), 
        pca_params['p_k'], 
        pca_params['v_I'], 
        pca_params['T_99_limit'], 
        pca_params['SPE_99_limit'], 
        pca_params['X'], 
        time
    )
    
    fai = result[9]  # FAIæ˜¯ç¬¬10ä¸ªè¿”å›å€¼
    
    print(f"   æ•°æ®é•¿åº¦: {len(fai)} ä¸ªæ—¶é—´ç‚¹")
    print(f"   FAIç»Ÿè®¡: å‡å€¼={np.mean(fai):.6f}, æ ‡å‡†å·®={np.std(fai):.6f}")
    print(f"   FAIèŒƒå›´: [{np.min(fai):.6f}, {np.max(fai):.6f}]")
    
    return fai

def analyze_thresholds(fai, sample_id, sample_type):
    """åˆ†æé˜ˆå€¼æƒ…å†µ"""
    print(f"\nğŸ“Š æ ·æœ¬{sample_id} ({sample_type}) é˜ˆå€¼åˆ†æ:")
    
    # æŒ‰ç…§åŸç‰ˆæ–¹å¼è®¡ç®—é˜ˆå€¼
    nm = 3000
    mm = len(fai)
    
    if mm > nm:
        # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
        baseline_fai = fai[nm:mm]
        mean_fai = np.mean(baseline_fai)
        std_fai = np.std(baseline_fai)
        print(f"   ä½¿ç”¨ååŠæ®µæ•°æ®({nm}:{mm})è®¡ç®—é˜ˆå€¼")
        print(f"   åŸºçº¿ç»Ÿè®¡: å‡å€¼={mean_fai:.6f}, æ ‡å‡†å·®={std_fai:.6f}")
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        mean_fai = np.mean(fai)
        std_fai = np.std(fai)
        print(f"   æ•°æ®é•¿åº¦ä¸è¶³{nm}ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—é˜ˆå€¼")
        print(f"   å…¨é‡ç»Ÿè®¡: å‡å€¼={mean_fai:.6f}, æ ‡å‡†å·®={std_fai:.6f}")
    
    # è®¡ç®—å„çº§é˜ˆå€¼
    threshold_configs = {
        '3Ïƒ (åŸç‰ˆ)': {'multipliers': [3, 4.5, 6]},
        '2.5Ïƒ (å®½æ¾)': {'multipliers': [2.5, 3.5, 4.5]}, 
        '2Ïƒ (å¾ˆå®½æ¾)': {'multipliers': [2, 3, 4]},
        '1.5Ïƒ (æå®½æ¾)': {'multipliers': [1.5, 2.5, 3.5]}
    }
    
    print(f"\n   å„é˜ˆå€¼é…ç½®ä¸‹çš„æ£€æµ‹ç»“æœ:")
    print(f"   {'é…ç½®':<12} {'T1':<10} {'T2':<10} {'T3':<10} {'>T1':<8} {'>T2':<8} {'>T3':<8} {'>T1%':<8}")
    print(f"   {'-'*80}")
    
    best_config = None
    target_ratio = 0.05 if sample_type == "æ•…éšœ" else 0.01  # æ•…éšœæ ·æœ¬æœŸæœ›5%å¼‚å¸¸ç‡ï¼Œæ­£å¸¸æ ·æœ¬1%
    
    for config_name, config in threshold_configs.items():
        t1 = mean_fai + config['multipliers'][0] * std_fai
        t2 = mean_fai + config['multipliers'][1] * std_fai
        t3 = mean_fai + config['multipliers'][2] * std_fai
        
        above_t1 = np.sum(fai > t1)
        above_t2 = np.sum(fai > t2)
        above_t3 = np.sum(fai > t3)
        ratio_t1 = above_t1 / len(fai)
        
        print(f"   {config_name:<12} {t1:<10.4f} {t2:<10.4f} {t3:<10.4f} {above_t1:<8} {above_t2:<8} {above_t3:<8} {ratio_t1*100:<7.2f}%")
        
        # ä¸ºæ•…éšœæ ·æœ¬é€‰æ‹©åˆé€‚çš„é…ç½®
        if sample_type == "æ•…éšœ" and best_config is None and above_t1 > 0:
            best_config = config_name
    
    if sample_type == "æ•…éšœ":
        if best_config:
            print(f"\n   ğŸ’¡ æ¨èé…ç½®: {best_config} (é¦–ä¸ªæœ‰æ£€æµ‹ç»“æœçš„é…ç½®)")
        else:
            print(f"\n   âš ï¸  æ‰€æœ‰é…ç½®éƒ½æ— æ³•æ£€æµ‹åˆ°å¼‚å¸¸ç‚¹ï¼æ•°æ®å¯èƒ½å­˜åœ¨é—®é¢˜")
    
    return threshold_configs

def analyze_continuity(fai, sample_id, sample_type):
    """åˆ†æå¼‚å¸¸ç‚¹çš„è¿ç»­æ€§"""
    print(f"\nğŸ”— æ ·æœ¬{sample_id} ({sample_type}) è¿ç»­æ€§åˆ†æ:")
    
    # ä½¿ç”¨2Ïƒé˜ˆå€¼è¿›è¡Œè¿ç»­æ€§åˆ†æï¼ˆç›¸å¯¹å®½æ¾ï¼‰
    nm = 3000
    mm = len(fai)
    
    if mm > nm:
        baseline_fai = fai[nm:mm]
        mean_fai = np.mean(baseline_fai)
        std_fai = np.std(baseline_fai)
    else:
        mean_fai = np.mean(fai)
        std_fai = np.std(fai)
    
    threshold = mean_fai + 2 * std_fai  # ä½¿ç”¨2Ïƒé˜ˆå€¼
    above_indices = np.where(fai > threshold)[0]
    
    if len(above_indices) == 0:
        print(f"   æ²¡æœ‰ç‚¹è¶…è¿‡2Ïƒé˜ˆå€¼({threshold:.6f})")
        return
    
    print(f"   ä½¿ç”¨2Ïƒé˜ˆå€¼({threshold:.6f})è¿›è¡Œè¿ç»­æ€§åˆ†æ")
    print(f"   è¶…è¿‡é˜ˆå€¼çš„ç‚¹æ•°: {len(above_indices)} ({len(above_indices)/len(fai)*100:.2f}%)")
    
    # åˆ†æè¿ç»­æ®µ
    continuous_segments = []
    current_segment = [above_indices[0]]
    
    for i in range(1, len(above_indices)):
        gap = above_indices[i] - above_indices[i-1]
        if gap <= 5:  # å…è®¸4ä¸ªç‚¹çš„é—´éš”
            current_segment.append(above_indices[i])
        else:
            if len(current_segment) >= 3:  # è‡³å°‘3ä¸ªç‚¹æ‰ç®—è¿ç»­æ®µ
                continuous_segments.append(current_segment)
            current_segment = [above_indices[i]]
    
    # æ·»åŠ æœ€åä¸€æ®µ
    if len(current_segment) >= 3:
        continuous_segments.append(current_segment)
    
    print(f"   è¿ç»­å¼‚å¸¸æ®µæ•°: {len(continuous_segments)}")
    
    if len(continuous_segments) > 0:
        print(f"   è¿ç»­æ®µè¯¦æƒ…:")
        for j, segment in enumerate(continuous_segments[:5]):  # æ˜¾ç¤ºå‰5æ®µ
            start_pos = segment[0]
            end_pos = segment[-1]
            length = len(segment)
            max_fai = np.max(fai[segment])
            avg_fai = np.mean(fai[segment])
            print(f"     æ®µ{j+1}: ä½ç½®[{start_pos:4d}-{end_pos:4d}] é•¿åº¦={length:3d} å³°å€¼={max_fai:.6f} å‡å€¼={avg_fai:.6f}")
        
        # åˆ†æé—´éš”
        if len(continuous_segments) > 1:
            gaps = []
            for i in range(1, len(continuous_segments)):
                gap = continuous_segments[i][0] - continuous_segments[i-1][-1]
                gaps.append(gap)
            print(f"   æ®µé—´é—´éš”: å¹³å‡={np.mean(gaps):.1f}, æœ€å°={np.min(gaps)}, æœ€å¤§={np.max(gaps)}")
    else:
        print(f"   æ²¡æœ‰å‘ç°è¿ç»­å¼‚å¸¸æ®µï¼ˆé•¿åº¦>=3ï¼‰")
        # æ˜¾ç¤ºå­¤ç«‹å¼‚å¸¸ç‚¹
        print(f"   å­¤ç«‹å¼‚å¸¸ç‚¹åˆ†å¸ƒ:")
        for i in range(min(10, len(above_indices))):
            idx = above_indices[i]
            print(f"     ä½ç½®{idx:4d}: FAI={fai[idx]:.6f} (è¶…å‡ºé˜ˆå€¼: {fai[idx]-threshold:.6f})")

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ”¬ FAIé˜ˆå€¼å’Œè¿ç»­æ€§åˆ†æå·¥å…·")
    print("="*80)
    
    # æµ‹è¯•æ ·æœ¬é…ç½®
    test_samples = {
        '10': 'æ­£å¸¸',   # æ­£å¸¸æ ·æœ¬
        '335': 'æ•…éšœ'   # æ•…éšœæ ·æœ¬
    }
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½æ¨¡å‹å’Œå‚æ•°...")
    models = load_models()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åˆ†ææ¯ä¸ªæ ·æœ¬
    for sample_id, sample_type in test_samples.items():
        print(f"\n{'='*60}")
        print(f"åˆ†ææ ·æœ¬ {sample_id} ({sample_type})")
        print(f"{'='*60}")
        
        try:
            # è®¡ç®—FAI
            fai = compute_fai_for_sample(sample_id, models)
            
            # é˜ˆå€¼åˆ†æ
            analyze_thresholds(fai, sample_id, sample_type)
            
            # è¿ç»­æ€§åˆ†æ
            analyze_continuity(fai, sample_id, sample_type)
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ ·æœ¬{sample_id}å¤±è´¥: {e}")
            continue
    
    print(f"\n{'='*80}")
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print("ğŸ’¡ æ ¹æ®åˆ†æç»“æœè°ƒæ•´é˜ˆå€¼é…ç½®:")
    print("   1. å¦‚æœæ•…éšœæ ·æœ¬æ²¡æœ‰å¼‚å¸¸ç‚¹ï¼Œé™ä½é˜ˆå€¼å€æ•°")
    print("   2. å¦‚æœæ­£å¸¸æ ·æœ¬å¼‚å¸¸ç‚¹è¿‡å¤šï¼Œæé«˜é˜ˆå€¼å€æ•°") 
    print("   3. è§‚å¯Ÿè¿ç»­æ€§ï¼Œè®¾è®¡åˆé€‚çš„5ç‚¹æ£€æµ‹ç­–ç•¥")
    print("="*80)

if __name__ == "__main__":
    main()