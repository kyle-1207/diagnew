# ä¸­æ–‡æ³¨é‡Šï¼šå¯¼å…¥å¸¸ç”¨åº“å’Œè‡ªå®šä¹‰æ¨¡å—
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from scipy import ndimage  # æ·»åŠ ç”¨äºæ—¶åºå¹³æ»‘çš„å¯¼å…¥

# å¯¼å…¥æ–°çš„æ•°æ®åŠ è½½å™¨
from data_loader_transformer import TransformerBatteryDataset, create_transformer_dataloader

# å†…å­˜ç›‘æ§å‡½æ•°
def print_gpu_memory():
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {allocated:.1f}GB / {cached:.1f}GB / {total:.1f}GB (å·²ç”¨/ç¼“å­˜/æ€»è®¡)")

# æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
def setup_mixed_precision():
    """è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ"""
    scaler = torch.cuda.amp.GradScaler()
    print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
    return scaler

# æ•°æ®å¤„ç†å‡½æ•°ï¼ˆä»BiLSTMè„šæœ¬å¤åˆ¶ï¼‰
def check_data_quality(data, name, sample_id=None):
    """è¯¦ç»†çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
    prefix = f"æ ·æœ¬ {sample_id} - " if sample_id else ""
    print(f"\nğŸ” {prefix}{name} æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # æ•°å€¼ç»Ÿè®¡
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    print(f"   æ•°å€¼èŒƒå›´: [{data_np.min():.6f}, {data_np.max():.6f}]")
    print(f"   å‡å€¼: {data_np.mean():.6f}")
    print(f"   æ ‡å‡†å·®: {data_np.std():.6f}")
    print(f"   ä¸­ä½æ•°: {np.median(data_np):.6f}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥
    nan_count = np.isnan(data_np).sum()
    inf_count = np.isinf(data_np).sum()
    zero_count = (data_np == 0).sum()
    negative_count = (data_np < 0).sum()
    
    print(f"   NaNæ•°é‡: {nan_count}")
    print(f"   Infæ•°é‡: {inf_count}")
    print(f"   é›¶å€¼æ•°é‡: {zero_count}")
    print(f"   è´Ÿå€¼æ•°é‡: {negative_count}")
    
    # å¼‚å¸¸å€¼æ¯”ä¾‹
    total_elements = data_np.size
    print(f"   NaNæ¯”ä¾‹: {nan_count/total_elements*100:.2f}%")
    print(f"   Infæ¯”ä¾‹: {inf_count/total_elements*100:.2f}%")
    print(f"   é›¶å€¼æ¯”ä¾‹: {zero_count/total_elements*100:.2f}%")
    print(f"   è´Ÿå€¼æ¯”ä¾‹: {negative_count/total_elements*100:.2f}%")
    
    # å¼‚å¸¸å€¼è­¦å‘Š
    if nan_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°NaNå€¼ï¼")
    if inf_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼")
    if data_np.min() < -1e6 or data_np.max() > 1e6:
        print(f"   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¤§å€¼ï¼èŒƒå›´: [{data_np.min():.2e}, {data_np.max():.2e}]")
    
    return {
        'has_nan': nan_count > 0,
        'has_inf': inf_count > 0,
        'has_extreme_values': data_np.min() < -1e6 or data_np.max() > 1e6,
        'data_type': data.dtype,
        'shape': data.shape
    }

def physics_based_data_processing(data, name, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆå‚è€ƒè®ºæ–‡æ–¹æ³•ï¼‰"""
    print(f"\nğŸ”§ åŸºäºç‰©ç†çº¦æŸå¤„ç† {name}...")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç±»å‹
    if isinstance(data, np.ndarray):
        print(f"   åŸå§‹ç±»å‹: numpy.ndarray, dtype={data.dtype}")
    elif isinstance(data, torch.Tensor):
        print(f"   åŸå§‹ç±»å‹: torch.Tensor, dtype={data.dtype}")
    else:
        print(f"   åŸå§‹ç±»å‹: {type(data)}")
    
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    print(f"   åŸå§‹æ•°æ®ç‚¹æ•°é‡: {original_data_points}")
    
    print("   æ‰§è¡ŒåŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†...")
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤1: å¤„ç†ç¼ºå¤±æ•°æ®...")
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        print(f"     æ£€æµ‹åˆ° {complete_nan_rows.sum()} è¡Œå®Œå…¨ç¼ºå¤±çš„æ•°æ®")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡ç¼ºå¤±æ•°æ®å¤„ç†")
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
                print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢å…¨NaNè¡Œ")
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
                print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢")
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    print("   æ­¥éª¤2: å¤„ç†å¼‚å¸¸æ•°æ®...")
    
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        print(f"     å¤„ç†vin_2æ•°æ®ï¼ˆ225åˆ—ï¼‰")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºç”µå‹èŒƒå›´[0,5]Vçš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
            else:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: ç”µå‹å€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹ç›¸å…³åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºèŒƒå›´[-5,5]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        print(f"     å¤„ç†vin_3æ•°æ®ï¼ˆ226åˆ—ï¼‰ï¼Œç¬¬224åˆ—ä¸ºç‰¹æ®Šä¿ç•™åˆ—")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       SOCé¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
            else:
                print(f"       SOCé¢„æµ‹åˆ—{col}: SOCå€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       å•ä½“SOCåˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       SOCåå·®åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        special_col = 224
        print(f"       ç‰¹æ®Šä¿ç•™åˆ—{special_col}: ä¿æŒåŸå€¼ä¸å˜")
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
            
    elif feature_type == 'current':
        # ç”µæµç‰©ç†çº¦æŸï¼š-100Aåˆ°100A
        valid_mask = (data_np >= -100) & (data_np <= 100)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-100,100]Açš„å¼‚å¸¸å€¼")
            data_np[data_np < -100] = -100
            data_np[data_np > 100] = 100
            
    elif feature_type == 'temperature':
        # æ¸©åº¦ç‰©ç†çº¦æŸï¼š-40Â°Cåˆ°80Â°C
        valid_mask = (data_np >= -40) & (data_np <= 80)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np < -40] = -40
            data_np[data_np > 80] = 80
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤3: å¤„ç†é‡‡æ ·æ•…éšœ...")
    
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        print(f"     æ£€æµ‹åˆ° {fault_mask.sum()} ä¸ªé‡‡æ ·æ•…éšœç‚¹")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢æ•…éšœç‚¹ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡é‡‡æ ·æ•…éšœå¤„ç†")
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢ {col_fault_mask.sum()} ä¸ªæ•…éšœå€¼")
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢")
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    print("   æ­¥éª¤4: æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        print(f"     âš ï¸  ä»æœ‰ {final_nan_count} ä¸ªNaNå’Œ {final_inf_count} ä¸ªInfå€¼")
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    else:
        print("     âœ… æ‰€æœ‰å¼‚å¸¸å€¼å·²å¤„ç†å®Œæˆ")
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    # æ£€æŸ¥æ•°æ®ç‚¹æ•°é‡æ˜¯å¦ä¿æŒä¸€è‡´
    final_data_points = data_tensor.shape[0]
    if final_data_points == original_data_points:
        print(f"   å¤„ç†å®Œæˆ: {data_tensor.shape}, dtype={data_tensor.dtype}")
        print(f"   âœ… æ•°æ®ç‚¹æ•°é‡ä¿æŒä¸€è‡´: {original_data_points} -> {final_data_points}")
    else:
        print(f"   âš ï¸  æ•°æ®ç‚¹æ•°é‡å‘ç”Ÿå˜åŒ–: {original_data_points} -> {final_data_points}")
    
    return data_tensor

def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
            
    elif feature_type == 'current':
        # ç”µæµç‰©ç†çº¦æŸï¼š-100Aåˆ°100A
        valid_mask = (data_np >= -100) & (data_np <= 100)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            data_np[data_np < -100] = -100
            data_np[data_np > 100] = 100
            
    elif feature_type == 'temperature':
        # æ¸©åº¦ç‰©ç†çº¦æŸï¼š-40Â°Cåˆ°80Â°C
        valid_mask = (data_np >= -40) & (data_np <= 80)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            data_np[data_np < -40] = -40
            data_np[data_np > 80] = 80
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    return data_tensor

# GPUè®¾å¤‡é…ç½® - ä½¿ç”¨GPU0å’ŒGPU1è¿›è¡Œæ•°æ®å¹¶è¡Œ
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # ä½¿ç”¨GPU0å’ŒGPU1
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ åŒGPUå¹¶è¡Œé…ç½®:")
    print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i} ({props.name}): {props.total_memory/1024**3:.1f}GB")
    print(f"   ä¸»GPUè®¾å¤‡: cuda:0")
    print(f"   æ•°æ®å¹¶è¡Œæ¨¡å¼: å¯ç”¨")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# ä¸­æ–‡æ³¨é‡Šï¼šå¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒmatplotlibé…ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# Linuxç¯å¢ƒå­—ä½“è®¾ç½® - ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
import matplotlib.font_manager as fm
import os

# å°è¯•å¤šç§å­—ä½“æ–¹æ¡ˆ
font_options = [
    'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC',
    'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS'
]

# æ£€æŸ¥å¯ç”¨å­—ä½“
available_fonts = []
for font in font_options:
    try:
        fm.findfont(font)
        available_fonts.append(font)
    except:
        continue

# è®¾ç½®å­—ä½“
if available_fonts:
    plt.rcParams['font.sans-serif'] = available_fonts
    print(f"âœ… ä½¿ç”¨å­—ä½“: {available_fonts[0]}")
else:
    # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

#----------------------------------------é‡è¦è¯´æ˜ï¼šä½¿ç”¨é¢„è®¡ç®—çœŸå®å€¼------------------------------
# æ–°ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„çœŸå®Terminal Voltageå’ŒPack SOCå€¼è¿›è¡Œè®­ç»ƒ
# 
# æ•°æ®æµç¨‹ï¼š
# 1. è¾“å…¥ï¼švin_1å‰5ç»´ + å½“å‰æ—¶åˆ»çœŸå®ç”µå‹ + å½“å‰æ—¶åˆ»çœŸå®SOC (7ç»´)
# 2. è¾“å‡ºï¼šä¸‹ä¸€æ—¶åˆ»çœŸå®ç”µå‹ + ä¸‹ä¸€æ—¶åˆ»çœŸå®SOC (2ç»´)
# 3. æ— éœ€æ•°å€¼è½¬æ¢ï¼Œç›´æ¥é¢„æµ‹ç‰©ç†å€¼
#
# ä¼˜åŠ¿ï¼š
# - é¿å…äº†å¤æ‚çš„æ•°å€¼èŒƒå›´è½¬æ¢
# - ç›´æ¥å­¦ä¹ ç‰©ç†é‡ä¹‹é—´çš„å…³ç³»
# - è®­ç»ƒç›®æ ‡æ˜ç¡®ï¼Œæ”¶æ•›æ›´å¿«

#----------------------------------------Transformeræ¨¡å‹å®šä¹‰------------------------------
class TransformerPredictor(nn.Module):
    """æ—¶åºé¢„æµ‹Transformeræ¨¡å‹ - ç›´æ¥é¢„æµ‹çœŸå®ç‰©ç†å€¼"""
    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2):
        super(TransformerPredictor, self).__init__()
        self.input_size = input_size
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model, dtype=torch.float32))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚ - ç›´æ¥è¾“å‡ºç‰©ç†å€¼ï¼Œä¸ä½¿ç”¨Sigmoid
        self.output_layer = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model//2, output_size)
            # ç§»é™¤Sigmoidï¼Œç›´æ¥è¾“å‡ºç‰©ç†å€¼
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        # x: [batch, input_size] - 2ç»´è¾“å…¥
        if len(x.shape) == 2:
            # æ·»åŠ åºåˆ—ç»´åº¦ï¼š[batch, input_size] -> [batch, 1, input_size]
            x = x.unsqueeze(1)
        
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch, seq_len, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)
        x = x + pos_enc
        
        # Transformerç¼–ç 
        transformer_out = self.transformer(x)  # [batch, seq_len, d_model]
        
        # è¾“å‡ºå±‚ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        output = self.output_layer(transformer_out[:, -1, :])  # [batch, output_size]
        
        return output  # [batch, output_size] ç›´æ¥è¿”å›2ç»´

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("="*60)
    print("ğŸš€ Transformerè®­ç»ƒ - Linuxç¯å¢ƒç‰ˆæœ¬")
    print("="*60)
    
    # Linuxç¯å¢ƒæ£€æŸ¥
    import platform
    print(f"ğŸ–¥ï¸  è¿è¡Œç¯å¢ƒ: {platform.system()} {platform.release()}")
    print(f"ğŸ Pythonç‰ˆæœ¬: {platform.python_version()}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"ğŸš€ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ”¢ GPUæ•°é‡: {torch.cuda.device_count()}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    #----------------------------------------æ•°æ®åŠ è½½------------------------------
    # è®­ç»ƒæ ·æœ¬IDï¼ˆä½¿ç”¨QAS 0-200æ ·æœ¬ï¼‰
    def load_train_samples():
        """ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬ID"""
        try:
            import pandas as pd
            labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
            df = pd.read_excel(labels_path)
            
            # æå–0-200èŒƒå›´çš„æ ·æœ¬
            all_samples = df['Num'].tolist()
            train_samples = [i for i in all_samples if 0 <= i <= 200]
            
            print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬:")
            print(f"   è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-200")
            print(f"   å®é™…å¯ç”¨æ ·æœ¬: {len(train_samples)} ä¸ª")
            
            return train_samples
        except Exception as e:
            print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
            print("âš ï¸  ä½¿ç”¨é»˜è®¤æ ·æœ¬èŒƒå›´ 0-20")
            return list(range(21))
    
    train_samples = load_train_samples()
    print(f"ğŸ“Š ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªæ ·æœ¬")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ”§ GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"ğŸ”§ å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    
    # ä½¿ç”¨æ–°çš„æ•°æ®åŠ è½½å™¨
    print("\nğŸ“¥ åŠ è½½é¢„è®¡ç®—æ•°æ®...")
    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = TransformerBatteryDataset(data_path='/mnt/bz25t/bzhy/zhanglikang/project/QAS', sample_ids=train_samples)
        
        if len(dataset) == 0:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è®­ç»ƒæ•°æ®")
            print("è¯·ç¡®ä¿å·²è¿è¡Œ precompute_targets.py ç”Ÿæˆé¢„è®¡ç®—æ•°æ®")
            return
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} ä¸ªè®­ç»ƒæ•°æ®å¯¹")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        BATCH_SIZE = 4000  # ä»2000å¢åŠ åˆ°4000
        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, 
                                num_workers=4, pin_memory=True)
        print(f"ğŸ“¦ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹æ¬¡å¤§å°: {BATCH_SIZE}, num_workers: 4")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        sample_input, sample_target = dataset[0]
        print(f"ğŸ“Š æ•°æ®æ ¼å¼:")
        print(f"   è¾“å…¥ç»´åº¦: {sample_input.shape} (å‰5ç»´vin_1 + ç”µå‹ + SOC)")
        print(f"   ç›®æ ‡ç»´åº¦: {sample_target.shape} (ä¸‹ä¸€æ—¶åˆ»ç”µå‹ + SOC)")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²è¿è¡Œ precompute_targets.py ç”Ÿæˆé¢„è®¡ç®—æ•°æ®")
        return
    
    #----------------------------------------æ¨¡å‹åˆå§‹åŒ–------------------------------
    # åˆå§‹åŒ–Transformeræ¨¡å‹
    transformer = TransformerPredictor(
        input_size=7,      # vin_1å‰5ç»´ + ç”µå‹ + SOC
        d_model=128,       # æ¨¡å‹ç»´åº¦
        nhead=8,           # æ³¨æ„åŠ›å¤´æ•°
        num_layers=3,      # Transformerå±‚æ•°
        output_size=2      # è¾“å‡ºï¼šç”µå‹ + SOC
    ).to(device).float()
    
    # å¯ç”¨æ•°æ®å¹¶è¡Œ
    if torch.cuda.device_count() > 1:
        transformer = torch.nn.DataParallel(transformer)
        print(f"âœ… å¯ç”¨æ•°æ®å¹¶è¡Œï¼Œä½¿ç”¨ {torch.cuda.device_count()} å¼ GPU")
    else:
        print("âš ï¸  å•GPUæ¨¡å¼")
    
    print(f"ğŸ§  Transformeræ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in transformer.parameters()):,}")
    
    #----------------------------------------è®­ç»ƒå‚æ•°è®¾ç½®------------------------------
    LR = 1.5e-3            # å­¦ä¹ ç‡ä»1e-3å¢åŠ åˆ°1.5e-3
    EPOCH = 40             # è®­ç»ƒè½®æ•°ä»30å¢åŠ åˆ°40
    lr_decay_freq = 15     # å­¦ä¹ ç‡è¡°å‡é¢‘ç‡ä»10å¢åŠ åˆ°15
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(transformer.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_freq, gamma=0.9)
    criterion = nn.MSELoss()
    
    # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = setup_mixed_precision()
    
    print(f"âš™ï¸  è®­ç»ƒå‚æ•°ï¼ˆä¿å®ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰:")
    print(f"   å­¦ä¹ ç‡: {LR} (ä»1e-3å¢åŠ åˆ°1.5e-3)")
    print(f"   è®­ç»ƒè½®æ•°: {EPOCH} (ä»30å¢åŠ åˆ°40)")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} (ä»2000å¢åŠ åˆ°4000)")
    print(f"   å­¦ä¹ ç‡è¡°å‡é¢‘ç‡: {lr_decay_freq} (ä»10å¢åŠ åˆ°15)")
    print(f"   é¢„æµ‹æ‰¹æ¬¡å¤§å°: 15000 (ä»10000å¢åŠ åˆ°15000)")
    print(f"   MC-AEæ‰¹æ¬¡å¤§å°: 6000 (ä»3000å¢åŠ åˆ°6000)")
    print(f"   MC-AEè®­ç»ƒè½®æ•°: 250 (ä»300å‡å°‘åˆ°250)")
    print(f"   MC-AEå­¦ä¹ ç‡: 7e-4 (ä»5e-4å¢åŠ åˆ°7e-4)")
    print(f"   æ··åˆç²¾åº¦è®­ç»ƒ: å¯ç”¨")
    print(f"   DataLoaderä¼˜åŒ–: num_workers=4, pin_memory=True")
    
    #----------------------------------------å¼€å§‹è®­ç»ƒ------------------------------
    print("\n" + "="*60)
    print("ğŸ¯ å¼€å§‹Transformerè®­ç»ƒ")
    print("="*60)
    
    transformer.train()
    train_losses = []
    
    for epoch in range(EPOCH):
        epoch_loss = 0
        batch_count = 0
        
        for batch_input, batch_target in train_loader:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            batch_input = batch_input.to(device)
            batch_target = batch_target.to(device)
            
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                pred_output = transformer(batch_input)
                loss = criterion(pred_output, batch_target)
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += loss.item()
            batch_count += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = epoch_loss / batch_count
        train_losses.append(avg_loss)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if epoch % 5 == 0 or epoch == EPOCH - 1:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch: {epoch:3d} | Loss: {avg_loss:.6f} | LR: {current_lr:.6f}')
    
    print("\nâœ… Transformerè®­ç»ƒå®Œæˆ!")
    
    #----------------------------------------è®­ç»ƒç»“æœåˆ†æ------------------------------
    print("\n" + "="*60)
    print("ğŸ“Š è®­ç»ƒç»“æœåˆ†æ")
    print("="*60)
    
    # æœ€ç»ˆæŸå¤±
    final_loss = train_losses[-1]
    print(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_loss:.6f}")
    
    # æŸå¤±æ”¹å–„
    initial_loss = train_losses[0]
    improvement = (initial_loss - final_loss) / initial_loss * 100
    print(f"ğŸ“ˆ æŸå¤±æ”¹å–„: {improvement:.2f}% (ä» {initial_loss:.6f} åˆ° {final_loss:.6f})")
    
    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    transformer.eval()
    with torch.no_grad():
        total_samples = 0
        total_loss = 0
        voltage_errors = []
        soc_errors = []
        
        for batch_input, batch_target in train_loader:
            batch_input = batch_input.to(device)
            batch_target = batch_target.to(device)
            
            pred_output = transformer(batch_input)
            
            # è®¡ç®—æ€»æŸå¤±
            loss = criterion(pred_output, batch_target)
            total_loss += loss.item()
            total_samples += batch_input.size(0)
            
            # åˆ†åˆ«è®¡ç®—ç”µå‹å’ŒSOCè¯¯å·®
            voltage_error = torch.abs(pred_output[:, 0] - batch_target[:, 0])
            soc_error = torch.abs(pred_output[:, 1] - batch_target[:, 1])
            
            voltage_errors.extend(voltage_error.cpu().numpy())
            soc_errors.extend(soc_error.cpu().numpy())
        
        avg_test_loss = total_loss / len(train_loader)
        avg_voltage_error = np.mean(voltage_errors)
        avg_soc_error = np.mean(soc_errors)
        
        print(f"ğŸ” æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡æµ‹è¯•æŸå¤±: {avg_test_loss:.6f}")
        print(f"   å¹³å‡ç”µå‹è¯¯å·®: {avg_voltage_error:.4f} V")
        print(f"   å¹³å‡SOCè¯¯å·®: {avg_soc_error:.4f}")
        print(f"   ç”µå‹è¯¯å·®æ ‡å‡†å·®: {np.std(voltage_errors):.4f} V")
        print(f"   SOCè¯¯å·®æ ‡å‡†å·®: {np.std(soc_errors):.4f}")
    
    #----------------------------------------ä¿å­˜æ¨¡å‹------------------------------
    print("\n" + "="*60)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
    print("="*60)
    
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    if not os.path.exists('models'):
        os.makedirs('models')
    
    # ä¿å­˜Transformeræ¨¡å‹
    model_path = 'models/transformer_model.pth'
    torch.save(transformer.state_dict(), model_path)
    print(f"âœ… Transformeræ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    training_history = {
        'train_losses': train_losses,
        'final_loss': final_loss,
        'avg_voltage_error': avg_voltage_error,
        'avg_soc_error': avg_soc_error,
        'training_samples': train_samples,
        'model_params': {
            'input_size': 7,
            'd_model': 128,
            'nhead': 8,
            'num_layers': 3,
            'output_size': 2
        }
    }
    
    history_path = 'models/transformer_training_history.pkl'
    with open(history_path, 'wb') as f:
        pickle.dump(training_history, f)
    print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
    
    #----------------------------------------ç»˜åˆ¶è®­ç»ƒæ›²çº¿------------------------------
    print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    
    plt.figure(figsize=(12, 4))
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, 'b-', linewidth=2)
    plt.title('Transformer Training Loss / Transformerè®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14)
    plt.xlabel('Training Epochs / è®­ç»ƒè½®æ•°', fontsize=12)
    plt.ylabel('MSE Loss / MSEæŸå¤±', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡
    
    # è¯¯å·®åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.hist(voltage_errors, bins=50, alpha=0.7, label=f'Voltage Error (Mean: {avg_voltage_error:.4f}V) / ç”µå‹è¯¯å·® (å‡å€¼: {avg_voltage_error:.4f}V)', color='red')
    plt.hist(soc_errors, bins=50, alpha=0.7, label=f'SOC Error (Mean: {avg_soc_error:.4f}) / SOCè¯¯å·® (å‡å€¼: {avg_soc_error:.4f})', color='blue')
    plt.title('Prediction Error Distribution / é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=14)
    plt.xlabel('Absolute Error / ç»å¯¹è¯¯å·®', fontsize=12)
    plt.ylabel('Frequency / é¢‘æ¬¡', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = 'models/transformer_training_results.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… è®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {plot_path}")
    
    #----------------------------------------è®­ç»ƒå®Œæˆæ€»ç»“------------------------------
    print("\n" + "="*60)
    print("ğŸ‰ Transformerè®­ç»ƒå®Œæˆæ€»ç»“")
    print("="*60)
    print("âœ… ä¸»è¦æˆæœ:")
    print("   1. ä½¿ç”¨é¢„è®¡ç®—çš„çœŸå®Terminal Voltageå’ŒPack SOCæ•°æ®")
    print("   2. 7ç»´è¾“å…¥ â†’ 2ç»´ç‰©ç†å€¼è¾“å‡ºï¼Œæ— éœ€æ•°å€¼è½¬æ¢")
    print("   3. ç›´æ¥å­¦ä¹ ç‰©ç†é‡ä¹‹é—´çš„æ—¶åºå…³ç³»")
    print("   4. æ¨¡å‹å’Œè®­ç»ƒå†å²å·²ä¿å­˜åˆ°models/ç›®å½•")
    print("")
    print("ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_loss:.6f}")
    print(f"   å¹³å‡ç”µå‹é¢„æµ‹è¯¯å·®: {avg_voltage_error:.4f} V")
    print(f"   å¹³å‡SOCé¢„æµ‹è¯¯å·®: {avg_soc_error:.4f}")
    print("")
    print("ğŸ”„ ä¸‹ä¸€æ­¥:")
    print("   1. å¯ä»¥è¿è¡ŒTest_combine.pyè¿›è¡Œæ€§èƒ½å¯¹æ¯”")
    print("   2. æ£€æŸ¥é¢„æµ‹ç»“æœçš„ç‰©ç†åˆç†æ€§")
    print("   3. ä¸BiLSTMåŸºå‡†è¿›è¡Œè¯¦ç»†å¯¹æ¯”")
    
    #----------------------------------------é˜¶æ®µ2: åŠ è½½vin_2å’Œvin_3æ•°æ®ï¼Œè¿›è¡ŒTransformeré¢„æµ‹æ›¿æ¢------------------------
    print("\n" + "="*60)
    print("ğŸ”„ é˜¶æ®µ2: åŠ è½½vin_2å’Œvin_3æ•°æ®ï¼Œè¿›è¡ŒTransformeré¢„æµ‹æ›¿æ¢")
    print("="*60)
    
    # åŠ è½½æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„vin_2å’Œvin_3æ•°æ®
    all_vin1_data = []
    all_vin2_data = []
    all_vin3_data = []
    
    print("ğŸ“¥ åŠ è½½åŸå§‹vin_2å’Œvin_3æ•°æ®...")
    processed_count = 0
    failed_count = 0
    
    for sample_id in train_samples:
        try:
            # åŠ è½½vin_1æ•°æ®ï¼ˆç”¨äºTransformeré¢„æµ‹ï¼‰
            vin1_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_1.pkl'
            with open(vin1_path, 'rb') as file:
                vin1_data = pickle.load(file)
                if isinstance(vin1_data, torch.Tensor):
                    vin1_data = vin1_data.cpu()
                else:
                    vin1_data = torch.tensor(vin1_data)
                all_vin1_data.append(vin1_data)
            
            # åŠ è½½vin_2æ•°æ®å¹¶è¿›è¡Œé™é»˜å¤„ç†
            vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
            with open(vin2_path, 'rb') as file:
                vin2_data = pickle.load(file)
            
            # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
            all_vin2_data.append(vin2_processed)
            
            # åŠ è½½vin_3æ•°æ®å¹¶è¿›è¡Œé™é»˜å¤„ç†
            vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
            with open(vin3_path, 'rb') as file:
                vin3_data = pickle.load(file)
            
            # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
            all_vin3_data.append(vin3_processed)
            
            processed_count += 1
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if processed_count % 10 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {processed_count}/{len(train_samples)} ä¸ªæ ·æœ¬")
                
        except Exception as e:
            print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
            continue
    
    # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(train_samples)}")
    print(f"   æˆåŠŸå¤„ç†: {processed_count}")
    print(f"   å¤„ç†å¤±è´¥: {failed_count}")
    print(f"   æˆåŠŸç‡: {processed_count/len(train_samples)*100:.1f}%")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_vin1 = torch.cat(all_vin1_data, dim=0).float()
    combined_vin2 = torch.cat(all_vin2_data, dim=0).float()
    combined_vin3 = torch.cat(all_vin3_data, dim=0).float()
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®å½¢çŠ¶:")
    print(f"   vin_1: {combined_vin1.shape}")
    print(f"   vin_2: {combined_vin2.shape}")
    print(f"   vin_3: {combined_vin3.shape}")
    
    # åˆå¹¶åçš„æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("\nğŸ” åˆå¹¶åæ•°æ®è´¨é‡æ£€æŸ¥:")
    print(f"   vin_2 NaNæ•°é‡: {torch.isnan(combined_vin2).sum()}")
    print(f"   vin_2 Infæ•°é‡: {torch.isinf(combined_vin2).sum()}")
    print(f"   vin_3 NaNæ•°é‡: {torch.isnan(combined_vin3).sum()}")
    print(f"   vin_3 Infæ•°é‡: {torch.isinf(combined_vin3).sum()}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼éœ€è¦å¤„ç†
    vin2_has_issues = (torch.isnan(combined_vin2).any() or 
                       torch.isinf(combined_vin2).any() or 
                       combined_vin2.min() < -1e6 or 
                       combined_vin2.max() > 1e6)
    
    vin3_has_issues = (torch.isnan(combined_vin3).any() or 
                       torch.isinf(combined_vin3).any() or 
                       combined_vin3.min() < -1e6 or 
                       combined_vin3.max() > 1e6)
    
    if vin2_has_issues or vin3_has_issues:
        print("\nâš ï¸  æ£€æµ‹åˆ°æ•°æ®é—®é¢˜ï¼Œè¿›è¡Œä¿®å¤...")
        
        # ä¿®å¤NaNå’ŒInfå€¼
        if torch.isnan(combined_vin2).any() or torch.isinf(combined_vin2).any():
            print("   ä¿®å¤vin_2ä¸­çš„NaNå’ŒInfå€¼")
            combined_vin2 = torch.where(torch.isnan(combined_vin2) | torch.isinf(combined_vin2), 
                                       torch.zeros_like(combined_vin2), combined_vin2)
        
        if torch.isnan(combined_vin3).any() or torch.isinf(combined_vin3).any():
            print("   ä¿®å¤vin_3ä¸­çš„NaNå’ŒInfå€¼")
            combined_vin3 = torch.where(torch.isnan(combined_vin3) | torch.isinf(combined_vin3), 
                                        torch.zeros_like(combined_vin3), combined_vin3)
        
        # æ£€æŸ¥ä¿®å¤åçš„æ•°æ®
        print("\nğŸ” ä¿®å¤åæ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"   vin_2 NaNæ•°é‡: {torch.isnan(combined_vin2).sum()}")
        print(f"   vin_2 Infæ•°é‡: {torch.isinf(combined_vin2).sum()}")
        print(f"   vin_3 NaNæ•°é‡: {torch.isnan(combined_vin3).sum()}")
        print(f"   vin_3 Infæ•°é‡: {torch.isinf(combined_vin3).sum()}")
    else:
        print("\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®å¤")
    
    # ä½¿ç”¨Transformerè¿›è¡Œé¢„æµ‹å’Œæ›¿æ¢
    print("\nğŸ”„ ä½¿ç”¨Transformeré¢„æµ‹æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]...")
    transformer.eval()
    
    # é¢„å…ˆåŠ è½½æ‰€æœ‰targetsæ•°æ®ï¼ˆä¼˜åŒ–I/Oï¼‰
    print("ğŸ“¥ é¢„å…ˆåŠ è½½æ‰€æœ‰targetsæ•°æ®...")
    all_targets = {}
    for sample_id in train_samples:
        targets_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/targets.pkl'
        with open(targets_path, 'rb') as f:
            all_targets[sample_id] = pickle.load(f)
    print(f"âœ… å·²åŠ è½½ {len(all_targets)} ä¸ªæ ·æœ¬çš„targetsæ•°æ®")
    
    # æ‰¹é‡æ„å»ºTransformerè¾“å…¥æ•°æ®
    print("ğŸ”§ æ‰¹é‡æ„å»ºTransformerè¾“å…¥æ•°æ®...")
    transformer_inputs = torch.zeros(len(combined_vin1), 7, dtype=torch.float32)
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„èµ·å§‹å’Œç»“æŸç´¢å¼•
    sample_indices = []
    current_idx = 0
    for sample_idx, sample_id in enumerate(train_samples):
        sample_len = len(all_vin1_data[sample_idx])
        sample_indices.append((current_idx, current_idx + sample_len, sample_id))
        current_idx += sample_len
    
    # æ‰¹é‡å¡«å……è¾“å…¥æ•°æ®
    for start_idx, end_idx, sample_id in sample_indices:
        targets = all_targets[sample_id]
        terminal_voltages = np.array(targets['terminal_voltages'])
        pack_socs = np.array(targets['pack_socs'])
        
        # å¡«å……vin_1å‰5ç»´
        transformer_inputs[start_idx:end_idx, 0:5] = combined_vin1[start_idx:end_idx, 0, 0:5]
        # å¡«å……å½“å‰æ—¶åˆ»çœŸå®ç”µå‹
        transformer_inputs[start_idx:end_idx, 5] = torch.tensor(terminal_voltages[:end_idx-start_idx], dtype=torch.float32)
        # å¡«å……å½“å‰æ—¶åˆ»çœŸå®SOC
        transformer_inputs[start_idx:end_idx, 6] = torch.tensor(pack_socs[:end_idx-start_idx], dtype=torch.float32)
    
    print(f"âœ… è¾“å…¥æ•°æ®æ„å»ºå®Œæˆ: {transformer_inputs.shape}")
    
    # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    print("ğŸ“Š GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
    print_gpu_memory()
    
    # æ‰¹é‡é¢„æµ‹ï¼ˆä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼‰
    print("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹...")
    transformer_inputs = transformer_inputs.to(device)
    
    with torch.no_grad():
        # ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜GPUåˆ©ç”¨ç‡
        batch_size = 15000  # ä»10000å¢åŠ åˆ°15000
        predictions = []
        total_batches = (len(transformer_inputs) + batch_size - 1) // batch_size
        
        for i in range(0, len(transformer_inputs), batch_size):
            batch_idx = i // batch_size + 1
            batch_data = transformer_inputs[i:i+batch_size]
            batch_pred = transformer(batch_data)
            predictions.append(batch_pred.cpu())
            
            # æ˜¾ç¤ºè¿›åº¦
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                print(f"   è¿›åº¦: {batch_idx}/{total_batches} ({batch_idx/total_batches*100:.1f}%)")
        
        transformer_predictions = torch.cat(predictions, dim=0)
    
    print(f"âœ… Transformeré¢„æµ‹å®Œæˆ: {transformer_predictions.shape}")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†ï¼Œå½“å‰ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
    
    # æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]
    vin2_modified = combined_vin2.clone()
    vin3_modified = combined_vin3.clone()
    
    # ç¡®ä¿é•¿åº¦åŒ¹é…
    min_len = min(len(transformer_predictions), len(vin2_modified), len(vin3_modified))
    
    # æ›¿æ¢BiLSTMé¢„æµ‹å€¼ä¸ºTransformeré¢„æµ‹å€¼
    vin2_modified[:min_len, 0] = transformer_predictions[:min_len, 0]  # ç”µå‹é¢„æµ‹
    vin3_modified[:min_len, 0] = transformer_predictions[:min_len, 1]  # SOCé¢„æµ‹
    
    print(f"ğŸ”„ æ›¿æ¢å®Œæˆ:")
    print(f"   åŸå§‹vin_2[:,0]èŒƒå›´: [{combined_vin2[:, 0].min():.4f}, {combined_vin2[:, 0].max():.4f}]")
    print(f"   Transformer vin_2[:,0]èŒƒå›´: [{vin2_modified[:, 0].min():.4f}, {vin2_modified[:, 0].max():.4f}]")
    print(f"   åŸå§‹vin_3[:,0]èŒƒå›´: [{combined_vin3[:, 0].min():.4f}, {combined_vin3[:, 0].max():.4f}]")
    print(f"   Transformer vin_3[:,0]èŒƒå›´: [{vin3_modified[:, 0].min():.4f}, {vin3_modified[:, 0].max():.4f}]")
    
    #----------------------------------------é˜¶æ®µ3: è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹------------------------
    print("\n" + "="*60)
    print("ğŸ§  é˜¶æ®µ3: è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨Transformerå¢å¼ºæ•°æ®ï¼‰")
    print("="*60)
    
    # å‚è€ƒTrain_BILSTM.pyçš„MC-AEè®­ç»ƒé€»è¾‘
    from Function_ import custom_activation
    
    # å®šä¹‰ç‰¹å¾åˆ‡ç‰‡ç»´åº¦ï¼ˆä¸Train_BILSTM.pyä¸€è‡´ï¼‰
    # vin_2.pkl
    dim_x = 2
    dim_y = 110
    dim_z = 110
    dim_q = 3
    
    # åˆ†å‰²vin_2ç‰¹å¾å¼ é‡
    x_recovered = vin2_modified[:, :dim_x]
    y_recovered = vin2_modified[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_modified[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_modified[:, dim_x + dim_y + dim_z:]
    
    # vin_3.pkl
    dim_x2 = 2
    dim_y2 = 110
    dim_z2 = 110
    dim_q2 = 4
    
    x_recovered2 = vin3_modified[:, :dim_x2]
    y_recovered2 = vin3_modified[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_modified[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_modified[:, dim_x2 + dim_y2 + dim_z2:]
    
    print(f"ğŸ“Š MC-AEè®­ç»ƒæ•°æ®å‡†å¤‡:")
    print(f"   vin_2ç‰¹å¾: x{x_recovered.shape}, y{y_recovered.shape}, z{z_recovered.shape}, q{q_recovered.shape}")
    print(f"   vin_3ç‰¹å¾: x{x_recovered2.shape}, y{y_recovered2.shape}, z{z_recovered2.shape}, q{q_recovered2.shape}")
    
    # MC-AEè®­ç»ƒå‚æ•°ï¼ˆä¿å®ˆä¼˜åŒ–ï¼‰
    EPOCH_MCAE = 250       # ä»300å‡å°‘åˆ°250
    LR_MCAE = 7e-4         # ä»5e-4å¢åŠ åˆ°7e-4
    BATCHSIZE_MCAE = 6000  # ä»3000å¢åŠ åˆ°6000
    
    # è‡ªå®šä¹‰å¤šè¾“å…¥æ•°æ®é›†ç±»
    class MCDataset(Dataset):
        def __init__(self, x, y, z, q):
            self.x = x.to(torch.double)
            self.y = y.to(torch.double)
            self.z = z.to(torch.double)
            self.q = q.to(torch.double)
        def __len__(self):
            return len(self.x)
        def __getitem__(self, idx):
            return self.x[idx], self.y[idx], self.z[idx], self.q[idx]
    
    # ç¬¬ä¸€ç»„ç‰¹å¾ï¼ˆvin_2ï¼‰çš„MC-AEè®­ç»ƒ
    print("\nğŸ”§ è®­ç»ƒç¬¬ä¸€ç»„MC-AEæ¨¡å‹ï¼ˆvin_2ï¼‰...")
    train_loader_u = DataLoader(MCDataset(x_recovered, y_recovered, z_recovered, q_recovered), 
                               batch_size=BATCHSIZE_MCAE, shuffle=False, 
                               num_workers=4, pin_memory=True)
    
    net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, 
                    activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    
    optimizer_mcae = torch.optim.Adam(net.parameters(), lr=LR_MCAE)
    loss_f = nn.MSELoss()
    
    # è®°å½•è®­ç»ƒæŸå¤±
    train_losses_mcae1 = []
    
    for epoch in range(EPOCH_MCAE):
        total_loss = 0
        num_batches = 0
        for iteration, (x, y, z, q) in enumerate(train_loader_u):
            x = x.to(device)
            y = y.to(device)
            z = z.to(device)
            q = q.to(device)
            net = net.double()
            recon_im, recon_p = net(x, z, q)
            loss_u = loss_f(y, recon_im)
            total_loss += loss_u.item()
            num_batches += 1
            optimizer_mcae.zero_grad()
            loss_u.backward()
            optimizer_mcae.step()
        avg_loss = total_loss / num_batches
        train_losses_mcae1.append(avg_loss)
        if epoch % 50 == 0:
            print(f'MC-AE1 Epoch: {epoch:3d} | Average Loss: {avg_loss:.6f}')
    
    # è·å–ç¬¬ä¸€ç»„é‡æ„è¯¯å·®
    train_loader2 = DataLoader(MCDataset(x_recovered, y_recovered, z_recovered, q_recovered), 
                              batch_size=len(x_recovered), shuffle=False,
                              num_workers=4, pin_memory=True)
    for iteration, (x, y, z, q) in enumerate(train_loader2):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        net = net.double()
        recon_imtest, recon = net(x, z, q)
    
    AA = recon_imtest.cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU
    
    # ç¬¬äºŒç»„ç‰¹å¾ï¼ˆvin_3ï¼‰çš„MC-AEè®­ç»ƒ
    print("\nğŸ”§ è®­ç»ƒç¬¬äºŒç»„MC-AEæ¨¡å‹ï¼ˆvin_3ï¼‰...")
    train_loader_soc = DataLoader(MCDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), 
                                 batch_size=BATCHSIZE_MCAE, shuffle=False,
                                 num_workers=4, pin_memory=True)
    
    netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                     activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    optimizer_mcae2 = torch.optim.Adam(netx.parameters(), lr=LR_MCAE)
    
    # è®°å½•è®­ç»ƒæŸå¤±
    train_losses_mcae2 = []
    
    for epoch in range(EPOCH_MCAE):
        total_loss = 0
        num_batches = 0
        for iteration, (x, y, z, q) in enumerate(train_loader_soc):
            x = x.to(device)
            y = y.to(device)
            z = z.to(device)
            q = q.to(device)
            netx = netx.double()
            recon_im, z = netx(x, z, q)
            loss_x = loss_f(y, recon_im)
            total_loss += loss_x.item()
            num_batches += 1
            optimizer_mcae2.zero_grad()
            loss_x.backward()
            optimizer_mcae2.step()
        avg_loss = total_loss / num_batches
        train_losses_mcae2.append(avg_loss)
        if epoch % 50 == 0:
            print(f'MC-AE2 Epoch: {epoch:3d} | Average Loss: {avg_loss:.6f}')
    
    # è·å–ç¬¬äºŒç»„é‡æ„è¯¯å·®
    train_loaderx2 = DataLoader(MCDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), 
                               batch_size=len(x_recovered2), shuffle=False,
                               num_workers=4, pin_memory=True)
    for iteration, (x, y, z, q) in enumerate(train_loaderx2):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        netx = netx.double()
        recon_imtestx, z = netx(x, z, q)
    
    BB = recon_imtestx.cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX
    
    # ä¿å­˜ä¸­é—´ç»“æœï¼Œé¿å…é‡æ–°è®­ç»ƒ
    print("\nğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ...")
    model_suffix = "_transformer"
    
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    if not os.path.exists('models'):
        os.makedirs('models')
    
    # ä¿å­˜é‡æ„è¯¯å·®æ•°æ®
    np.save(f'models/ERRORU{model_suffix}.npy', ERRORU)
    np.save(f'models/ERRORX{model_suffix}.npy', ERRORX)
    print(f"âœ… ä¸­é—´ç»“æœå·²ä¿å­˜: ERRORU{model_suffix}.npy, ERRORX{model_suffix}.npy")
    
    # ä¿å­˜MC-AEè®­ç»ƒå†å²ï¼ˆç”¨äºæ–­ç‚¹ç»­ç®—ï¼‰
    mcae_intermediate_history = {
        'train_losses_mcae1': train_losses_mcae1,
        'train_losses_mcae2': train_losses_mcae2,
        'final_mcae1_loss': train_losses_mcae1[-1] if train_losses_mcae1 else None,
        'final_mcae2_loss': train_losses_mcae2[-1] if train_losses_mcae2 else None,
        'training_samples': len(train_samples),
        'epochs': EPOCH_MCAE,
        'learning_rate': LR_MCAE,
        'batch_size': BATCHSIZE_MCAE
    }
    
    with open(f'models/mcae_intermediate_history{model_suffix}.pkl', 'wb') as f:
        pickle.dump(mcae_intermediate_history, f)
    print(f"âœ… MC-AEä¸­é—´è®­ç»ƒå†å²å·²ä¿å­˜: mcae_intermediate_history{model_suffix}.pkl")
    
    print("âœ… MC-AEè®­ç»ƒå®Œæˆ!")
    
    #----------------------------------------MC-AEè®­ç»ƒç»“æœå¯è§†åŒ–------------------------
    print("\nğŸ“ˆ ç»˜åˆ¶MC-AEè®­ç»ƒç»“æœ...")
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å­å›¾1: MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿
    ax1 = axes[0, 0]
    epochs = range(1, len(train_losses_mcae1) + 1)
    ax1.plot(epochs, train_losses_mcae1, 'b-', linewidth=2, label='MC-AE1 Training Loss')
    ax1.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
    ax1.set_ylabel('MSE Loss / MSEæŸå¤±')
    ax1.set_title('MC-AE1 Training Loss / MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_yscale('log')
    
    # å­å›¾2: MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿ 
    ax2 = axes[0, 1]
    ax2.plot(epochs, train_losses_mcae2, 'r-', linewidth=2, label='MC-AE2 Training Loss')
    ax2.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
    ax2.set_ylabel('MSE Loss / MSEæŸå¤±')
    ax2.set_title('MC-AE2 Training Loss / MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.set_yscale('log')
    
    # å­å›¾3: MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ
    ax3 = axes[1, 0]
    reconstruction_errors_1 = ERRORU.flatten()
    mean_error_1 = np.mean(np.abs(reconstruction_errors_1))
    ax3.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
             label=f'MC-AE1 Reconstruction Error (Mean: {mean_error_1:.4f}) / MC-AE1é‡æ„è¯¯å·® (å‡å€¼: {mean_error_1:.4f})')
    ax3.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
    ax3.set_ylabel('Frequency / é¢‘æ•°')
    ax3.set_title('MC-AE1 Reconstruction Error Distribution / MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å­å›¾4: MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ
    ax4 = axes[1, 1]
    reconstruction_errors_2 = ERRORX.flatten()
    mean_error_2 = np.mean(np.abs(reconstruction_errors_2))
    ax4.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
             label=f'MC-AE2 Reconstruction Error (Mean: {mean_error_2:.4f}) / MC-AE2é‡æ„è¯¯å·® (å‡å€¼: {mean_error_2:.4f})')
    ax4.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
    ax4.set_ylabel('Frequency / é¢‘æ•°')
    ax4.set_title('MC-AE2 Reconstruction Error Distribution / MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = f'models/transformer_mcae_training_results.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… MC-AEè®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {plot_path}")
    
    #----------------------------------------é˜¶æ®µ4: PCAåˆ†æå’Œä¿å­˜æ¨¡å‹------------------------
    print("\n" + "="*60)
    print("ğŸ“Š é˜¶æ®µ4: PCAåˆ†æï¼Œä¿å­˜æ¨¡å‹å’Œå‚æ•°")
    print("="*60)
    
    # è¯Šæ–­ç‰¹å¾æå–ä¸PCAåˆ†æ
    from Function_ import DiagnosisFeature, PCA
    
    print("ğŸ” æå–è¯Šæ–­ç‰¹å¾...")
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    print(f"   è¯Šæ–­ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_data.shape}")
    
    print("ğŸ” è¿›è¡ŒPCAåˆ†æ...")
    v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = PCA(df_data, 0.95, 0.95)
    
    print(f"âœ… PCAåˆ†æå®Œæˆ:")
    print(f"   ä¸»æˆåˆ†æ•°é‡: {k}")
    print(f"   è§£é‡Šæ–¹å·®æ¯”: {v_ratio}")
    print(f"   TÂ²æ§åˆ¶é™: 95%={T_95_limit:.4f}, 99%={T_99_limit:.4f}")
    print(f"   SPEæ§åˆ¶é™: 95%={SPE_95_limit:.4f}, 99%={SPE_99_limit:.4f}")
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œåˆ†æç»“æœ
    print("\nğŸ’¾ ä¿å­˜Transformerå¢å¼ºè®­ç»ƒç»“æœ...")
    model_suffix = "_transformer"
    
    # 1. ä¿å­˜Transformeræ¨¡å‹ï¼ˆå·²ä¿å­˜ï¼‰
    
    # 2. ä¿å­˜MC-AEæ¨¡å‹
    torch.save(net.state_dict(), f'models/net_model{model_suffix}.pth')
    torch.save(netx.state_dict(), f'models/netx_model{model_suffix}.pth')
    print(f"âœ… MC-AEæ¨¡å‹å·²ä¿å­˜: models/net_model{model_suffix}.pth, models/netx_model{model_suffix}.pth")
    
    # 3. ä¿å­˜è¯Šæ–­ç‰¹å¾ï¼ˆåˆ†å—ä¿å­˜ï¼Œé¿å…Excelæ–‡ä»¶è¿‡å¤§ï¼‰
    print(f"ğŸ’¾ ä¿å­˜è¯Šæ–­ç‰¹å¾ï¼ˆæ•°æ®é‡: {df_data.shape}ï¼‰...")
    
    # CSVæ–‡ä»¶ä¿å­˜ï¼ˆæ— å¤§å°é™åˆ¶ï¼‰
    csv_path = f'models/diagnosis_feature{model_suffix}.csv'
    df_data.to_csv(csv_path, index=False)
    print(f"âœ… è¯Šæ–­ç‰¹å¾CSVå·²ä¿å­˜: {csv_path}")
    
    # Excelæ–‡ä»¶åˆ†å—ä¿å­˜ï¼ˆé¿å…è¶…è¿‡Excelè¡Œæ•°é™åˆ¶ï¼‰
    excel_path = f'models/diagnosis_feature{model_suffix}.xlsx'
    max_rows_per_sheet = 1000000  # Excelé™åˆ¶çº¦104ä¸‡è¡Œï¼Œç•™äº›ä½™é‡
    
    if len(df_data) > max_rows_per_sheet:
        print(f"âš ï¸  æ•°æ®é‡è¿‡å¤§({len(df_data)}è¡Œ)ï¼Œè¿›è¡Œåˆ†å—ä¿å­˜...")
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªå·¥ä½œè¡¨
            num_sheets = (len(df_data) + max_rows_per_sheet - 1) // max_rows_per_sheet
            
            for i in range(num_sheets):
                start_idx = i * max_rows_per_sheet
                end_idx = min((i + 1) * max_rows_per_sheet, len(df_data))
                chunk = df_data.iloc[start_idx:end_idx]
                
                sheet_name = f'Sheet_{i+1}' if i > 0 else 'Sheet_1'
                chunk.to_excel(writer, sheet_name=sheet_name, index=False)
                print(f"   å·¥ä½œè¡¨ {i+1}/{num_sheets}: {start_idx+1}-{end_idx} è¡Œ")
        
        print(f"âœ… è¯Šæ–­ç‰¹å¾Excelå·²åˆ†å—ä¿å­˜: {excel_path} ({num_sheets}ä¸ªå·¥ä½œè¡¨)")
    else:
        # æ•°æ®é‡ä¸å¤§ï¼Œç›´æ¥ä¿å­˜
        df_data.to_excel(excel_path, index=False)
        print(f"âœ… è¯Šæ–­ç‰¹å¾Excelå·²ä¿å­˜: {excel_path}")
    
    # 4. ä¿å­˜PCAåˆ†æç»“æœ
    np.save(f'models/v_I{model_suffix}.npy', v_I)
    np.save(f'models/v{model_suffix}.npy', v)
    np.save(f'models/v_ratio{model_suffix}.npy', v_ratio)
    np.save(f'models/p_k{model_suffix}.npy', p_k)
    np.save(f'models/data_mean{model_suffix}.npy', data_mean)
    np.save(f'models/data_std{model_suffix}.npy', data_std)
    np.save(f'models/T_95_limit{model_suffix}.npy', T_95_limit)
    np.save(f'models/T_99_limit{model_suffix}.npy', T_99_limit)
    np.save(f'models/SPE_95_limit{model_suffix}.npy', SPE_95_limit)
    np.save(f'models/SPE_99_limit{model_suffix}.npy', SPE_99_limit)
    np.save(f'models/P{model_suffix}.npy', P)
    np.save(f'models/k{model_suffix}.npy', k)
    np.save(f'models/P_t{model_suffix}.npy', P_t)
    np.save(f'models/X{model_suffix}.npy', X)
    np.save(f'models/data_nor{model_suffix}.npy', data_nor)
    print(f"âœ… PCAåˆ†æç»“æœå·²ä¿å­˜: models/*{model_suffix}.npy")
    
    # 5. ä¿å­˜MC-AEè®­ç»ƒå†å²
    mcae_training_history = {
        'mcae1_losses': train_losses_mcae1,
        'mcae2_losses': train_losses_mcae2,
        'final_mcae1_loss': train_losses_mcae1[-1],
        'final_mcae2_loss': train_losses_mcae2[-1],
        'mcae1_reconstruction_error_mean': np.mean(np.abs(ERRORU)),
        'mcae1_reconstruction_error_std': np.std(np.abs(ERRORU)),
        'mcae2_reconstruction_error_mean': np.mean(np.abs(ERRORX)),
        'mcae2_reconstruction_error_std': np.std(np.abs(ERRORX)),
        'training_samples': len(train_samples),
        'epochs': EPOCH_MCAE,
        'learning_rate': LR_MCAE,
        'batch_size': BATCHSIZE_MCAE
    }
    
    with open(f'models/transformer_mcae_training_history.pkl', 'wb') as f:
        pickle.dump(mcae_training_history, f)
    print(f"âœ… MC-AEè®­ç»ƒå†å²å·²ä¿å­˜: models/transformer_mcae_training_history.pkl")
    
    #----------------------------------------æœ€ç»ˆè®­ç»ƒå®Œæˆæ€»ç»“------------------------------
    print("\n" + "="*60)
    print("ğŸ‰ Transformerå®Œæ•´è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("="*60)
    print("âœ… è®­ç»ƒæµç¨‹æ€»ç»“:")
    print("   1. âœ… è®­ç»ƒTransformeræ—¶åºé¢„æµ‹æ¨¡å‹")
    print("   2. âœ… ä½¿ç”¨Transformeré¢„æµ‹æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]")
    print("   3. âœ… ä¿æŒPack Modelingè¾“å‡ºvin_2[:,1]å’Œvin_3[:,1]ä¸å˜")
    print("   4. âœ… MC-AEä½¿ç”¨Transformerå¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒ")
    print("   5. âœ… å®Œæ•´çš„PCAåˆ†æå’Œè¯Šæ–­ç‰¹å¾æå–")
    print("   6. âœ… æ‰€æœ‰æ¨¡å‹å’Œç»“æœæ–‡ä»¶æ·»åŠ '_transformer'åç¼€")
    print("   7. âœ… MC-AEè®­ç»ƒç»“æœå¯è§†åŒ–å›¾è¡¨")
    print("")
    print("ğŸ“Š å…³é”®æ”¹è¿›:")
    print("   - Transformeræ›¿æ¢BiLSTMè¿›è¡Œæ—¶åºé¢„æµ‹")
    print("   - ç›´æ¥ä½¿ç”¨çœŸå®ç‰©ç†å€¼è®­ç»ƒï¼Œæ— å¤æ‚è½¬æ¢")
    print("   - ä¿æŒä¸åŸå§‹MC-AEè®­ç»ƒæµç¨‹å®Œå…¨å…¼å®¹")
    print("   - ä¾¿äºä¸BiLSTMåŸºå‡†è¿›è¡Œå…¬å¹³å¯¹æ¯”")
    print("   - ä¿å®ˆä¼˜åŒ–ï¼šæ‰¹æ¬¡å¤§å°ç¿»å€ï¼Œå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    print("   - åŒGPUæ•°æ®å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨A100æ˜¾å­˜")
    print("   - DataLoaderä¼˜åŒ–ï¼šnum_workers=4, pin_memory=True")
    print("")
    print("ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   Transformerç”µå‹é¢„æµ‹è¯¯å·®: {avg_voltage_error:.4f} V")
    print(f"   Transformer SOCé¢„æµ‹è¯¯å·®: {avg_soc_error:.4f}")
    print(f"   PCAä¸»æˆåˆ†æ•°é‡: {k}")
    print("")
    print("ğŸ”„ ä¸‹ä¸€æ­¥å¯ä»¥:")
    print("   1. è¿è¡ŒTest_combine.pyè¿›è¡Œè¯¦ç»†æ€§èƒ½å¯¹æ¯”")
    print("   2. åˆ†æTransformer vs BiLSTMçš„æ•…éšœæ£€æµ‹æ•ˆæœ")
    print("   3. æ£€æŸ¥ä¸‰çª—å£æ£€æµ‹æœºåˆ¶çš„æ”¹è¿›æ•ˆæœ")

if __name__ == "__main__":
    main() 