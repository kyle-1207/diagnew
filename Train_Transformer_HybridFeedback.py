# ä¸­æ–‡æ³¨é‡Šï¼šæ··åˆåé¦ˆç­–ç•¥ç‰ˆTransformerè®­ç»ƒè„šæœ¬ - åŸºäºTrain_Transformer.pyæ¶æ„
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
from Comprehensive_calculation import Comprehensive_calculation
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
from scipy import ndimage
import copy
import time


# å¯¼å…¥Transformeræ•°æ®åŠ è½½å™¨
from data_loader_transformer import TransformerBatteryDataset, create_transformer_dataloader

# æ¿€è¿›åé¦ˆç­–ç•¥é…ç½® - ä¸“æ³¨é™ä½å‡é˜³ç‡
HYBRID_FEEDBACK_CONFIG = {
    # æ•°æ®åˆ†ç»„é…ç½®ï¼ˆä¸¥æ ¼æŒ‰ç…§READMEè§„èŒƒï¼‰
    'train_samples': list(range(8)),        # QAS 0-7 (8ä¸ªæ­£å¸¸æ ·æœ¬)
    'feedback_samples': [8, 9],             # QAS 8-9 (2ä¸ªæ­£å¸¸åé¦ˆæ ·æœ¬)
    
    # æ¿€è¿›åé¦ˆæœºåˆ¶é…ç½®
    'feedback_frequency': 3,                # æ¯3ä¸ªepochæ£€æŸ¥ä¸€æ¬¡ï¼ˆå¤§å¹…æé«˜ï¼‰
    'use_feedback': True,                   # å¯ç”¨åé¦ˆæœºåˆ¶
    'feedback_start_epoch': 30,             # ç¬¬30è½®å°±å¼€å§‹å¯ç”¨åé¦ˆï¼ˆæå‰ä»‹å…¥ï¼‰
    
    # æä¸¥æ ¼çš„åé¦ˆè§¦å‘é˜ˆå€¼ï¼ˆç›®æ ‡ï¼šæ­£å¸¸æ ·æœ¬æ¥è¿‘0%å‡é˜³ç‡ï¼‰
    'false_positive_thresholds': {
        'warning': 0.001,       # 0.1%é¢„è­¦ï¼ˆæä½é˜ˆå€¼ï¼‰
        'standard': 0.002,      # 0.2%æ ‡å‡†åé¦ˆï¼ˆæ¿€è¿›ï¼‰
        'enhanced': 0.005,      # 0.5%å¼ºåŒ–åé¦ˆï¼ˆæ¿€è¿›ï¼‰
        'emergency': 0.01       # 1%ç´§æ€¥åé¦ˆï¼ˆåŸæ¥çš„æ ‡å‡†ï¼‰
    },
    
    # æ¿€è¿›çš„æ··åˆæƒé‡é…ç½®
    'mcae_weight': 0.6,                     # é™ä½MC-AEæƒé‡
    'transformer_weight': 0.4,             # æé«˜Transformeræƒé‡ï¼ˆå¢å¼ºé¢„æµ‹ç²¾åº¦ï¼‰
    
    # æ¿€è¿›çš„è‡ªé€‚åº”å­¦ä¹ ç‡é…ç½®
    'adaptive_lr_factors': {
        'standard': 0.5,        # æ ‡å‡†åé¦ˆï¼šLR * 0.5ï¼ˆæ¿€è¿›è°ƒæ•´ï¼‰
        'enhanced': 0.3,        # å¼ºåŒ–åé¦ˆï¼šLR * 0.3ï¼ˆæ¿€è¿›è°ƒæ•´ï¼‰
        'emergency': 0.1        # ç´§æ€¥åé¦ˆï¼šLR * 0.1ï¼ˆææ¿€è¿›è°ƒæ•´ï¼‰
    },
    
    # æ–°å¢ï¼šåŠ¨æ€åé¦ˆå¼ºåº¦é…ç½®
    'dynamic_feedback_weights': {
        'min_feedback_weight': 0.1,        # æœ€å°åé¦ˆæƒé‡
        'max_feedback_weight': 2.0,        # æœ€å¤§åé¦ˆæƒé‡ï¼ˆå¯è¶…è¿‡åŸºç¡€è®­ç»ƒï¼‰
        'weight_increment': 0.2,           # æ¯æ¬¡åé¦ˆå¢å¼ºå¹…åº¦
        'consecutive_trigger_boost': 1.5   # è¿ç»­è§¦å‘æ—¶çš„æƒé‡æå‡å€æ•°
    },
    
    # æ–°å¢ï¼šæ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒé…ç½®ï¼ˆåŸºäºé˜ˆå€¼ç›¸å¯¹ä¼˜åŒ–ï¼‰
    'normal_sample_focus': {
        'enable': True,                     # å¯ç”¨æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒ
        'focus_weight': 3.0,               # æ­£å¸¸æ ·æœ¬çš„æŸå¤±æƒé‡å€æ•°
        'threshold_margin': 0.8,           # ç›®æ ‡ï¼šFAI < threshold1 * 0.8 (ä¿æŒ20%å®‰å…¨è¾¹è·)
        'relative_penalty': True,          # å¯ç”¨ç›¸å¯¹é˜ˆå€¼æƒ©ç½š
        'penalty_factor': 5.0              # è¶…å‡ºç›®æ ‡é˜ˆå€¼çš„æƒ©ç½šå› å­
    }
}

# å¤ç”¨Train_Transformer.pyçš„å†…å­˜ç›‘æ§å’Œæ··åˆç²¾åº¦è®¾ç½®
def print_gpu_memory():
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {allocated:.1f}GB / {cached:.1f}GB / {total:.1f}GB (å·²ç”¨/ç¼“å­˜/æ€»è®¡)")

def setup_mixed_precision():
    """è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ"""
    scaler = torch.cuda.amp.GradScaler()
    print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
    return scaler

# å¤ç”¨Train_Transformer.pyçš„æ•°æ®å¤„ç†å‡½æ•°
def check_data_quality(data, name, sample_id=None):
    """è¯¦ç»†çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
    prefix = f"æ ·æœ¬ {sample_id} - " if sample_id else ""
    print(f"\nğŸ” {prefix}{name} æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # æ•°å€¼ç»Ÿè®¡
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    print(f"   æ•°å€¼èŒƒå›´: [{data_np.min():.6f}, {data_np.max():.6f}]")
    print(f"   å‡å€¼: {data_np.mean():.6f}")
    print(f"   æ ‡å‡†å·®: {data_np.std():.6f}")
    print(f"   ä¸­ä½æ•°: {np.median(data_np):.6f}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥
    nan_count = np.isnan(data_np).sum()
    inf_count = np.isinf(data_np).sum()
    zero_count = (data_np == 0).sum()
    negative_count = (data_np < 0).sum()
    
    print(f"   NaNæ•°é‡: {nan_count}")
    print(f"   Infæ•°é‡: {inf_count}")
    print(f"   é›¶å€¼æ•°é‡: {zero_count}")
    print(f"   è´Ÿå€¼æ•°é‡: {negative_count}")
    
    # å¼‚å¸¸å€¼æ¯”ä¾‹
    total_elements = data_np.size
    print(f"   NaNæ¯”ä¾‹: {nan_count/total_elements*100:.2f}%")
    print(f"   Infæ¯”ä¾‹: {inf_count/total_elements*100:.2f}%")
    print(f"   é›¶å€¼æ¯”ä¾‹: {zero_count/total_elements*100:.2f}%")
    print(f"   è´Ÿå€¼æ¯”ä¾‹: {negative_count/total_elements*100:.2f}%")
    
    # å¼‚å¸¸å€¼è­¦å‘Š
    if nan_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°NaNå€¼ï¼")
    if inf_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼")
    if data_np.min() < -1e6 or data_np.max() > 1e6:
        print(f"   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¤§å€¼ï¼èŒƒå›´: [{data_np.min():.2e}, {data_np.max():.2e}]")
    
    return {
        'has_nan': nan_count > 0,
        'has_inf': inf_count > 0,
        'has_extreme_values': data_np.min() < -1e6 or data_np.max() > 1e6,
        'data_type': data.dtype,
        'shape': data.shape
    }

# å¤ç”¨Train_Transformer.pyçš„æ•°æ®é¢„å¤„ç†ï¼ˆé‡è¦ï¼šä¿æŒå®Œå…¨ä¸€è‡´ï¼‰
def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - ç»Ÿä¸€é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
            
    elif feature_type == 'current':
        # ç”µæµç‰©ç†çº¦æŸï¼š-100Aåˆ°100A
        valid_mask = (data_np >= -100) & (data_np <= 100)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            data_np[data_np < -100] = -100
            data_np[data_np > 100] = 100
            
    elif feature_type == 'temperature':
        # æ¸©åº¦ç‰©ç†çº¦æŸï¼š-40Â°Cåˆ°80Â°C
        valid_mask = (data_np >= -40) & (data_np <= 80)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            data_np[data_np < -40] = -40
            data_np[data_np > 80] = 80
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    return data_tensor

# GPUé…ç½®ä¼˜åŒ–ï¼šå°æ ·æœ¬è®­ç»ƒä½¿ç”¨å•GPUé¿å…è·¨å¡é€šä¿¡å¼€é”€
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # åªä½¿ç”¨GPU0
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ å•GPUä¼˜åŒ–é…ç½®:")
    print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i} ({props.name}): {props.total_memory/1024**3:.1f}GB")
    print(f"   ä¸»GPUè®¾å¤‡: cuda:0 (ç‰©ç†GPU0)")
    print(f"   ä¼˜åŒ–æ¨¡å¼: å°æ ·æœ¬è®­ç»ƒï¼Œé¿å…è·¨å¡é€šä¿¡å¼€é”€")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# Linuxç¯å¢ƒmatplotlibé…ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# Linuxç¯å¢ƒå­—ä½“è®¾ç½® - ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
import matplotlib.font_manager as fm
import os

# æ›´å…¨é¢çš„å­—ä½“æ£€æµ‹å’Œè®¾ç½®
def setup_chinese_fonts():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨è‹±æ–‡"""
    # å°è¯•å¤šç§ä¸­æ–‡å­—ä½“
    chinese_fonts = [
        'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC',
        'Noto Sans CJK JP', 'Noto Sans CJK TC', 'Source Han Sans CN',
        'Droid Sans Fallback', 'WenQuanYi Zen Hei', 'AR PL UMing CN'
    ]
    
    # æ£€æŸ¥ç³»ç»Ÿå­—ä½“
    system_fonts = [f.name for f in fm.fontManager.ttflist]
    print(f"ğŸ” ç³»ç»Ÿå¯ç”¨å­—ä½“æ•°é‡: {len(system_fonts)}")
    
    # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_chinese = []
    for font in chinese_fonts:
        if font in system_fonts:
            available_chinese.append(font)
            print(f"âœ… æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {font}")
    
    if available_chinese:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = available_chinese
        plt.rcParams['axes.unicode_minus'] = False
        print(f"ğŸ¨ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {available_chinese[0]}")
        return True
    else:
        # æ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Liberation Sans', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        return False

# è®¾ç½®å­—ä½“
use_chinese = setup_chinese_fonts()
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

#----------------------------------------é‡è¦è¯´æ˜ï¼šæ··åˆåé¦ˆç­–ç•¥æ ¸å¿ƒæ¶æ„------------------------------
# æ··åˆåé¦ˆç­–ç•¥æ¶æ„ï¼š
# 
# é˜¶æ®µ1: åŸºç¡€Transformerè®­ç»ƒ (æ ·æœ¬0-7, epoch 0-20)
# - ä½¿ç”¨QAS 0-7è®­ç»ƒæ ·æœ¬è¿›è¡Œæ ‡å‡†Transformerè®­ç»ƒ
# - ä¸å¯ç”¨åé¦ˆæœºåˆ¶ï¼Œå»ºç«‹åŸºç¡€é¢„æµ‹èƒ½åŠ›
#
# é˜¶æ®µ2: MC-AEè®­ç»ƒ (ä½¿ç”¨Transformerå¢å¼ºæ•°æ®)
# - ä½¿ç”¨Transformeré¢„æµ‹æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]
# - è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹
#
# é˜¶æ®µ3: æ··åˆåé¦ˆè®­ç»ƒ (æ ·æœ¬8-9, epoch 21-40)
# - ä½¿ç”¨QAS 8-9åé¦ˆæ ·æœ¬è¿›è¡Œå‡é˜³æ€§æ£€æµ‹
# - å®æ—¶ç›‘æ§å‡é˜³æ€§ç‡ï¼Œè§¦å‘å¤šçº§åé¦ˆæœºåˆ¶
# - è‡ªé€‚åº”è°ƒæ•´è®­ç»ƒç­–ç•¥å’Œå­¦ä¹ ç‡
#
# é˜¶æ®µ4: PCAåˆ†æå’Œæ¨¡å‹ä¿å­˜
# - ä½¿ç”¨Transformerå¢å¼ºæ•°æ®è®­ç»ƒMC-AE
# - è¿›è¡ŒPCAåˆ†æï¼Œä¿å­˜æ¨¡å‹å’Œå‚æ•°

#----------------------------------------å¤ç”¨Train_Transformer.pyçš„TransformerPredictoræ¨¡å‹------------------------------
class TransformerPredictor(nn.Module):
    """æ—¶åºé¢„æµ‹Transformeræ¨¡å‹ - ç›´æ¥é¢„æµ‹çœŸå®ç‰©ç†å€¼"""
    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=3, output_size=2):
        super(TransformerPredictor, self).__init__()
        self.input_size = input_size
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model, dtype=torch.float32))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚ - ç›´æ¥è¾“å‡ºç‰©ç†å€¼ï¼Œä¸ä½¿ç”¨Sigmoid
        self.output_layer = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model//2, output_size)
            # ç§»é™¤Sigmoidï¼Œç›´æ¥è¾“å‡ºç‰©ç†å€¼
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        # x: [batch, input_size] - 2ç»´è¾“å…¥
        if len(x.shape) == 2:
            # æ·»åŠ åºåˆ—ç»´åº¦ï¼š[batch, input_size] -> [batch, 1, input_size]
            x = x.unsqueeze(1)
        
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch, seq_len, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)
        x = x + pos_enc
        
        # Transformerç¼–ç 
        transformer_out = self.transformer(x)  # [batch, seq_len, d_model]
        
        # è¾“å‡ºå±‚ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        output = self.output_layer(transformer_out[:, -1, :])  # [batch, output_size]
        
        return output  # [batch, output_size] ç›´æ¥è¿”å›2ç»´

#----------------------------------------æ··åˆåé¦ˆç­–ç•¥æ ¸å¿ƒå‡½æ•°------------------------------
def calculate_comprehensive_fault_indicator(sample_data, mcae_net1, mcae_net2, pca_params, device):
    """
    è®¡ç®—ç»¼åˆæ•…éšœæŒ‡ç¤ºå™¨faiï¼ˆåŸºäºComprehensive_calculationï¼‰
    
    å‚æ•°:
        sample_data: æ ·æœ¬æ•°æ® (vin_2, vin_3)
        mcae_net1, mcae_net2: è®­ç»ƒå¥½çš„MC-AEæ¨¡å‹
        pca_params: PCAå‚æ•°å­—å…¸
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        fai: ç»¼åˆæ•…éšœæŒ‡ç¤ºå™¨æ•°ç»„
    """
    mcae_net1.eval()
    mcae_net2.eval()
    
    with torch.no_grad():
        # 1. å‡†å¤‡MC-AEè¾“å…¥æ•°æ®
        vin2_data, vin3_data = sample_data
        
        # ç¡®ä¿æ•°æ®æ˜¯tensorå¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        if not isinstance(vin2_data, torch.Tensor):
            vin2_data = torch.tensor(vin2_data, dtype=torch.float32)
        if not isinstance(vin3_data, torch.Tensor):
            vin3_data = torch.tensor(vin3_data, dtype=torch.float32)
        
        vin2_data = vin2_data.to(device)
        vin3_data = vin3_data.to(device)
        
        # 2. åˆ†å‰²ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        dim_x, dim_y, dim_z = 2, 110, 110
        x_recovered = vin2_data[:, :dim_x]
        y_recovered = vin2_data[:, dim_x:dim_x + dim_y] 
        z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
        q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
        
        dim_x2, dim_y2, dim_z2 = 2, 110, 110
        x_recovered2 = vin3_data[:, :dim_x2]
        y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
        z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
        q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
        
        # 3. MC-AEé‡æ„ï¼ˆç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼‰
        recon_im1, _ = mcae_net1(x_recovered.double(), z_recovered.double(), q_recovered.double())
        recon_im2, _ = mcae_net2(x_recovered2.double(), z_recovered2.double(), q_recovered2.double())
        
        # 4. è®¡ç®—é‡æ„è¯¯å·®ï¼ˆåœ¨CPUä¸Šè¿›è¡Œnumpyæ“ä½œï¼‰
        ERRORU = recon_im1.cpu().detach().numpy() - y_recovered.cpu().detach().numpy()
        ERRORX = recon_im2.cpu().detach().numpy() - y_recovered2.cpu().detach().numpy()
        
        # 5. è¯Šæ–­ç‰¹å¾æå–ï¼ˆå¤ç”¨Function_.pyï¼‰
        df_data = DiagnosisFeature(ERRORU, ERRORX)
        
        # 6. ç»¼åˆè¯Šæ–­è®¡ç®—ï¼ˆå¤ç”¨Comprehensive_calculation.pyï¼‰
        time = np.arange(df_data.shape[0])
        
        try:
            # è°ƒç”¨ç»¼åˆè®¡ç®—å‡½æ•°
            lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
                df_data.values,
                pca_params['data_mean'],
                pca_params['data_std'], 
                pca_params['v'].reshape(len(pca_params['v']), 1),
                pca_params['p_k'],
                pca_params['v_I'],
                pca_params['T_99_limit'],
                pca_params['SPE_99_limit'],
                pca_params['P'],
                time
            )
            
            # æŒ‰ç…§æºä»£ç æ–¹å¼é‡æ–°è®¡ç®—é˜ˆå€¼ï¼ˆä¸Test_.pyä¿æŒä¸€è‡´ï¼‰
            nm = 3000  # å›ºå®šå€¼ï¼Œä¸æºä»£ç ä¸€è‡´
            mm = len(fai)  # æ•°æ®æ€»é•¿åº¦
            
            # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
            if mm > nm:
                # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
                threshold1 = np.mean(fai[nm:mm]) + 3*np.std(fai[nm:mm])
                threshold2 = np.mean(fai[nm:mm]) + 4.5*np.std(fai[nm:mm])
                threshold3 = np.mean(fai[nm:mm]) + 6*np.std(fai[nm:mm])
            else:
                # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
                threshold1 = np.mean(fai) + 3*np.std(fai)
                threshold2 = np.mean(fai) + 4.5*np.std(fai)
                threshold3 = np.mean(fai) + 6*np.std(fai)
            
            return fai, threshold1, threshold2, threshold3
            
        except Exception as e:
            print(f"   âš ï¸ ç»¼åˆè¯Šæ–­è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›åŸºäºé‡æ„è¯¯å·®çš„ç®€å•æŒ‡æ ‡ä½œä¸ºåå¤‡
            simple_fai = np.mean(np.abs(ERRORU), axis=1) + np.mean(np.abs(ERRORX), axis=1)
            # ä½¿ç”¨ç®€å•ç»Ÿè®¡æ–¹æ³•è®¡ç®—åå¤‡é˜ˆå€¼ï¼ˆä»¿ç…§æºä»£ç æ–¹å¼ï¼‰
            nm = 3000
            mm = len(simple_fai)
            if mm > nm:
                default_threshold1 = np.mean(simple_fai[nm:mm]) + 3*np.std(simple_fai[nm:mm])
                default_threshold2 = np.mean(simple_fai[nm:mm]) + 4.5*np.std(simple_fai[nm:mm])
                default_threshold3 = np.mean(simple_fai[nm:mm]) + 6*np.std(simple_fai[nm:mm])
            else:
                default_threshold1 = np.mean(simple_fai) + 3*np.std(simple_fai)
                default_threshold2 = np.mean(simple_fai) + 4.5*np.std(simple_fai)
                default_threshold3 = np.mean(simple_fai) + 6*np.std(simple_fai)
            return simple_fai, default_threshold1, default_threshold2, default_threshold3

def calculate_training_threshold(train_samples, mcae_net1, mcae_net2, pca_params, device):
    """
    åŸºäºè®­ç»ƒæ ·æœ¬è®¡ç®—æ•…éšœæ£€æµ‹é˜ˆå€¼ï¼ˆæŒ‰ç…§æµ‹è¯•è„šæœ¬çš„æ–¹æ³•ï¼‰
    
    å‚æ•°:
        train_samples: è®­ç»ƒæ ·æœ¬IDåˆ—è¡¨
        mcae_net1, mcae_net2: è®­ç»ƒå¥½çš„MC-AEæ¨¡å‹
        pca_params: PCAå‚æ•°å­—å…¸
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        threshold1, threshold2, threshold3: ä¸‰çº§é˜ˆå€¼
    """
    print("ğŸ”§ è®¡ç®—è®­ç»ƒé˜¶æ®µæ•…éšœæ£€æµ‹é˜ˆå€¼...")
    
    all_training_fai = []
    
    for sample_id in train_samples:
        try:
            # åŠ è½½æ ·æœ¬æ•°æ®
            vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
            vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
            
            with open(vin2_path, 'rb') as f:
                vin2_data = pickle.load(f)
            with open(vin3_path, 'rb') as f:
                vin3_data = pickle.load(f)
            
            # æ•°æ®é¢„å¤„ç†
            vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
            vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
            
            # è®¡ç®—è¯¥æ ·æœ¬çš„ç»¼åˆæ•…éšœæŒ‡ç¤ºå™¨
            sample_data = (vin2_processed, vin3_processed)
            fai, _, _, _ = calculate_comprehensive_fault_indicator(sample_data, mcae_net1, mcae_net2, pca_params, device)
            
            all_training_fai.extend(fai)
            
            if (len(all_training_fai) // 1000) > ((len(all_training_fai) - len(fai)) // 1000):
                print(f"   å·²å¤„ç† {len(all_training_fai)} ä¸ªæ•°æ®ç‚¹")
                
        except Exception as e:
            print(f"   âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            continue
    
    all_training_fai = np.array(all_training_fai)
    print(f"   è®­ç»ƒæ•°æ®æ€»è®¡: {len(all_training_fai)} ä¸ªæ•°æ®ç‚¹")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(all_training_fai) == 0:
        print("   âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")
        # ä½¿ç”¨é»˜è®¤é˜ˆå€¼
        default_threshold = 1.0
        return default_threshold, default_threshold * 1.5, default_threshold * 2.0
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«NaNæˆ–Inf
    if np.any(np.isnan(all_training_fai)) or np.any(np.isinf(all_training_fai)):
        print("   âš ï¸ æ£€æµ‹åˆ°NaNæˆ–Infå€¼ï¼Œæ¸…ç†æ•°æ®...")
        all_training_fai = all_training_fai[~np.isnan(all_training_fai)]
        all_training_fai = all_training_fai[~np.isinf(all_training_fai)]
        print(f"   æ¸…ç†åæ•°æ®ç‚¹: {len(all_training_fai)}")
        
        if len(all_training_fai) == 0:
            print("   âŒ æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")
            default_threshold = 1.0
            return default_threshold, default_threshold * 1.5, default_threshold * 2.0
    
    # æŒ‰ç…§æµ‹è¯•è„šæœ¬çš„æ–¹æ³•è®¡ç®—é˜ˆå€¼
    nm = 3000  # å›ºå®šåˆ†å‰²ç‚¹
    mm = len(all_training_fai)
    
    if mm > nm:
        # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—åŸºå‡†ç»Ÿè®¡é‡
        fai_baseline = all_training_fai[nm:mm]
        print(f"   ä½¿ç”¨ååŠæ®µæ•°æ® ({nm}:{mm}) è®¡ç®—é˜ˆå€¼")
    else:
        # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        fai_baseline = all_training_fai
        print(f"   âš ï¸ æ•°æ®é•¿åº¦({mm})ä¸è¶³{nm}ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—é˜ˆå€¼")
    
    # è®¡ç®—ä¸‰çº§é˜ˆå€¼
    fai_mean = np.mean(fai_baseline)
    fai_std = np.std(fai_baseline)
    
    # æ£€æŸ¥ç»Ÿè®¡é‡æ˜¯å¦æœ‰æ•ˆ
    if np.isnan(fai_mean) or np.isnan(fai_std) or fai_std == 0:
        print("   âš ï¸ ç»Ÿè®¡é‡æ— æ•ˆï¼Œä½¿ç”¨æ•°æ®èŒƒå›´è®¡ç®—é˜ˆå€¼")
        fai_range = np.max(fai_baseline) - np.min(fai_baseline)
        fai_mean = np.median(fai_baseline)
        fai_std = fai_range / 6.0  # ä½¿ç”¨èŒƒå›´ä¼°è®¡æ ‡å‡†å·®
    
    threshold1 = fai_mean + 3 * fai_std      # 3Ïƒ
    threshold2 = fai_mean + 4.5 * fai_std    # 4.5Ïƒ  
    threshold3 = fai_mean + 6 * fai_std      # 6Ïƒ
    
    print(f"   é˜ˆå€¼ç»Ÿè®¡: å‡å€¼={fai_mean:.4f}, æ ‡å‡†å·®={fai_std:.4f}")
    print(f"   è®¡ç®—å¾—åˆ°é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    return threshold1, threshold2, threshold3

def calculate_false_positive_rate_comprehensive(feedback_samples, mcae_net1, mcae_net2, 
                                              pca_params, threshold, device):
    """
    åŸºäºç»¼åˆè¯Šæ–­æŒ‡æ ‡è®¡ç®—å‡é˜³æ€§ç‡
    
    å‚æ•°:
        feedback_samples: åé¦ˆæ ·æœ¬IDåˆ—è¡¨ï¼ˆå·²çŸ¥æ­£å¸¸æ ·æœ¬ï¼‰
        mcae_net1, mcae_net2: è®­ç»ƒå¥½çš„MC-AEæ¨¡å‹
        pca_params: PCAå‚æ•°å­—å…¸
        threshold: æ•…éšœæ£€æµ‹é˜ˆå€¼
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        false_positive_rate: å‡é˜³æ€§ç‡
        false_positives: å‡é˜³æ€§æ•°é‡
        total_normals: æ€»æ­£å¸¸æ ·æœ¬æ•°
    """
    print(f"ğŸ” è®¡ç®—åé¦ˆæ ·æœ¬ {feedback_samples} çš„å‡é˜³æ€§ç‡...")
    
    all_fai = []
    
    for sample_id in feedback_samples:  # [8, 9] éƒ½æ˜¯æ­£å¸¸æ ·æœ¬
        try:
            # åŠ è½½æ ·æœ¬æ•°æ®
            vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
            vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
            
            with open(vin2_path, 'rb') as f:
                vin2_data = pickle.load(f)
            with open(vin3_path, 'rb') as f:
                vin3_data = pickle.load(f)
            
            # æ•°æ®é¢„å¤„ç†
            vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
            vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
            
            # è®¡ç®—è¯¥æ ·æœ¬çš„ç»¼åˆæ•…éšœæŒ‡ç¤ºå™¨
            sample_data = (vin2_processed, vin3_processed)
            fai, _, _, _ = calculate_comprehensive_fault_indicator(sample_data, mcae_net1, mcae_net2, pca_params, device)
            
            all_fai.extend(fai)
            print(f"   æ ·æœ¬{sample_id}: {len(fai)}ä¸ªæ•°æ®ç‚¹")
            
        except Exception as e:
            print(f"   âŒ åé¦ˆæ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            continue
    
    if len(all_fai) == 0:
        print("   âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•åé¦ˆæ ·æœ¬æ•°æ®")
        return 0.0, 0, 0
    
    all_fai = np.array(all_fai)
    
    # è®¡ç®—å‡é˜³æ€§ç‡ï¼šæ­£å¸¸æ ·æœ¬ä¸­è¢«è¯¯åˆ¤ä¸ºæ•…éšœçš„æ¯”ä¾‹
    false_positives = (all_fai > threshold).sum()
    total_normals = len(all_fai)
    false_positive_rate = false_positives / total_normals
    
    print(f"   åé¦ˆæ ·æœ¬æ€»è®¡: {total_normals} ä¸ªæ•°æ®ç‚¹")
    print(f"   è¶…è¿‡é˜ˆå€¼({threshold:.4f}): {false_positives} ä¸ª")
    print(f"   å‡é˜³æ€§ç‡: {false_positive_rate:.4f} ({false_positive_rate*100:.2f}%)")
    
    return false_positive_rate, false_positives, total_normals

def detect_feedback_trigger(false_positive_rate, epoch, config, consecutive_triggers=0):
    """
    æ¿€è¿›åé¦ˆè§¦å‘æ£€æµ‹ï¼ˆä¸“æ³¨é™ä½å‡é˜³ç‡ï¼‰
    
    å‚æ•°:
        false_positive_rate: å½“å‰å‡é˜³æ€§ç‡
        epoch: å½“å‰è®­ç»ƒè½®æ•°
        config: åé¦ˆé…ç½®
        consecutive_triggers: è¿ç»­è§¦å‘æ¬¡æ•°
    
    è¿”å›:
        trigger_level: è§¦å‘ç­‰çº§ ('none', 'warning', 'standard', 'enhanced', 'emergency')
        lr_factor: å­¦ä¹ ç‡è°ƒæ•´å› å­
        feedback_weight: åé¦ˆæƒé‡
    """
    thresholds = config['false_positive_thresholds']
    dynamic_config = config['dynamic_feedback_weights']
    
    # åŠ¨æ€è®¡ç®—åé¦ˆæƒé‡ï¼ˆéšè¿ç»­è§¦å‘æ¬¡æ•°å¢å¼ºï¼‰
    base_weight = min(
        dynamic_config['max_feedback_weight'],
        dynamic_config['min_feedback_weight'] + consecutive_triggers * dynamic_config['weight_increment']
    )
    
    # è¿ç»­è§¦å‘æ—¶çš„æƒé‡æå‡
    if consecutive_triggers > 0:
        consecutive_boost = dynamic_config['consecutive_trigger_boost']
        base_weight *= (1 + consecutive_boost * min(consecutive_triggers / 3, 1.0))
    
    # æ¿€è¿›åé¦ˆç­–ç•¥ï¼šæä½é˜ˆå€¼è§¦å‘
    if false_positive_rate >= thresholds['emergency']:
        # ç´§æ€¥åé¦ˆï¼šå‡é˜³ç‡ >= 1%
        trigger_level = 'emergency'
        lr_factor = config['adaptive_lr_factors']['emergency']
        feedback_weight = min(base_weight * 2.0, dynamic_config['max_feedback_weight'])
    elif false_positive_rate >= thresholds['enhanced']:
        # å¼ºåŒ–åé¦ˆï¼šå‡é˜³ç‡ >= 0.5%
        trigger_level = 'enhanced'
        lr_factor = config['adaptive_lr_factors']['enhanced']
        feedback_weight = min(base_weight * 1.5, dynamic_config['max_feedback_weight'])
    elif false_positive_rate >= thresholds['standard']:
        # æ ‡å‡†åé¦ˆï¼šå‡é˜³ç‡ >= 0.2%
        trigger_level = 'standard'
        lr_factor = config['adaptive_lr_factors']['standard']
        feedback_weight = base_weight
    elif false_positive_rate >= thresholds['warning']:
        # è½»åº¦åé¦ˆï¼šå‡é˜³ç‡ >= 0.1%ï¼ˆä¸å†ä»…è®°å½•ï¼Œå¼€å§‹è½»åº¦å¹²é¢„ï¼‰
        trigger_level = 'warning'
        lr_factor = 0.8  # è½»åº¦å­¦ä¹ ç‡è°ƒæ•´
        feedback_weight = base_weight * 0.3
    else:
        trigger_level = 'none'
        lr_factor = 1.0
        feedback_weight = 0.0
    
    return trigger_level, lr_factor, feedback_weight

def prepare_feedback_data(feedback_samples, device, batch_size=1000):
    """
    å‡†å¤‡åé¦ˆæ•°æ®
    
    å‚æ•°:
        feedback_samples: åé¦ˆæ ·æœ¬IDåˆ—è¡¨ [8, 9]
        device: è®¡ç®—è®¾å¤‡
        batch_size: æ‰¹æ¬¡å¤§å°
    
    è¿”å›:
        feedback_data: (vin1_batch, targets_batch) ç”¨äºåé¦ˆè®¡ç®—
    """
    try:
        print(f"ğŸ”§ å‡†å¤‡åé¦ˆæ•°æ®ï¼ˆæ ·æœ¬ {feedback_samples}ï¼‰...")
        
        all_vin1_data = []
        all_targets = []
        sample_lengths = []
        
        for sample_id in feedback_samples:
            # åŠ è½½vin_1æ•°æ®
            vin1_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_1.pkl'
            with open(vin1_path, 'rb') as f:
                vin1_data = pickle.load(f)
                if isinstance(vin1_data, torch.Tensor):
                    vin1_data = vin1_data.cpu()
                else:
                    vin1_data = torch.tensor(vin1_data)
                
                # è®°å½•æ•°æ®é•¿åº¦å’Œæ ¼å¼
                sample_length = len(vin1_data)
                sample_lengths.append(sample_length)
                print(f"   æ ·æœ¬{sample_id}: {sample_length}ä¸ªæ•°æ®ç‚¹, æ ¼å¼{vin1_data.shape}")
                
                all_vin1_data.append(vin1_data)
            
            # åŠ è½½ç›®æ ‡æ•°æ®
            targets_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/targets.pkl'
            with open(targets_path, 'rb') as f:
                targets = pickle.load(f)
                terminal_voltages = np.array(targets['terminal_voltages'])
                pack_socs = np.array(targets['pack_socs'])
                
                print(f"   æ ·æœ¬{sample_id} targets: ç”µå‹{len(terminal_voltages)}ç‚¹, SOC{len(pack_socs)}ç‚¹")
                
                # ç»„åˆç›®æ ‡æ•°æ®ï¼šä¸‹ä¸€æ—¶åˆ»çš„ç”µå‹å’ŒSOC
                targets_combined = np.column_stack([terminal_voltages[1:], pack_socs[1:]])
                targets_tensor = torch.tensor(targets_combined, dtype=torch.float32)
                all_targets.append(targets_tensor)
        
        # æ£€æŸ¥æ•°æ®é•¿åº¦ä¸€è‡´æ€§
        if len(set(sample_lengths)) > 1:
            print(f"   âš ï¸ è­¦å‘Š: æ ·æœ¬é•¿åº¦ä¸ä¸€è‡´ {sample_lengths}")
            print(f"   ä½¿ç”¨æœ€å°é•¿åº¦: {min(sample_lengths)}")
            
            # ç»Ÿä¸€æˆªå–åˆ°æœ€å°é•¿åº¦
            min_length = min(sample_lengths)
            for i in range(len(all_vin1_data)):
                all_vin1_data[i] = all_vin1_data[i][:min_length]
                all_targets[i] = all_targets[i][:min_length-1]  # targetså°‘ä¸€ä¸ªç‚¹
        
        # åˆ†åˆ«å¤„ç†æ¯ä¸ªæ ·æœ¬ï¼Œé¿å…é•¿åº¦ä¸åŒ¹é…é—®é¢˜
        all_feedback_inputs = []
        all_feedback_targets = []
        
        for i, (vin1_data, targets_data) in enumerate(zip(all_vin1_data, all_targets)):
            # æ„å»ºè¾“å…¥æ•°æ®ï¼švin_1å‰5ç»´ + å½“å‰æ—¶åˆ»çœŸå®ç”µå‹ + å½“å‰æ—¶åˆ»çœŸå®SOC
            feedback_inputs = torch.zeros(len(vin1_data), 7, dtype=torch.float32)
            
            # æ ¹æ®æ•°æ®æ ¼å¼è°ƒæ•´ç´¢å¼•
            if len(vin1_data.shape) == 3:  # [time, 1, features]
                feedback_inputs[:, 0:5] = vin1_data[:, 0, 0:5]  # vin_1å‰5ç»´
            elif len(vin1_data.shape) == 2:  # [time, features]
                feedback_inputs[:, 0:5] = vin1_data[:, 0:5]  # vin_1å‰5ç»´
            else:
                print(f"   âš ï¸ æœªçŸ¥çš„vin1_dataæ ¼å¼: {vin1_data.shape}")
                continue
            
            # æ·»åŠ å½“å‰æ—¶åˆ»çš„çœŸå®å€¼ï¼ˆä»targetsä¸­è·å–å‰ä¸€æ—¶åˆ»çš„å€¼ï¼‰
            if len(targets_data) > 0:
                # ç¡®ä¿ç»´åº¦åŒ¹é…ï¼šfeedback_inputs[1:] å¯¹åº” targets_data[:-1]
                # å› ä¸ºfeedback_inputs[0]å¯¹åº”targets_data[0]ï¼Œä½†feedback_inputs[1]å¯¹åº”targets_data[0]
                feedback_inputs[1:, 5] = targets_data[:-1, 0]  # å½“å‰æ—¶åˆ»ç”µå‹
                feedback_inputs[1:, 6] = targets_data[:-1, 1]  # å½“å‰æ—¶åˆ»SOC
                
                # å¯¹åº”çš„ç›®æ ‡æ˜¯ä¸‹ä¸€æ—¶åˆ»çš„å€¼
                feedback_targets = targets_data[1:]
                feedback_inputs = feedback_inputs[1:]
                
                # ç¡®ä¿æˆªæ–­åçš„ç»´åº¦åŒ¹é…
                min_length = min(len(feedback_inputs), len(feedback_targets))
                if min_length > 0:
                    feedback_inputs = feedback_inputs[:min_length]
                    feedback_targets = feedback_targets[:min_length]
                    
                    all_feedback_inputs.append(feedback_inputs)
                    all_feedback_targets.append(feedback_targets)
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
        if all_feedback_inputs:
            combined_inputs = torch.cat(all_feedback_inputs, dim=0)
            combined_targets = torch.cat(all_feedback_targets, dim=0)
            
            # éšæœºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡
            total_samples = len(combined_inputs)
            if total_samples > batch_size:
                indices = torch.randperm(total_samples)[:batch_size]
                combined_inputs = combined_inputs[indices]
                combined_targets = combined_targets[indices]
            
            print(f"   âœ… åé¦ˆæ•°æ®å‡†å¤‡å®Œæˆ: è¾“å…¥{combined_inputs.shape}, ç›®æ ‡{combined_targets.shape}")
            return (combined_inputs, combined_targets)
        else:
            print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„åé¦ˆæ•°æ®")
            return None
        
    except Exception as e:
        print(f"   âŒ åé¦ˆæ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return None

def apply_normal_sample_focus_training(transformer, feedback_data, optimizer, criterion, config, device, current_threshold=None):
    """
    æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒ - åŸºäºé˜ˆå€¼ç›¸å¯¹ä¼˜åŒ–ï¼Œä½¿FAIä½äºthreshold1
    
    å‚æ•°:
        transformer: Transformeræ¨¡å‹
        feedback_data: æ­£å¸¸æ ·æœ¬åé¦ˆæ•°æ®
        optimizer: ä¼˜åŒ–å™¨
        criterion: æŸå¤±å‡½æ•°
        config: é…ç½®
        device: è®¡ç®—è®¾å¤‡
        current_threshold: å½“å‰threshold1å€¼
    
    è¿”å›:
        focus_loss: ç‰¹åŒ–è®­ç»ƒæŸå¤±
        avg_prediction_error: å¹³å‡é¢„æµ‹è¯¯å·®
        threshold_info: é˜ˆå€¼ç›¸å…³ä¿¡æ¯
    """
    if not config['normal_sample_focus']['enable'] or feedback_data is None:
        return torch.tensor(0.0, device=device), 0.0, "æœªå¯ç”¨"
    
    try:
        vin1_batch, targets_batch = feedback_data
        vin1_batch = vin1_batch.to(device)
        targets_batch = targets_batch.to(device)
        
        transformer.train()
        
        # å‰å‘ä¼ æ’­
        predictions = transformer(vin1_batch)
        
        # åŸºç¡€é¢„æµ‹æŸå¤±
        base_loss = criterion(predictions, targets_batch)
        
        # è®¡ç®—é¢„æµ‹è¯¯å·®ï¼ˆç”¨äºFAIä¼°ç®—ï¼‰
        prediction_errors = torch.abs(predictions - targets_batch)
        avg_prediction_error = prediction_errors.mean().item()
        
        # åŸºäºé˜ˆå€¼çš„ç›¸å¯¹ä¼˜åŒ–
        if current_threshold is not None and config['normal_sample_focus']['relative_penalty']:
            # ç›®æ ‡ï¼šFAI < threshold1 * marginï¼ˆä¾‹å¦‚ï¼š< threshold1 * 0.8ï¼‰
            target_threshold = current_threshold * config['normal_sample_focus']['threshold_margin']
            
            # é¢„æµ‹è¯¯å·®è¶Šå¤§ï¼ŒFAIè¶Šå¯èƒ½è¶…è¿‡ç›®æ ‡é˜ˆå€¼ï¼Œæ–½åŠ ç›¸å¯¹æƒ©ç½š
            # ä½¿ç”¨sigmoidå‡½æ•°å¹³æ»‘æƒ©ç½šï¼Œé¿å…æ¢¯åº¦çªå˜
            error_ratio = prediction_errors / target_threshold
            relative_penalty = torch.sigmoid(error_ratio - 1.0) * config['normal_sample_focus']['penalty_factor']
            threshold_penalty = torch.mean(relative_penalty * prediction_errors)
            
            threshold_info = f"ç›®æ ‡é˜ˆå€¼={target_threshold:.4f}, å½“å‰è¯¯å·®={avg_prediction_error:.4f}"
        else:
            # å¦‚æœæ²¡æœ‰é˜ˆå€¼ä¿¡æ¯ï¼Œä½¿ç”¨åŸºç¡€FAIæƒ©ç½š
            threshold_penalty = torch.mean(prediction_errors * config['normal_sample_focus']['penalty_factor'])
            threshold_info = f"åŸºç¡€æƒ©ç½šæ¨¡å¼, é¢„æµ‹è¯¯å·®={avg_prediction_error:.4f}"
        
        # æ€»æŸå¤± = åŸºç¡€æŸå¤± * æƒé‡ + é˜ˆå€¼ç›¸å¯¹æƒ©ç½š
        focus_weight = config['normal_sample_focus']['focus_weight']
        total_loss = base_loss * focus_weight + threshold_penalty
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        return total_loss, avg_prediction_error, threshold_info
        
    except Exception as e:
        print(f"   âŒ æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒå¤±è´¥: {e}")
        return torch.tensor(0.0, device=device), 0.0, f"å¤±è´¥: {e}"

def apply_hybrid_feedback(transformer, mcae_net1, mcae_net2, feedback_data, 
                         feedback_weight, mcae_weight, transformer_weight, device):
    """
    åº”ç”¨æ··åˆåé¦ˆæœºåˆ¶ï¼ˆåŸºäºå®é™…é¢„æµ‹è¯¯å·®ï¼‰
    
    å‚æ•°:
        transformer: Transformeræ¨¡å‹
        mcae_net1, mcae_net2: MC-AEæ¨¡å‹
        feedback_data: åé¦ˆæ•°æ® (vin1_batch, targets_batch)
        feedback_weight: åé¦ˆå¼ºåº¦æƒé‡
        mcae_weight: MC-AEæƒé‡
        transformer_weight: Transformeræƒé‡
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        feedback_loss: åé¦ˆæŸå¤±
        feedback_info: åé¦ˆä¿¡æ¯
    """
    if feedback_weight == 0.0 or feedback_data is None:
        return torch.tensor(0.0, device=device), "æ— åé¦ˆ"
    
    try:
        vin1_batch, targets_batch = feedback_data
        vin1_batch = vin1_batch.to(device)
        targets_batch = targets_batch.to(device)
        
        # è®¡ç®—Transformeråœ¨åé¦ˆæ ·æœ¬ä¸Šçš„é¢„æµ‹è¯¯å·®
        transformer.eval()
        with torch.no_grad():
            pred_output = transformer(vin1_batch)
            
        # è®¡ç®—é¢„æµ‹è¯¯å·®ï¼ˆMSEï¼‰
        prediction_error = torch.nn.functional.mse_loss(pred_output, targets_batch)
        
        # åŸºäºå®é™…é¢„æµ‹è¯¯å·®è®¡ç®—åé¦ˆæŸå¤±
        # è¯¯å·®è¶Šå¤§ï¼Œåé¦ˆæŸå¤±è¶Šå¤§ï¼Œè®­ç»ƒè°ƒæ•´è¶Šå¼ºçƒˆ
        feedback_loss = prediction_error * feedback_weight
        
        # æ·»åŠ æ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢è¿‡åº¦è°ƒæ•´
        l2_reg = sum(p.pow(2.0).sum() for p in transformer.parameters())
        regularization_loss = 1e-6 * l2_reg
        
        total_feedback_loss = feedback_loss + regularization_loss
        
        feedback_info = f"é¢„æµ‹è¯¯å·®: {prediction_error:.6f}, åé¦ˆæƒé‡: {feedback_weight:.2f}, åé¦ˆæŸå¤±: {feedback_loss:.6f}"
        
        return total_feedback_loss, feedback_info
        
    except Exception as e:
        print(f"   âš ï¸ åé¦ˆè®¡ç®—å¤±è´¥: {e}")
        # é™çº§ä¸ºç®€åŒ–åé¦ˆ
        fallback_loss = torch.tensor(feedback_weight * 0.001, device=device)
        return fallback_loss, f"é™çº§åé¦ˆ: {feedback_weight:.2f}"

#----------------------------------------ä¸»è®­ç»ƒå‡½æ•°------------------------------
def main():
    """æ··åˆåé¦ˆç­–ç•¥ä¸»è®­ç»ƒå‡½æ•°"""
    print("="*80)
    print("ğŸš€ æ··åˆåé¦ˆç­–ç•¥Transformerè®­ç»ƒ - Linuxç¯å¢ƒç‰ˆæœ¬")
    print("="*80)
    
    # Linuxç¯å¢ƒæ£€æŸ¥
    import platform
    print(f"ğŸ–¥ï¸  è¿è¡Œç¯å¢ƒ: {platform.system()} {platform.release()}")
    print(f"ğŸ Pythonç‰ˆæœ¬: {platform.python_version()}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"ğŸš€ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ”¢ GPUæ•°é‡: {torch.cuda.device_count()}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    #----------------------------------------é…ç½®æ˜¾ç¤º------------------------------
    print("\n" + "="*60)
    print("âš™ï¸  æ··åˆåé¦ˆç­–ç•¥é…ç½®")
    print("="*60)
    config = HYBRID_FEEDBACK_CONFIG
    print(f"ğŸ“Š æ•°æ®åˆ†ç»„:")
    print(f"   è®­ç»ƒæ ·æœ¬: {config['train_samples']} (QAS 0-7)")
    print(f"   åé¦ˆæ ·æœ¬: {config['feedback_samples']} (QAS 8-9)")
    print(f"ğŸ”§ æ¿€è¿›åé¦ˆæœºåˆ¶ï¼ˆä¸“æ³¨é™ä½å‡é˜³ç‡ï¼‰:")
    print(f"   åé¦ˆé¢‘ç‡: æ¯{config['feedback_frequency']}ä¸ªepoch ï¼ˆå¤§å¹…æé«˜ï¼‰")
    print(f"   åé¦ˆå¯åŠ¨è½®æ•°: ç¬¬{config['feedback_start_epoch']}è½® ï¼ˆæå‰ä»‹å…¥ï¼‰")
    print(f"   å‡é˜³æ€§é˜ˆå€¼ï¼ˆæä¸¥æ ¼ï¼‰: {config['false_positive_thresholds']}")
    print(f"   è‡ªé€‚åº”å­¦ä¹ ç‡å› å­ï¼ˆæ¿€è¿›ï¼‰: {config['adaptive_lr_factors']}")
    print(f"   MC-AEæƒé‡: {config['mcae_weight']}, Transformeræƒé‡: {config['transformer_weight']}")
    print(f"   åŠ¨æ€åé¦ˆæƒé‡: {config['dynamic_feedback_weights']}")
    print(f"   æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒ: ç›®æ ‡FAI < threshold1 * {config['normal_sample_focus']['threshold_margin']}")
    
    #----------------------------------------é˜¶æ®µ1: åŸºç¡€Transformerè®­ç»ƒ------------------------------
    print("\n" + "="*60)
    print("ğŸ¯ é˜¶æ®µ1: åŸºç¡€Transformerè®­ç»ƒï¼ˆæ ·æœ¬0-7ï¼‰")
    print("="*60)
    
    # åŠ è½½è®­ç»ƒæ ·æœ¬
    train_samples = config['train_samples']
    print(f"ğŸ“Š ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªè®­ç»ƒæ ·æœ¬: {train_samples}")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ”§ GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"ğŸ”§ å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    
    # éªŒè¯è®­ç»ƒæ ·æœ¬æ•°æ®æ˜¯å¦å­˜åœ¨
    print("\nğŸ” éªŒè¯è®­ç»ƒæ ·æœ¬æ•°æ®...")
    valid_samples = []
    for sample_id in train_samples:
        vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
        vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
        
        if os.path.exists(vin2_path) and os.path.exists(vin3_path):
            valid_samples.append(sample_id)
            print(f"   âœ… æ ·æœ¬ {sample_id}: æ•°æ®æ–‡ä»¶å­˜åœ¨")
        else:
            print(f"   âŒ æ ·æœ¬ {sample_id}: æ•°æ®æ–‡ä»¶ç¼ºå¤±")
    
    if len(valid_samples) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬æ•°æ®")
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„: /mnt/bz25t/bzhy/zhanglikang/project/QAS/")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(valid_samples)} ä¸ªæœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {valid_samples}")
    
    # ä½¿ç”¨å¤ç”¨çš„æ•°æ®åŠ è½½å™¨
    print("\nğŸ“¥ åŠ è½½é¢„è®¡ç®—æ•°æ®...")
    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = TransformerBatteryDataset(data_path='/mnt/bz25t/bzhy/zhanglikang/project/QAS', sample_ids=valid_samples)
        
        if len(dataset) == 0:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è®­ç»ƒæ•°æ®")
            print("è¯·ç¡®ä¿å·²è¿è¡Œ precompute_targets.py ç”Ÿæˆé¢„è®¡ç®—æ•°æ®")
            return
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} ä¸ªè®­ç»ƒæ•°æ®å¯¹")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        BATCH_SIZE = 4000  # ä»2000å¢åŠ åˆ°4000
        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, 
                                num_workers=4, pin_memory=True)
        print(f"ğŸ“¦ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹æ¬¡å¤§å°: {BATCH_SIZE}, num_workers: 4")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        sample_input, sample_target = dataset[0]
        print(f"ğŸ“Š æ•°æ®æ ¼å¼:")
        print(f"   è¾“å…¥ç»´åº¦: {sample_input.shape} (å‰5ç»´vin_1 + ç”µå‹ + SOC)")
        print(f"   ç›®æ ‡ç»´åº¦: {sample_target.shape} (ä¸‹ä¸€æ—¶åˆ»ç”µå‹ + SOC)")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²è¿è¡Œ precompute_targets.py ç”Ÿæˆé¢„è®¡ç®—æ•°æ®")
        return
    
    # åˆå§‹åŒ–Transformeræ¨¡å‹ï¼ˆç²¾ç®€é…ç½®ï¼ŒåŒ¹é…ä¿å®ˆè®­ç»ƒå‚æ•°ï¼‰
    transformer = TransformerPredictor(
        input_size=7,      # vin_1å‰5ç»´ + ç”µå‹ + SOC
        d_model=128,       # æ¨¡å‹ç»´åº¦ï¼ˆç²¾ç®€è§„æ¨¡ï¼‰
        nhead=8,           # æ³¨æ„åŠ›å¤´æ•°ï¼ˆç²¾ç®€è§„æ¨¡ï¼‰
        num_layers=3,      # Transformerå±‚æ•°ï¼ˆç²¾ç®€è§„æ¨¡ï¼‰
        output_size=2      # è¾“å‡ºï¼šç”µå‹ + SOC
    ).to(device).float()
    
    # å•GPUä¼˜åŒ–æ¨¡å¼ï¼ˆå°æ ·æœ¬è®­ç»ƒï¼Œé¿å…è·¨å¡é€šä¿¡å¼€é”€ï¼‰
    print("ğŸ”§ å•GPUä¼˜åŒ–æ¨¡å¼ï¼šé¿å…æ•°æ®å¹¶è¡Œå¼€é”€ï¼Œä¸“æ³¨äºå°æ ·æœ¬è®­ç»ƒ")
    
    print(f"ğŸ§  Transformeræ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in transformer.parameters()):,}")
    
    # è®­ç»ƒå‚æ•°è®¾ç½®
    LR = 1.5e-3            # å­¦ä¹ ç‡ä»1e-3å¢åŠ åˆ°1.5e-3
    EPOCH_PHASE1 = config['feedback_start_epoch']  # é˜¶æ®µ1è®­ç»ƒè½®æ•°
    EPOCH_PHASE2 = 120     # é˜¶æ®µ2æ€»è½®æ•°ï¼ˆä¿®æ­£ï¼šå¿…é¡»å¤§äºEPOCH_PHASE1ï¼‰
    lr_decay_freq = 15     # å­¦ä¹ ç‡è¡°å‡é¢‘ç‡ä»10å¢åŠ åˆ°15
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(transformer.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_freq, gamma=0.9)
    criterion = nn.MSELoss()
    
    # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = setup_mixed_precision()
    
    print(f"âš™ï¸  è®­ç»ƒå‚æ•°é…ç½®ï¼ˆç²¾ç®€æ¨¡å‹ + ä¿å®ˆè®­ç»ƒ + æ”¹è¿›åé¦ˆï¼‰:")
    print(f"   æ¨¡å‹è§„æ¨¡: d_model=128, nhead=8, layers=3ï¼ˆç²¾ç®€é…ç½®ï¼‰")
    print(f"   å­¦ä¹ ç‡: {LR}ï¼ˆä¿å®ˆå­¦ä¹ ç‡ï¼ŒåŒ¹é…ç²¾ç®€æ¨¡å‹ï¼‰")
    print(f"   é˜¶æ®µ1è®­ç»ƒè½®æ•°: {EPOCH_PHASE1}")
    print(f"   æ€»è®­ç»ƒè½®æ•°: {EPOCH_PHASE2}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   å­¦ä¹ ç‡è¡°å‡é¢‘ç‡: {lr_decay_freq}")
    print(f"   æ··åˆç²¾åº¦è®­ç»ƒ: å¯ç”¨")
    print(f"   åé¦ˆæœºåˆ¶: åŸºäºå®é™…é¢„æµ‹è¯¯å·® + æ›´ä¸¥æ ¼é˜ˆå€¼")
    
    # å¼€å§‹é˜¶æ®µ1è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹é˜¶æ®µ1è®­ç»ƒ...")
    transformer.train()
    train_losses_phase1 = []
    
    for epoch in range(EPOCH_PHASE1):
        epoch_loss = 0
        batch_count = 0
        
        for batch_input, batch_target in train_loader:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            batch_input = batch_input.to(device)
            batch_target = batch_target.to(device)
            
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                pred_output = transformer(batch_input)
                loss = criterion(pred_output, batch_target)
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += loss.item()
            batch_count += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = epoch_loss / batch_count
        train_losses_phase1.append(avg_loss)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if epoch % 5 == 0 or epoch == EPOCH_PHASE1 - 1:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'é˜¶æ®µ1 Epoch: {epoch:3d} | Loss: {avg_loss:.6f} | LR: {current_lr:.6f}')
    
    print(f"\nâœ… é˜¶æ®µ1è®­ç»ƒå®Œæˆ! æœ€ç»ˆæŸå¤±: {train_losses_phase1[-1]:.6f}")
    
    #----------------------------------------é˜¶æ®µ2: MC-AEè®­ç»ƒï¼ˆå¤ç”¨Train_Transformer.pyé€»è¾‘ï¼‰------------------------------
    print("\n" + "="*60)
    print("ğŸ”„ é˜¶æ®µ2: åŠ è½½vin_2å’Œvin_3æ•°æ®ï¼Œè¿›è¡ŒTransformeré¢„æµ‹æ›¿æ¢")
    print("="*60)
    
    # å¤ç”¨Train_Transformer.pyçš„é€»è¾‘åŠ è½½å’Œå¤„ç†æ•°æ®
    all_vin1_data = []
    all_vin2_data = []
    all_vin3_data = []
    
    print("ğŸ“¥ åŠ è½½åŸå§‹vin_2å’Œvin_3æ•°æ®...")
    processed_count = 0
    failed_count = 0
    
    for sample_id in train_samples:
        try:
            # åŠ è½½vin_1æ•°æ®ï¼ˆç”¨äºTransformeré¢„æµ‹ï¼‰
            vin1_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_1.pkl'
            with open(vin1_path, 'rb') as file:
                vin1_data = pickle.load(file)
                if isinstance(vin1_data, torch.Tensor):
                    vin1_data = vin1_data.cpu()
                else:
                    vin1_data = torch.tensor(vin1_data)
                all_vin1_data.append(vin1_data)
            
            # åŠ è½½vin_2æ•°æ®å¹¶è¿›è¡Œé™é»˜å¤„ç†
            vin2_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_2.pkl'
            with open(vin2_path, 'rb') as file:
                vin2_data = pickle.load(file)
            
            # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            vin2_processed = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
            all_vin2_data.append(vin2_processed)
            
            # åŠ è½½vin_3æ•°æ®å¹¶è¿›è¡Œé™é»˜å¤„ç†
            vin3_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/vin_3.pkl'
            with open(vin3_path, 'rb') as file:
                vin3_data = pickle.load(file)
            
            # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            vin3_processed = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
            all_vin3_data.append(vin3_processed)
            
            processed_count += 1
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if processed_count % 5 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {processed_count}/{len(train_samples)} ä¸ªæ ·æœ¬")
                
        except Exception as e:
            print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
            continue
    
    # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(train_samples)}")
    print(f"   æˆåŠŸå¤„ç†: {processed_count}")
    print(f"   å¤„ç†å¤±è´¥: {failed_count}")
    print(f"   æˆåŠŸç‡: {processed_count/len(train_samples)*100:.1f}%")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_vin1 = torch.cat(all_vin1_data, dim=0).float()
    combined_vin2 = torch.cat(all_vin2_data, dim=0).float()
    combined_vin3 = torch.cat(all_vin3_data, dim=0).float()
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®å½¢çŠ¶:")
    print(f"   vin_1: {combined_vin1.shape}")
    print(f"   vin_2: {combined_vin2.shape}")
    print(f"   vin_3: {combined_vin3.shape}")
    
    # ä½¿ç”¨Transformerè¿›è¡Œé¢„æµ‹å’Œæ›¿æ¢ï¼ˆå¤ç”¨Train_Transformer.pyé€»è¾‘ï¼‰
    print("\nğŸ”„ ä½¿ç”¨Transformeré¢„æµ‹æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]...")
    transformer.eval()
    
    # é¢„å…ˆåŠ è½½æ‰€æœ‰targetsæ•°æ®ï¼ˆä¼˜åŒ–I/Oï¼‰
    print("ğŸ“¥ é¢„å…ˆåŠ è½½æ‰€æœ‰targetsæ•°æ®...")
    all_targets = {}
    for sample_id in train_samples:
        targets_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}/targets.pkl'
        with open(targets_path, 'rb') as f:
            all_targets[sample_id] = pickle.load(f)
    print(f"âœ… å·²åŠ è½½ {len(all_targets)} ä¸ªæ ·æœ¬çš„targetsæ•°æ®")
    
    # æ‰¹é‡æ„å»ºTransformerè¾“å…¥æ•°æ®
    print("ğŸ”§ æ‰¹é‡æ„å»ºTransformerè¾“å…¥æ•°æ®...")
    transformer_inputs = torch.zeros(len(combined_vin1), 7, dtype=torch.float32)
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„èµ·å§‹å’Œç»“æŸç´¢å¼•
    sample_indices = []
    current_idx = 0
    for sample_idx, sample_id in enumerate(train_samples):
        sample_len = len(all_vin1_data[sample_idx])
        sample_indices.append((current_idx, current_idx + sample_len, sample_id))
        current_idx += sample_len
    
    # æ‰¹é‡å¡«å……è¾“å…¥æ•°æ®
    for start_idx, end_idx, sample_id in sample_indices:
        targets = all_targets[sample_id]
        terminal_voltages = np.array(targets['terminal_voltages'])
        pack_socs = np.array(targets['pack_socs'])
        
        # å¡«å……vin_1å‰5ç»´
        transformer_inputs[start_idx:end_idx, 0:5] = combined_vin1[start_idx:end_idx, 0, 0:5]
        # å¡«å……å½“å‰æ—¶åˆ»çœŸå®ç”µå‹
        transformer_inputs[start_idx:end_idx, 5] = torch.tensor(terminal_voltages[:end_idx-start_idx], dtype=torch.float32)
        # å¡«å……å½“å‰æ—¶åˆ»çœŸå®SOC
        transformer_inputs[start_idx:end_idx, 6] = torch.tensor(pack_socs[:end_idx-start_idx], dtype=torch.float32)
    
    print(f"âœ… è¾“å…¥æ•°æ®æ„å»ºå®Œæˆ: {transformer_inputs.shape}")
    
    # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    print("ğŸ“Š GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
    print_gpu_memory()
    
    # æ‰¹é‡é¢„æµ‹ï¼ˆä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼‰
    print("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹...")
    transformer_inputs = transformer_inputs.to(device)
    
    with torch.no_grad():
        # ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜GPUåˆ©ç”¨ç‡
        batch_size = 15000  # ä»10000å¢åŠ åˆ°15000
        predictions = []
        total_batches = (len(transformer_inputs) + batch_size - 1) // batch_size
        
        for i in range(0, len(transformer_inputs), batch_size):
            batch_idx = i // batch_size + 1
            batch_data = transformer_inputs[i:i+batch_size]
            batch_pred = transformer(batch_data)
            predictions.append(batch_pred.cpu())
            
            # æ˜¾ç¤ºè¿›åº¦
            if batch_idx % 5 == 0 or batch_idx == total_batches:
                print(f"   è¿›åº¦: {batch_idx}/{total_batches} ({batch_idx/total_batches*100:.1f}%)")
        
        transformer_predictions = torch.cat(predictions, dim=0)
    
    print(f"âœ… Transformeré¢„æµ‹å®Œæˆ: {transformer_predictions.shape}")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†ï¼Œå½“å‰ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
    
    # æ›¿æ¢vin_2[:,0]å’Œvin_3[:,0]
    vin2_modified = combined_vin2.clone()
    vin3_modified = combined_vin3.clone()
    
    # ç¡®ä¿é•¿åº¦åŒ¹é…
    min_len = min(len(transformer_predictions), len(vin2_modified), len(vin3_modified))
    
    # æ›¿æ¢BiLSTMé¢„æµ‹å€¼ä¸ºTransformeré¢„æµ‹å€¼
    vin2_modified[:min_len, 0] = transformer_predictions[:min_len, 0]  # ç”µå‹é¢„æµ‹
    vin3_modified[:min_len, 0] = transformer_predictions[:min_len, 1]  # SOCé¢„æµ‹
    
    print(f"ğŸ”„ æ›¿æ¢å®Œæˆ:")
    print(f"   åŸå§‹vin_2[:,0]èŒƒå›´: [{combined_vin2[:, 0].min():.4f}, {combined_vin2[:, 0].max():.4f}]")
    print(f"   Transformer vin_2[:,0]èŒƒå›´: [{vin2_modified[:, 0].min():.4f}, {vin2_modified[:, 0].max():.4f}]")
    print(f"   åŸå§‹vin_3[:,0]èŒƒå›´: [{combined_vin3[:, 0].min():.4f}, {combined_vin3[:, 0].max():.4f}]")
    print(f"   Transformer vin_3[:,0]èŒƒå›´: [{vin3_modified[:, 0].min():.4f}, {vin3_modified[:, 0].max():.4f}]")
    
    # è®­ç»ƒMC-AEæ¨¡å‹ï¼ˆå¤ç”¨Train_Transformer.pyçš„å®Œæ•´é€»è¾‘ï¼‰
    print("\nğŸ§  è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨Transformerå¢å¼ºæ•°æ®ï¼‰...")
    
    # å¤ç”¨Class_.pyä¸­çš„CombinedAEå’ŒFunction_.pyä¸­çš„custom_activation
    from Function_ import custom_activation
    
    # å®šä¹‰ç‰¹å¾åˆ‡ç‰‡ç»´åº¦ï¼ˆä¸Train_Transformer.pyä¸€è‡´ï¼‰
    # vin_2.pkl
    dim_x = 2
    dim_y = 110
    dim_z = 110
    dim_q = 3
    
    # åˆ†å‰²vin_2ç‰¹å¾å¼ é‡
    x_recovered = vin2_modified[:, :dim_x]
    y_recovered = vin2_modified[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_modified[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_modified[:, dim_x + dim_y + dim_z:]
    
    # vin_3.pkl
    dim_x2 = 2
    dim_y2 = 110
    dim_z2 = 110
    dim_q2 = 4
    
    x_recovered2 = vin3_modified[:, :dim_x2]
    y_recovered2 = vin3_modified[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_modified[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_modified[:, dim_x2 + dim_y2 + dim_z2:]
    
    print(f"ğŸ“Š MC-AEè®­ç»ƒæ•°æ®å‡†å¤‡:")
    print(f"   vin_2ç‰¹å¾: x{x_recovered.shape}, y{y_recovered.shape}, z{z_recovered.shape}, q{q_recovered.shape}")
    print(f"   vin_3ç‰¹å¾: x{x_recovered2.shape}, y{y_recovered2.shape}, z{z_recovered2.shape}, q{q_recovered2.shape}")
    
    # MC-AEè®­ç»ƒå‚æ•°ï¼ˆä¸æºä»£ç Train_.pyå®Œå…¨ä¸€è‡´ï¼‰
    EPOCH_MCAE = 300       # æ¢å¤æºä»£ç çš„300è½®è®­ç»ƒ
    LR_MCAE = 5e-4         # æ¢å¤æºä»£ç çš„5e-4å­¦ä¹ ç‡
    BATCHSIZE_MCAE = 100   # æ¢å¤æºä»£ç çš„100æ‰¹æ¬¡å¤§å°
    
    print(f"\nğŸ”§ MC-AEè®­ç»ƒå‚æ•°ï¼ˆä¸æºä»£ç Train_.pyå®Œå…¨å¯¹é½ï¼‰:")
    print(f"   è®­ç»ƒè½®æ•°: {EPOCH_MCAE} (æºä»£ç : 300)")
    print(f"   å­¦ä¹ ç‡: {LR_MCAE} (æºä»£ç : 5e-4)")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCHSIZE_MCAE} (æºä»£ç : 100)")
    print(f"   ä¼˜åŒ–å™¨: Adam")
    print(f"   æŸå¤±å‡½æ•°: MSELoss")
    print(f"   æ¿€æ´»å‡½æ•°: MC-AE1ç”¨custom_activation, MC-AE2ç”¨sigmoid")
    
    # è‡ªå®šä¹‰å¤šè¾“å…¥æ•°æ®é›†ç±»ï¼ˆå¤ç”¨Class_.pyä¸­çš„å®šä¹‰ï¼‰
    class MCDataset(Dataset):
        def __init__(self, x, y, z, q):
            self.x = x.to(torch.double)
            self.y = y.to(torch.double)
            self.z = z.to(torch.double)
            self.q = q.to(torch.double)
        def __len__(self):
            return len(self.x)
        def __getitem__(self, idx):
            return self.x[idx], self.y[idx], self.z[idx], self.q[idx]
    
    # ç¬¬ä¸€ç»„ç‰¹å¾ï¼ˆvin_2ï¼‰çš„MC-AEè®­ç»ƒ
    print("\nğŸ”§ è®­ç»ƒç¬¬ä¸€ç»„MC-AEæ¨¡å‹ï¼ˆvin_2ï¼‰...")
    train_loader_u = DataLoader(MCDataset(x_recovered, y_recovered, z_recovered, q_recovered), 
                               batch_size=BATCHSIZE_MCAE, shuffle=False)
    
    net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, 
                    activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    
    optimizer_mcae = torch.optim.Adam(net.parameters(), lr=LR_MCAE)
    loss_f = nn.MSELoss()
    
    # è®°å½•è®­ç»ƒæŸå¤±
    train_losses_mcae1 = []
    
    for epoch in range(EPOCH_MCAE):
        total_loss = 0
        num_batches = 0
        for iteration, (x, y, z, q) in enumerate(train_loader_u):
            x = x.to(device)
            y = y.to(device)
            z = z.to(device)
            q = q.to(device)
            net = net.double()
            recon_im, recon_p = net(x, z, q)
            loss_u = loss_f(y, recon_im)
            total_loss += loss_u.item()
            num_batches += 1
            optimizer_mcae.zero_grad()
            loss_u.backward()
            optimizer_mcae.step()
        avg_loss = total_loss / num_batches
        train_losses_mcae1.append(avg_loss)
        if epoch % 50 == 0:
            print(f'MC-AE1 Epoch: {epoch:3d} | Average Loss: {avg_loss:.6f}')
    
    # è·å–ç¬¬ä¸€ç»„é‡æ„è¯¯å·®
    train_loader2 = DataLoader(MCDataset(x_recovered, y_recovered, z_recovered, q_recovered), 
                              batch_size=len(x_recovered), shuffle=False)
    for iteration, (x, y, z, q) in enumerate(train_loader2):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        net = net.double()
        recon_imtest, recon = net(x, z, q)
    
    AA = recon_imtest.cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU
    
    # ç¬¬äºŒç»„ç‰¹å¾ï¼ˆvin_3ï¼‰çš„MC-AEè®­ç»ƒ
    print("\nğŸ”§ è®­ç»ƒç¬¬äºŒç»„MC-AEæ¨¡å‹ï¼ˆvin_3ï¼‰...")
    train_loader_soc = DataLoader(MCDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), 
                                 batch_size=BATCHSIZE_MCAE, shuffle=False)
    
    netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                     activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    optimizer_mcae2 = torch.optim.Adam(netx.parameters(), lr=LR_MCAE)
    
    # è®°å½•è®­ç»ƒæŸå¤±
    train_losses_mcae2 = []
    
    for epoch in range(EPOCH_MCAE):
        total_loss = 0
        num_batches = 0
        for iteration, (x, y, z, q) in enumerate(train_loader_soc):
            x = x.to(device)
            y = y.to(device)
            z = z.to(device)
            q = q.to(device)
            netx = netx.double()
            recon_im, z = netx(x, z, q)
            loss_x = loss_f(y, recon_im)
            total_loss += loss_x.item()
            num_batches += 1
            optimizer_mcae2.zero_grad()
            loss_x.backward()
            optimizer_mcae2.step()
        avg_loss = total_loss / num_batches
        train_losses_mcae2.append(avg_loss)
        if epoch % 50 == 0:
            print(f'MC-AE2 Epoch: {epoch:3d} | Average Loss: {avg_loss:.6f}')
    
    # è·å–ç¬¬äºŒç»„é‡æ„è¯¯å·®
    train_loaderx2 = DataLoader(MCDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), 
                               batch_size=len(x_recovered2), shuffle=False)
    for iteration, (x, y, z, q) in enumerate(train_loaderx2):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        netx = netx.double()
        recon_imtestx, z = netx(x, z, q)
    
    BB = recon_imtestx.cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX
    
    print("âœ… MC-AEè®­ç»ƒå®Œæˆ! (ä¸æºä»£ç Train_.pyå‚æ•°å®Œå…¨ä¸€è‡´)")
    print(f"   MC-AE1æœ€ç»ˆæŸå¤±: {train_losses_mcae1[-1]:.6f}")
    print(f"   MC-AE2æœ€ç»ˆæŸå¤±: {train_losses_mcae2[-1]:.6f}")
    print(f"   è®­ç»ƒå‚æ•°: è½®æ•°{EPOCH_MCAE}, å­¦ä¹ ç‡{LR_MCAE}, æ‰¹æ¬¡{BATCHSIZE_MCAE}")
    
    #----------------------------------------é˜¶æ®µ3: æ··åˆåé¦ˆè®­ç»ƒ------------------------------
    print("\n" + "="*60)
    print("ğŸ”® é˜¶æ®µ3: æ··åˆåé¦ˆè®­ç»ƒï¼ˆæ ·æœ¬8-9ï¼Œè½®æ•°21-40ï¼‰")
    print("="*60)
    
    # é˜¶æ®µ3åœ¨é˜¶æ®µ4ä¹‹åè¿›è¡Œï¼Œå› ä¸ºéœ€è¦PCAå‚æ•°è®¡ç®—é˜ˆå€¼
    print("âš ï¸ é˜¶æ®µ3å°†åœ¨PCAåˆ†æå®Œæˆåè¿›è¡Œï¼Œéœ€è¦å…ˆè·å–æ•…éšœæ£€æµ‹é˜ˆå€¼")
    train_losses_phase2 = []
    feedback_history = []
    consecutive_triggers = 0  # è¿ç»­è§¦å‘è®¡æ•°å™¨
    
    #----------------------------------------é˜¶æ®µ4: PCAåˆ†æå’Œä¿å­˜æ¨¡å‹ï¼ˆå¤ç”¨Train_Transformer.pyé€»è¾‘ï¼‰------------------------------
    print("\n" + "="*60)
    print("ğŸ“Š é˜¶æ®µ4: PCAåˆ†æï¼Œä¿å­˜æ¨¡å‹å’Œå‚æ•°")
    print("="*60)
    
    # è¯Šæ–­ç‰¹å¾æå–ä¸PCAåˆ†æï¼ˆå¤ç”¨Function_.pyä¸­çš„å‡½æ•°ï¼‰
    from Function_ import DiagnosisFeature, PCA
    
    print("ğŸ” æå–è¯Šæ–­ç‰¹å¾...")
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    print(f"   è¯Šæ–­ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_data.shape}")
    
    print("ğŸ” è¿›è¡ŒPCAåˆ†æ...")
    v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = PCA(df_data, 0.95, 0.95)
    
    print(f"âœ… PCAåˆ†æå®Œæˆ:")
    print(f"   ä¸»æˆåˆ†æ•°é‡: {k}")
    print(f"   è§£é‡Šæ–¹å·®æ¯”: {v_ratio}")
    print(f"   TÂ²æ§åˆ¶é™: 95%={T_95_limit:.4f}, 99%={T_99_limit:.4f}")
    print(f"   SPEæ§åˆ¶é™: 95%={SPE_95_limit:.4f}, 99%={SPE_99_limit:.4f}")
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œåˆ†æç»“æœ
    print("\nğŸ’¾ ä¿å­˜æ··åˆåé¦ˆè®­ç»ƒç»“æœ...")
    model_suffix = "_hybrid_feedback"
    
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    if not os.path.exists('models'):
        os.makedirs('models')
    
    # 1. ä¿å­˜Transformeræ¨¡å‹
    transformer_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/Transformer/models/transformer_model{model_suffix}.pth',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/transformer_model{model_suffix}.pth',
        f'./transformer_model{model_suffix}.pth',
        f'/mnt/bz25t/bzhy/zhanglikang/project/transformer_model{model_suffix}.pth',
        f'models/transformer_model{model_suffix}.pth'
    ]
    
    transformer_saved = False
    for save_path in transformer_save_paths:
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            torch.save(transformer.state_dict(), save_path)
            print(f"âœ… Transformeræ¨¡å‹å·²ä¿å­˜: {save_path}")
            transformer_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜Transformeræ¨¡å‹åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not transformer_saved:
        print("âŒ è­¦å‘Š: Transformeræ¨¡å‹ä¿å­˜å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
    
    # 2. ä¿å­˜MC-AEæ¨¡å‹
    mcae_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/Transformer/models/net_model{model_suffix}.pth',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/net_model{model_suffix}.pth',
        f'./net_model{model_suffix}.pth',
        f'/mnt/bz25t/bzhy/zhanglikang/project/net_model{model_suffix}.pth',
        f'models/net_model{model_suffix}.pth'
    ]
    
    mcae1_saved = False
    for save_path in mcae_save_paths:
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            torch.save(net.state_dict(), save_path)
            print(f"âœ… MC-AE1æ¨¡å‹å·²ä¿å­˜: {save_path}")
            mcae1_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜MC-AE1æ¨¡å‹åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    mcae2_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/Transformer/models/netx_model{model_suffix}.pth',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/netx_model{model_suffix}.pth',
        f'./netx_model{model_suffix}.pth',
        f'/mnt/bz25t/bzhy/zhanglikang/project/netx_model{model_suffix}.pth',
        f'models/netx_model{model_suffix}.pth'
    ]
    
    mcae2_saved = False
    for save_path in mcae2_save_paths:
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            torch.save(netx.state_dict(), save_path)
            print(f"âœ… MC-AE2æ¨¡å‹å·²ä¿å­˜: {save_path}")
            mcae2_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜MC-AE2æ¨¡å‹åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not mcae1_saved or not mcae2_saved:
        print("âŒ è­¦å‘Š: éƒ¨åˆ†MC-AEæ¨¡å‹ä¿å­˜å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
    
    # 3. ä¿å­˜é‡æ„è¯¯å·®æ•°æ®
    error_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/ERRORU{model_suffix}.npy',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/ERRORU{model_suffix}.npy',
        f'./ERRORU{model_suffix}.npy',
        f'/mnt/bz25t/bzhy/zhanglikang/project/ERRORU{model_suffix}.npy',
        f'models/ERRORU{model_suffix}.npy'
    ]
    
    erroru_saved = False
    for save_path in error_save_paths:
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            np.save(save_path, ERRORU)
            print(f"âœ… ERRORUå·²ä¿å­˜: {save_path}")
            erroru_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜ERRORUåˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    errorx_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/ERRORX{model_suffix}.npy',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/ERRORX{model_suffix}.npy',
        f'./ERRORX{model_suffix}.npy',
        f'/mnt/bz25t/bzhy/zhanglikang/project/ERRORX{model_suffix}.npy',
        f'models/ERRORX{model_suffix}.npy'
    ]
    
    errorx_saved = False
    for save_path in errorx_save_paths:
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            np.save(save_path, ERRORX)
            print(f"âœ… ERRORXå·²ä¿å­˜: {save_path}")
            errorx_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜ERRORXåˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not erroru_saved or not errorx_saved:
        print("âŒ è­¦å‘Š: éƒ¨åˆ†é‡æ„è¯¯å·®æ•°æ®ä¿å­˜å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
    
    # 4. ä¿å­˜PCAåˆ†æç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿å­˜å…³é”®å‚æ•°ï¼‰
    pca_files = [
        ('v_I', v_I), ('v', v), ('v_ratio', v_ratio), ('p_k', p_k),
        ('data_mean', data_mean), ('data_std', data_std),
        ('T_95_limit', T_95_limit), ('T_99_limit', T_99_limit),
        ('SPE_95_limit', SPE_95_limit), ('SPE_99_limit', SPE_99_limit),
        ('P', P), ('k', k), ('P_t', P_t), ('X', X), ('data_nor', data_nor)
    ]
    
    pca_save_paths = [
        f'/mnt/bz25t/bzhy/datasave/',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/',
        f'./',
        f'/mnt/bz25t/bzhy/zhanglikang/project/',
        f'models/'
    ]
    
    pca_saved_count = 0
    for save_dir in pca_save_paths:
        try:
            os.makedirs(save_dir, exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(save_dir)
                print(f"ğŸ“Š è·¯å¾„ {save_dir} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            for name, data in pca_files:
                save_path = f'{save_dir}{name}{model_suffix}.npy'
                np.save(save_path, data)
            print(f"âœ… PCAåˆ†æç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
            pca_saved_count += 1
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜PCAç»“æœåˆ° {save_dir} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if pca_saved_count == 0:
        print("âŒ è­¦å‘Š: PCAåˆ†æç»“æœä¿å­˜å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
        print("ğŸ’¡ å°è¯•æ‰‹åŠ¨åˆ›å»ºç›®å½•: mkdir -p /mnt/bz25t/bzhy/zhanglikang/project")
    
    # 5. ä¿å­˜PCAå‚æ•°å­—å…¸ï¼ˆç”¨äºåé¦ˆé˜¶æ®µï¼‰
    pca_params = {
        'v_I': v_I,
        'v': v,
        'v_ratio': v_ratio,
        'p_k': p_k,
        'data_mean': data_mean,
        'data_std': data_std,
        'T_95_limit': T_95_limit,
        'T_99_limit': T_99_limit,
        'SPE_95_limit': SPE_95_limit,
        'SPE_99_limit': SPE_99_limit,
        'P': P,
        'k': k,
        'P_t': P_t,
        'X': X,
        'data_nor': data_nor
    }
    
    # å°è¯•å¤šä¸ªä¿å­˜è·¯å¾„ï¼Œå¤„ç†ç£ç›˜ç©ºé—´ä¸è¶³é—®é¢˜
    save_paths = [
                f'/mnt/bz25t/bzhy/datasave/Transformer/models/pca_params{model_suffix}.pkl',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/pca_params{model_suffix}.pkl',
        f'./pca_params{model_suffix}.pkl',
        f'/mnt/bz25t/bzhy/zhanglikang/project/pca_params{model_suffix}.pkl',
        f'models/pca_params{model_suffix}.pkl'
    ]
    
    saved = False
    for save_path in save_paths:
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            with open(save_path, 'wb') as f:
                pickle.dump(pca_params, f)
            print(f"âœ… PCAå‚æ•°å­—å…¸å·²ä¿å­˜: {save_path}")
            saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not saved:
        print("âŒ è­¦å‘Š: æ‰€æœ‰ä¿å­˜è·¯å¾„éƒ½å¤±è´¥ï¼ŒPCAå‚æ•°æœªä¿å­˜")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
        print("ğŸ’¡ å°è¯•æ‰‹åŠ¨åˆ›å»ºç›®å½•: mkdir -p /mnt/bz25t/bzhy/zhanglikang/project")
        # å°†PCAå‚æ•°ä¿å­˜åˆ°å†…å­˜ä¸­ï¼Œä¾›åç»­ä½¿ç”¨
        global_saved_pca_params = pca_params
    
    # 6. è®¡ç®—è®­ç»ƒé˜¶æ®µæ•…éšœæ£€æµ‹é˜ˆå€¼
    if len(valid_samples) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬ï¼Œæ— æ³•è®¡ç®—æ•…éšœæ£€æµ‹é˜ˆå€¼")
        return
    
    # ç¡®ä¿MC-AEæ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    net = net.to(device)
    netx = netx.to(device)
    
    print(f"ğŸ” è®¾å¤‡æ£€æŸ¥: netåœ¨{next(net.parameters()).device}, netxåœ¨{next(netx.parameters()).device}, ç›®æ ‡è®¾å¤‡{device}")
    
    threshold1, threshold2, threshold3 = calculate_training_threshold(
        valid_samples, net, netx, pca_params, device)
    
    # ä¿å­˜é˜ˆå€¼
    thresholds = {
        'threshold1': threshold1,  # 3Ïƒé˜ˆå€¼
        'threshold2': threshold2,  # 4.5Ïƒé˜ˆå€¼
        'threshold3': threshold3   # 6Ïƒé˜ˆå€¼
    }
    
    # å°è¯•å¤šä¸ªä¿å­˜è·¯å¾„ï¼Œå¤„ç†ç£ç›˜ç©ºé—´ä¸è¶³é—®é¢˜
    threshold_save_paths = [
                f'/mnt/bz25t/bzhy/datasave/fault_thresholds{model_suffix}.pkl',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/fault_thresholds{model_suffix}.pkl',
        f'./fault_thresholds{model_suffix}.pkl',
        f'/mnt/bz25t/bzhy/zhanglikang/project/fault_thresholds{model_suffix}.pkl',
        f'models/fault_thresholds{model_suffix}.pkl'
    ]
    
    threshold_saved = False
    for save_path in threshold_save_paths:
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            with open(save_path, 'wb') as f:
                pickle.dump(thresholds, f)
            print(f"âœ… æ•…éšœæ£€æµ‹é˜ˆå€¼å·²ä¿å­˜: {save_path}")
            threshold_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜é˜ˆå€¼åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not threshold_saved:
        print("âŒ è­¦å‘Š: æ‰€æœ‰é˜ˆå€¼ä¿å­˜è·¯å¾„éƒ½å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
        print("ğŸ’¡ å°è¯•æ‰‹åŠ¨åˆ›å»ºç›®å½•: mkdir -p /mnt/bz25t/bzhy/zhanglikang/project")
        # å°†é˜ˆå€¼ä¿å­˜åˆ°å†…å­˜ä¸­ï¼Œä¾›åç»­ä½¿ç”¨
        global_saved_thresholds = thresholds
    
    #----------------------------------------ç°åœ¨å¼€å§‹é˜¶æ®µ3: æ··åˆåé¦ˆè®­ç»ƒ------------------------------
    print("\n" + "="*60)
    print("ğŸ”® ç°åœ¨å¼€å§‹é˜¶æ®µ3: æ··åˆåé¦ˆè®­ç»ƒï¼ˆä½¿ç”¨è®¡ç®—å‡ºçš„é˜ˆå€¼ï¼‰")
    print("="*60)
    
    # ä½¿ç”¨è®¡ç®—å‡ºçš„é˜ˆå€¼è¿›è¡Œåé¦ˆè®­ç»ƒ
    current_threshold = threshold1
    print(f"âœ… ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„é˜ˆå€¼: {current_threshold:.4f}")
    
    # ç»§ç»­è®­ç»ƒï¼ˆé˜¶æ®µ3ï¼šæ··åˆåé¦ˆï¼‰
    transformer.train()
    
    print(f"\nğŸ¯ å¼€å§‹é˜¶æ®µ3è®­ç»ƒï¼ˆepoch {EPOCH_PHASE1+1}-{EPOCH_PHASE2}ï¼‰...")
    
    for epoch in range(EPOCH_PHASE1, EPOCH_PHASE2):
        epoch_loss = 0
        batch_count = 0
        feedback_triggered = False
        trigger_info = "æ— åé¦ˆ"
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åé¦ˆ
        if (epoch >= config['feedback_start_epoch'] and 
            epoch % config['feedback_frequency'] == 0):
            
            print(f"\nğŸ” Epoch {epoch}: æ£€æŸ¥åé¦ˆè§¦å‘æ¡ä»¶...")
            
            try:
                # ç¡®ä¿MC-AEæ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                net = net.to(device)
                netx = netx.to(device)
                
                # è®¡ç®—å½“å‰çš„å‡é˜³æ€§ç‡ï¼ˆåŸºäºç»¼åˆè¯Šæ–­æŒ‡æ ‡ï¼‰
                false_positive_rate, false_positives, total_normals = calculate_false_positive_rate_comprehensive(
                    config['feedback_samples'], net, netx, pca_params, current_threshold, device)
                
                print(f"   å½“å‰å‡é˜³æ€§ç‡: {false_positive_rate:.4f} ({false_positive_rate*100:.2f}%)")
                
                # æ£€æµ‹åé¦ˆè§¦å‘ï¼ˆé›†æˆè¿ç»­è§¦å‘è¿½è¸ªï¼‰
                trigger_level, lr_factor, feedback_weight = detect_feedback_trigger(
                    false_positive_rate, epoch, config, consecutive_triggers)
                
                if trigger_level != 'none':
                    feedback_triggered = True
                    consecutive_triggers += 1  # å¢åŠ è¿ç»­è§¦å‘è®¡æ•°
                    trigger_info = f"{trigger_level}åé¦ˆ (æƒé‡:{feedback_weight:.2f}, LRå› å­:{lr_factor:.2f}, è¿ç»­:{consecutive_triggers})"
                    
                    # è°ƒæ•´å­¦ä¹ ç‡
                    if lr_factor != 1.0:
                        for param_group in optimizer.param_groups:
                            param_group['lr'] *= lr_factor
                        print(f"   å­¦ä¹ ç‡è°ƒæ•´: {param_group['lr']:.6f}")
                    
                    # å‡†å¤‡åé¦ˆæ•°æ®ï¼ˆåŸºäºå®é™…é¢„æµ‹è¯¯å·®ï¼‰
                    feedback_data = prepare_feedback_data(config['feedback_samples'], device, batch_size=500)
                    
                    if feedback_data is not None:
                        # 1. åº”ç”¨æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒï¼ˆåŸºäºé˜ˆå€¼ç›¸å¯¹ä¼˜åŒ–ï¼‰
                        focus_loss, avg_pred_error, threshold_info = apply_normal_sample_focus_training(
                            transformer, feedback_data, optimizer, criterion, config, device, current_threshold)
                        print(f"   æ­£å¸¸æ ·æœ¬ç‰¹åŒ–è®­ç»ƒ: æŸå¤±={focus_loss:.6f}, {threshold_info}")
                        
                        # 2. åº”ç”¨æ··åˆåé¦ˆ
                        feedback_loss, feedback_info = apply_hybrid_feedback(
                            transformer, net, netx, feedback_data, 
                            feedback_weight, config['mcae_weight'], config['transformer_weight'], device)
                        
                        print(f"   {feedback_info}")
                    else:
                        print(f"   âš ï¸ åé¦ˆæ•°æ®å‡†å¤‡å¤±è´¥ï¼Œè·³è¿‡åé¦ˆè®­ç»ƒ")
                    
                    # è®°å½•åé¦ˆå†å²
                    feedback_history.append({
                        'epoch': epoch,
                        'false_positive_rate': false_positive_rate,
                        'trigger_level': trigger_level,
                        'feedback_weight': feedback_weight,
                        'lr_factor': lr_factor,
                        'false_positives': false_positives,
                        'total_normals': total_normals,
                        'consecutive_triggers': consecutive_triggers
                    })
                else:
                    consecutive_triggers = 0  # é‡ç½®è¿ç»­è§¦å‘è®¡æ•°
                    print(f"   æ— éœ€åé¦ˆ (å‡é˜³æ€§ç‡: {false_positive_rate:.4f})")
                    
            except Exception as e:
                print(f"   âŒ åé¦ˆè®¡ç®—å¤±è´¥: {e}")
                print("   ç»§ç»­æ­£å¸¸è®­ç»ƒ...")
                feedback_triggered = False
        
        # æ­£å¸¸è®­ç»ƒå¾ªç¯
        for batch_input, batch_target in train_loader:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            batch_input = batch_input.to(device)
            batch_target = batch_target.to(device)
            
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                pred_output = transformer(batch_input)
                loss = criterion(pred_output, batch_target)
                
                # å¦‚æœæœ‰åé¦ˆï¼Œæ·»åŠ åé¦ˆæŸå¤±
                if feedback_triggered and 'feedback_loss' in locals():
                    total_loss = loss + 0.1 * feedback_loss  # åé¦ˆæŸå¤±æƒé‡ä¸º0.1
                else:
                    total_loss = loss
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += total_loss.item()
            batch_count += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåœ¨åé¦ˆè°ƒæ•´ä¹‹åï¼‰
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = epoch_loss / batch_count
        train_losses_phase2.append(avg_loss)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if epoch % 2 == 0 or epoch == EPOCH_PHASE2 - 1:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'é˜¶æ®µ3 Epoch: {epoch:3d} | Loss: {avg_loss:.6f} | LR: {current_lr:.6f} | {trigger_info}')
    
    if train_losses_phase2:
        print(f"\nâœ… é˜¶æ®µ3æ··åˆåé¦ˆè®­ç»ƒå®Œæˆ! æœ€ç»ˆæŸå¤±: {train_losses_phase2[-1]:.6f}")
    else:
        print(f"\nâš ï¸ é˜¶æ®µ3æ··åˆåé¦ˆè®­ç»ƒå®Œæˆ! ä½†è®­ç»ƒæŸå¤±åˆ—è¡¨ä¸ºç©º")
    print(f"ğŸ“Š åé¦ˆè§¦å‘æ¬¡æ•°: {len(feedback_history)}")
    if feedback_history:
        avg_fpr = np.mean([h['false_positive_rate'] for h in feedback_history])
        print(f"ğŸ“Š å¹³å‡å‡é˜³æ€§ç‡: {avg_fpr:.4f} ({avg_fpr*100:.2f}%)")
    
    # 5. ä¿å­˜æ··åˆåé¦ˆè®­ç»ƒå†å²
    hybrid_feedback_history = {
        'phase1_losses': train_losses_phase1,
        'phase2_losses': train_losses_phase2,
        'mcae1_losses': train_losses_mcae1,
        'mcae2_losses': train_losses_mcae2,
        'feedback_history': feedback_history,
        'final_phase1_loss': train_losses_phase1[-1] if train_losses_phase1 else None,
        'final_phase2_loss': train_losses_phase2[-1] if train_losses_phase2 else None,
        'config': config,
        'feedback_triggers': len(feedback_history),
        'model_params': {
            'input_size': 7,
            'd_model': 128,
            'nhead': 8,
            'num_layers': 3,
            'output_size': 2
        }
    }
    
    # å°è¯•å¤šä¸ªä¿å­˜è·¯å¾„ï¼Œå¤„ç†ç£ç›˜ç©ºé—´ä¸è¶³é—®é¢˜
    history_save_paths = [
                f'/mnt/bz25t/bzhy/datasave/hybrid_feedback_training_history.pkl',  # ç”¨æˆ·æŒ‡å®šè·¯å¾„
        f'/tmp/hybrid_feedback_training_history.pkl',
        f'./hybrid_feedback_training_history.pkl',
        f'/mnt/bz25t/bzhy/zhanglikang/project/hybrid_feedback_training_history.pkl',
        f'models/hybrid_feedback_training_history.pkl'
    ]
    
    history_saved = False
    for save_path in history_save_paths:
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import shutil
                total, used, free = shutil.disk_usage(os.path.dirname(save_path))
                print(f"ğŸ“Š è·¯å¾„ {os.path.dirname(save_path)} å¯ç”¨ç©ºé—´: {free / (1024**3):.2f} GB")
            except:
                pass
            
            with open(save_path, 'wb') as f:
                pickle.dump(hybrid_feedback_history, f)
            print(f"âœ… æ··åˆåé¦ˆè®­ç»ƒå†å²å·²ä¿å­˜: {save_path}")
            history_saved = True
            break
        except OSError as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒå†å²åˆ° {save_path} å¤±è´¥: {e}")
            print(f"   é”™è¯¯ä»£ç : {e.errno}, é”™è¯¯ä¿¡æ¯: {e.strerror}")
            continue
    
    if not history_saved:
        print("âŒ è­¦å‘Š: æ‰€æœ‰è®­ç»ƒå†å²ä¿å­˜è·¯å¾„éƒ½å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®å½•æƒé™æˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨ä½ç½®")
        print("ğŸ’¡ å°è¯•æ‰‹åŠ¨åˆ›å»ºç›®å½•: mkdir -p /mnt/bz25t/bzhy/zhanglikang/project")
        # å°†è®­ç»ƒå†å²ä¿å­˜åˆ°å†…å­˜ä¸­ï¼Œä¾›åç»­ä½¿ç”¨
        global_saved_training_history = hybrid_feedback_history
    
    # 6. ä¿å­˜è¯Šæ–­ç‰¹å¾
    print(f"ğŸ’¾ ä¿å­˜è¯Šæ–­ç‰¹å¾ï¼ˆæ•°æ®é‡: {df_data.shape}ï¼‰...")
    csv_path = f'models/diagnosis_feature{model_suffix}.csv'
    df_data.to_csv(csv_path, index=False)
    print(f"âœ… è¯Šæ–­ç‰¹å¾CSVå·²ä¿å­˜: {csv_path}")
    
    #----------------------------------------ç»˜åˆ¶æ··åˆåé¦ˆè®­ç»ƒç»“æœ------------------------------
    print("\nğŸ“ˆ ç»˜åˆ¶æ··åˆåé¦ˆè®­ç»ƒç»“æœ...")
    
    # åˆ›å»ºç»¼åˆå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # å­å›¾1: ä¸¤é˜¶æ®µè®­ç»ƒæŸå¤±å¯¹æ¯”
    ax1 = axes[0, 0]
    epochs_phase1 = range(1, len(train_losses_phase1) + 1)
    epochs_phase2 = range(len(train_losses_phase1) + 1, len(train_losses_phase1) + len(train_losses_phase2) + 1)
    
    ax1.plot(epochs_phase1, train_losses_phase1, 'b-', linewidth=2, label='é˜¶æ®µ1: åŸºç¡€è®­ç»ƒ')
    if train_losses_phase2:
        ax1.plot(epochs_phase2, train_losses_phase2, 'r-', linewidth=2, label='é˜¶æ®µ3: æ··åˆåé¦ˆ')
    ax1.axvline(x=len(train_losses_phase1), color='gray', linestyle='--', alpha=0.7, label='åé¦ˆå¯åŠ¨ç‚¹')
    
    if use_chinese:
        ax1.set_xlabel('è®­ç»ƒè½®æ•°')
        ax1.set_ylabel('MSEæŸå¤±')
        ax1.set_title('æ··åˆåé¦ˆç­–ç•¥è®­ç»ƒæŸå¤±')
    else:
        ax1.set_xlabel('Training Epochs')
        ax1.set_ylabel('MSE Loss')
        ax1.set_title('Hybrid Feedback Training Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # å­å›¾2: MC-AE1è®­ç»ƒæŸå¤±
    ax2 = axes[0, 1]
    epochs = range(1, len(train_losses_mcae1) + 1)
    ax2.plot(epochs, train_losses_mcae1, 'g-', linewidth=2, label='MC-AE1 Training Loss')
    if use_chinese:
        ax2.set_xlabel('è®­ç»ƒè½®æ•°')
        ax2.set_ylabel('MSEæŸå¤±')
        ax2.set_title('MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿')
    else:
        ax2.set_xlabel('Training Epochs')
        ax2.set_ylabel('MSE Loss')
        ax2.set_title('MC-AE1 Training Loss')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.set_yscale('log')
    
    # å­å›¾3: MC-AE2è®­ç»ƒæŸå¤±
    ax3 = axes[0, 2]
    ax3.plot(epochs, train_losses_mcae2, 'orange', linewidth=2, label='MC-AE2 Training Loss')
    if use_chinese:
        ax3.set_xlabel('è®­ç»ƒè½®æ•°')
        ax3.set_ylabel('MSEæŸå¤±')
        ax3.set_title('MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿')
    else:
        ax3.set_xlabel('Training Epochs')
        ax3.set_ylabel('MSE Loss')
        ax3.set_title('MC-AE2 Training Loss')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    ax3.set_yscale('log')
    
    # å­å›¾4: åé¦ˆè§¦å‘å†å²
    ax4 = axes[1, 0]
    if feedback_history:
        feedback_epochs = [item['epoch'] for item in feedback_history]
        feedback_rates = [item['false_positive_rate'] for item in feedback_history]
        feedback_levels = [item['trigger_level'] for item in feedback_history]
        
        # ç”¨ä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒçš„åé¦ˆç­‰çº§
        color_map = {'standard': 'yellow', 'enhanced': 'orange', 'emergency': 'red'}
        for i, (epoch, rate, level) in enumerate(zip(feedback_epochs, feedback_rates, feedback_levels)):
            color = color_map.get(level, 'gray')
            ax4.scatter(epoch, rate, c=color, s=100, alpha=0.7, 
                       label=level if level not in [item.get_text() for item in ax4.get_legend_handles_labels()[1]] else "")
        
        # æ·»åŠ é˜ˆå€¼çº¿
        thresholds = config['false_positive_thresholds']
        ax4.axhline(y=thresholds['standard'], color='yellow', linestyle='--', alpha=0.5, label='æ ‡å‡†é˜ˆå€¼')
        ax4.axhline(y=thresholds['enhanced'], color='orange', linestyle='--', alpha=0.5, label='å¼ºåŒ–é˜ˆå€¼')
        ax4.axhline(y=thresholds['emergency'], color='red', linestyle='--', alpha=0.5, label='ç´§æ€¥é˜ˆå€¼')
    
    if use_chinese:
        ax4.set_xlabel('è®­ç»ƒè½®æ•°')
        ax4.set_ylabel('å‡é˜³æ€§ç‡')
        ax4.set_title('åé¦ˆè§¦å‘å†å²')
    else:
        ax4.set_xlabel('Training Epochs')
        ax4.set_ylabel('False Positive Rate')
        ax4.set_title('Feedback Trigger History')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # å­å›¾5: MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ
    ax5 = axes[1, 1]
    reconstruction_errors_1 = ERRORU.flatten()
    mean_error_1 = np.mean(np.abs(reconstruction_errors_1))
    if use_chinese:
        ax5.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
                label=f'MC-AE1é‡æ„è¯¯å·® (å‡å€¼: {mean_error_1:.4f})')
        ax5.set_xlabel('ç»å¯¹é‡æ„è¯¯å·®')
        ax5.set_ylabel('é¢‘æ•°')
        ax5.set_title('MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ')
    else:
        ax5.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
                label=f'MC-AE1 Reconstruction Error (Mean: {mean_error_1:.4f})')
        ax5.set_xlabel('Absolute Reconstruction Error')
        ax5.set_ylabel('Frequency')
        ax5.set_title('MC-AE1 Reconstruction Error Distribution')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # å­å›¾6: MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ
    ax6 = axes[1, 2]
    reconstruction_errors_2 = ERRORX.flatten()
    mean_error_2 = np.mean(np.abs(reconstruction_errors_2))
    if use_chinese:
        ax6.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
                label=f'MC-AE2é‡æ„è¯¯å·® (å‡å€¼: {mean_error_2:.4f})')
        ax6.set_xlabel('ç»å¯¹é‡æ„è¯¯å·®')
        ax6.set_ylabel('é¢‘æ•°')
        ax6.set_title('MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ')
    else:
        ax6.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
                label=f'MC-AE2 Reconstruction Error (Mean: {mean_error_2:.4f})')
        ax6.set_xlabel('Absolute Reconstruction Error')
        ax6.set_ylabel('Frequency')
        ax6.set_title('MC-AE2 Reconstruction Error Distribution')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = f'/mnt/bz25t/bzhy/datasave/Transformer/models/hybrid_feedback_training_results.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ··åˆåé¦ˆè®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {plot_path}")
    
    #----------------------------------------æœ€ç»ˆè®­ç»ƒå®Œæˆæ€»ç»“------------------------------
    print("\n" + "="*80)
    print("ğŸ‰ æ··åˆåé¦ˆç­–ç•¥è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print("âœ… è®­ç»ƒæµç¨‹æ€»ç»“:")
    print("   é˜¶æ®µ1: âœ… TransformeråŸºç¡€è®­ç»ƒ (æ ·æœ¬0-7, epoch 0-20)")
    print("   é˜¶æ®µ2: âœ… MC-AEè®­ç»ƒ (ä½¿ç”¨Transformerå¢å¼ºæ•°æ®)")
    print("   é˜¶æ®µ3: âœ… æ··åˆåé¦ˆè®­ç»ƒ (æ ·æœ¬8-9, epoch 21-40)")
    print("   é˜¶æ®µ4: âœ… PCAåˆ†æå’Œæ¨¡å‹ä¿å­˜")
    print("")
    print("ğŸ”§ å…³é”®ä¿®å¤ (ä¸æºä»£ç Train_.pyå¯¹é½):")
    print(f"   - MC-AEè®­ç»ƒè½®æ•°: {EPOCH_MCAE} (æºä»£ç : 300)")
    print(f"   - MC-AEå­¦ä¹ ç‡: {LR_MCAE} (æºä»£ç : 5e-4)")
    print(f"   - MC-AEæ‰¹æ¬¡å¤§å°: {BATCHSIZE_MCAE} (æºä»£ç : 100)")
    print("   - æ¿€æ´»å‡½æ•°: MC-AE1ç”¨custom_activation, MC-AE2ç”¨sigmoid")
    print("")
    print("ğŸ“Š å…³é”®åˆ›æ–°:")
    print("   - æ•°æ®éš”ç¦»ç­–ç•¥ï¼šè®­ç»ƒ/åé¦ˆ/æµ‹è¯•æ ·æœ¬ä¸¥æ ¼åˆ†ç¦»")
    print("   - å¤šçº§åé¦ˆè§¦å‘ï¼š1%é¢„è­¦ã€3%æ ‡å‡†ã€5%å¼ºåŒ–ã€10%ç´§æ€¥")
    print("   - æ··åˆæƒé‡æœºåˆ¶ï¼šMC-AEæƒé‡0.8ï¼ŒTransformeræƒé‡0.2")
    print("   - è‡ªé€‚åº”å­¦ä¹ ç‡ï¼šæ ¹æ®å‡é˜³æ€§ç‡åŠ¨æ€è°ƒæ•´")
    print("   - å®æ—¶åé¦ˆç›‘æ§ï¼šæ¯15ä¸ªepochæ£€æŸ¥è§¦å‘æ¡ä»¶")
    print("")
    print("ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   é˜¶æ®µ1æœ€ç»ˆæŸå¤±: {train_losses_phase1[-1]:.6f}")
    if train_losses_phase2:
        print(f"   é˜¶æ®µ3æœ€ç»ˆæŸå¤±: {train_losses_phase2[-1]:.6f}")
    else:
        print(f"   é˜¶æ®µ3æœ€ç»ˆæŸå¤±: æ— æ•°æ®")
    print(f"   åé¦ˆè§¦å‘æ¬¡æ•°: {len(feedback_history)}")
    print(f"   PCAä¸»æˆåˆ†æ•°é‡: {k}")
    print("")
    print("ğŸ”„ ä¸‹ä¸€æ­¥å¯ä»¥:")
    print("   1. è¿è¡ŒTest_combine_transonly.pyè¿›è¡Œæ€§èƒ½è¯„ä¼°")
    print("   2. ä¸BiLSTMåŸºå‡†è¿›è¡Œè¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("   3. åˆ†ææ··åˆåé¦ˆç­–ç•¥çš„æ”¹è¿›æ•ˆæœ")
    print("   4. è°ƒæ•´åé¦ˆå‚æ•°è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–")

if __name__ == "__main__":
    main()