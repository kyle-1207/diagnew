# ä¸­æ–‡æ³¨é‡Šï¼šå¯¼å…¥å¸¸ç”¨åº“å’Œè‡ªå®šä¹‰æ¨¡å—
#æ³¨æ„è·¯å¾„
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle

# GPUè®¾å¤‡é…ç½®
import os
# ä½¿ç”¨æŒ‡å®šçš„GPUè®¾å¤‡
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # åªä½¿ç”¨GPU2
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # è¿™é‡Œçš„cuda:0å®é™…ä¸Šæ˜¯ç‰©ç†GPU2

# æ‰“å°GPUä¿¡æ¯
if torch.cuda.is_available():
    print("\nğŸ–¥ï¸ GPUé…ç½®ä¿¡æ¯:")
    print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\n   GPU {i} ({props.name}):")
        print(f"      æ€»æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
    print(f"\n   å½“å‰ä½¿ç”¨: ä»…GPU2 (å°æ ·æœ¬è®­ç»ƒä¼˜åŒ–ï¼Œé¿å…è·¨å¡é€šä¿¡å¼€é”€)")
    print(f"   ä¸»GPUè®¾å¤‡: cuda:0 (ç‰©ç†GPU2)")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

# ä¸­æ–‡æ³¨é‡Šï¼šå¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

#----------------------------------------BiLSTMåŸºå‡†è®­ç»ƒé…ç½®------------------------------
print("="*50)
print("BiLSTMåŸºå‡†è®­ç»ƒæ¨¡å¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
print("ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("è·³è¿‡Transformerè®­ç»ƒï¼Œç›´æ¥è¿›è¡ŒMC-AEè®­ç»ƒ")
print("å¯ç”¨åŒGPUæ•°æ®å¹¶è¡Œå’Œæ··åˆç²¾åº¦è®­ç»ƒ")
print("="*50)

#----------------------------------------æ•°æ®åŠ è½½------------------------------
# ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬IDï¼ˆ0-200å·ï¼‰
def load_train_samples():
    """ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬IDï¼ˆå°æ ·æœ¬å¿«é€Ÿè®­ç»ƒç‰ˆæœ¬ï¼‰"""
    try:
        import pandas as pd
        labels_path = '../QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–0-9èŒƒå›´çš„æ ·æœ¬ä½œä¸ºè®­ç»ƒæ•°æ®
        all_samples = df['Num'].tolist()
        train_samples = [i for i in all_samples if 0 <= i <= 9]
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½è®­ç»ƒæ ·æœ¬ï¼ˆå°æ ·æœ¬å¿«é€Ÿè®­ç»ƒï¼‰:")
        print(f"   è®­ç»ƒæ ·æœ¬èŒƒå›´: 0-9")
        print(f"   å®é™…å¯ç”¨æ ·æœ¬: {len(train_samples)} ä¸ª")
        print(f"   æ ·æœ¬ID: {train_samples}")
        
        return train_samples
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æ ·æœ¬èŒƒå›´ 0-9")
        return list(range(10))

train_samples = load_train_samples()
print(f"ä½¿ç”¨QASç›®å½•ä¸­çš„{len(train_samples)}ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")

# å®šä¹‰è®­ç»ƒå‚æ•°ï¼ˆä¸æºä»£ç Train_.pyå®Œå…¨ä¸€è‡´ï¼‰
EPOCH = 300  # æ¢å¤æºä»£ç çš„300è½®è®­ç»ƒ
INIT_LR = 5e-4  # ä¸æºä»£ç LR=5e-4ä¸€è‡´  
MAX_LR = 5e-4   # ä¿æŒä¸æºä»£ç ä¸€è‡´
BATCHSIZE = 100  # æ¢å¤æºä»£ç çš„100æ‰¹æ¬¡å¤§å°
WARMUP_EPOCHS = 5  # é¢„çƒ­è½®æ•°

# æ¢¯åº¦è£å‰ªå‚æ•°ä¼˜åŒ–ï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰
MAX_GRAD_NORM = 1.0  # é™ä½æœ€å¤§æ¢¯åº¦é˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
MIN_GRAD_NORM = 0.001  # é™ä½æœ€å°æ¢¯åº¦é˜ˆå€¼ï¼Œå‡å°‘æ¢¯åº¦è¿‡å°è­¦å‘Š

# å­¦ä¹ ç‡è°ƒåº¦å‡½æ•°ï¼ˆå‚è€ƒæºä»£ç ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼‰
def get_lr(epoch):
    return INIT_LR  # ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼Œå‚è€ƒæºä»£ç 

# æ˜¾ç¤ºè®­ç»ƒå‚æ•°ï¼ˆä¸æºä»£ç Train_.pyå¯¹é½ï¼‰
print(f"\nâš™ï¸  BiLSTMè®­ç»ƒå‚æ•°ï¼ˆä¸æºä»£ç Train_.pyå®Œå…¨ä¸€è‡´ï¼‰:")
print(f"   è®­ç»ƒè½®æ•°: {EPOCH} (æºä»£ç : 300)")
print(f"   å­¦ä¹ ç‡: {INIT_LR} (æºä»£ç : 5e-4)")
print(f"   æ‰¹æ¬¡å¤§å°: {BATCHSIZE} (æºä»£ç : 100)")
print(f"   é¢„çƒ­è½®æ•°: {WARMUP_EPOCHS}")
print(f"   æœ€å¤§æ¢¯åº¦é˜ˆå€¼: {MAX_GRAD_NORM}")
print(f"   æœ€å°æ¢¯åº¦é˜ˆå€¼: {MIN_GRAD_NORM}")
print(f"   å­¦ä¹ ç‡è°ƒåº¦: å›ºå®šå­¦ä¹ ç‡ (ä¸æºä»£ç ä¸€è‡´)")
print(f"   æ•°æ®å¹¶è¡Œ: ç¦ç”¨ï¼ˆå•GPUä¼˜åŒ–ï¼‰")
print(f"   æ··åˆç²¾åº¦: ç¦ç”¨ (ä¸æºä»£ç ä¸€è‡´)")
print(f"   æ•°æ®ç±»å‹: float32 (ä¸æºä»£ç ä¸€è‡´)")
print(f"   æ¿€æ´»å‡½æ•°: MC-AE1ç”¨custom_activation, MC-AE2ç”¨sigmoid (ä¸æºä»£ç ä¸€è‡´)")
print(f"   æ¢¯åº¦å¤„ç†: ç®€åŒ–ç‰ˆæœ¬ (ä¸æºä»£ç ä¸€è‡´)")
print(f"   è®­ç»ƒæ ·æœ¬: 0-9 (å…±{len(train_samples)}ä¸ªæ ·æœ¬)")

#----------------------------------------MC-AEè®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼‰------------------------
print("="*50)
print("é˜¶æ®µ1: å‡†å¤‡MC-AEè®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
def check_data_quality(data, name, sample_id=None):
    """è¯¦ç»†çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
    prefix = f"æ ·æœ¬ {sample_id} - " if sample_id else ""
    print(f"\nğŸ” {prefix}{name} æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # æ•°å€¼ç»Ÿè®¡
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    print(f"   æ•°å€¼èŒƒå›´: [{data_np.min():.6f}, {data_np.max():.6f}]")
    print(f"   å‡å€¼: {data_np.mean():.6f}")
    print(f"   æ ‡å‡†å·®: {data_np.std():.6f}")
    print(f"   ä¸­ä½æ•°: {np.median(data_np):.6f}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥
    nan_count = np.isnan(data_np).sum()
    inf_count = np.isinf(data_np).sum()
    zero_count = (data_np == 0).sum()
    negative_count = (data_np < 0).sum()
    
    print(f"   NaNæ•°é‡: {nan_count}")
    print(f"   Infæ•°é‡: {inf_count}")
    print(f"   é›¶å€¼æ•°é‡: {zero_count}")
    print(f"   è´Ÿå€¼æ•°é‡: {negative_count}")
    
    # å¼‚å¸¸å€¼æ¯”ä¾‹
    total_elements = data_np.size
    print(f"   NaNæ¯”ä¾‹: {nan_count/total_elements*100:.2f}%")
    print(f"   Infæ¯”ä¾‹: {inf_count/total_elements*100:.2f}%")
    print(f"   é›¶å€¼æ¯”ä¾‹: {zero_count/total_elements*100:.2f}%")
    print(f"   è´Ÿå€¼æ¯”ä¾‹: {negative_count/total_elements*100:.2f}%")
    
    # å¼‚å¸¸å€¼è­¦å‘Š
    if nan_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°NaNå€¼ï¼")
    if inf_count > 0:
        print(f"   âš ï¸  æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼")
    if data_np.min() < -1e6 or data_np.max() > 1e6:
        print(f"   âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¤§å€¼ï¼èŒƒå›´: [{data_np.min():.2e}, {data_np.max():.2e}]")
    
    return {
        'has_nan': nan_count > 0,
        'has_inf': inf_count > 0,
        'has_extreme_values': data_np.min() < -1e6 or data_np.max() > 1e6,
        'data_type': data.dtype,
        'shape': data.shape
    }

def physics_based_data_processing(data, name, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆå‚è€ƒè®ºæ–‡æ–¹æ³•ï¼‰"""
    print(f"\nğŸ”§ åŸºäºç‰©ç†çº¦æŸå¤„ç† {name}...")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç±»å‹
    if isinstance(data, np.ndarray):
        print(f"   åŸå§‹ç±»å‹: numpy.ndarray, dtype={data.dtype}")
    elif isinstance(data, torch.Tensor):
        print(f"   åŸå§‹ç±»å‹: torch.Tensor, dtype={data.dtype}")
    else:
        print(f"   åŸå§‹ç±»å‹: {type(data)}")
    
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    print(f"   åŸå§‹æ•°æ®ç‚¹æ•°é‡: {original_data_points}")
    
    print("   æ‰§è¡ŒåŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†...")
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤1: å¤„ç†ç¼ºå¤±æ•°æ®...")
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        print(f"     æ£€æµ‹åˆ° {complete_nan_rows.sum()} è¡Œå®Œå…¨ç¼ºå¤±çš„æ•°æ®")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡ç¼ºå¤±æ•°æ®å¤„ç†")
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
                print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢å…¨NaNè¡Œ")
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
                print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢")
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    print("   æ­¥éª¤2: å¤„ç†å¼‚å¸¸æ•°æ®...")
    
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        print(f"     å¤„ç†vin_2æ•°æ®ï¼ˆ225åˆ—ï¼‰")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºç”µå‹èŒƒå›´[0,5]Vçš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
            else:
                print(f"       ç”µå‹é¢„æµ‹åˆ—{col}: ç”µå‹å€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       ç”µå‹ç›¸å…³åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºç”µå‹èŒƒå›´[-5,5]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
            else:
                print(f"       ç”µå‹ç›¸å…³åˆ—{col}: ç”µå‹å€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        print(f"     å¤„ç†vin_3æ•°æ®ï¼ˆ226åˆ—ï¼‰ï¼Œç¬¬224åˆ—ä¸ºç‰¹æ®Šä¿ç•™åˆ—")
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       SOCé¢„æµ‹åˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
            else:
                print(f"       SOCé¢„æµ‹åˆ—{col}: SOCå€¼åœ¨æ­£å¸¸èŒƒå›´å†…")
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                print(f"       å•ä½“SOCåˆ—{col}: æ£€æµ‹åˆ° {col_invalid_count} ä¸ªè¶…å‡ºSOCèŒƒå›´[-0.2,2.0]çš„å¼‚å¸¸å€¼")
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       SOCåå·®åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            print(f"       æ¸©åº¦åˆ—{temp_col}: æ£€æµ‹åˆ° {temp_invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        special_col = 224
        print(f"       ç‰¹æ®Šä¿ç•™åˆ—{special_col}: ä¿æŒåŸå€¼ä¸å˜")
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            print(f"       ç”µæµåˆ—{current_col}: æ£€æµ‹åˆ° {current_invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-1004,162]Açš„å¼‚å¸¸å€¼")
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                print(f"       å…¶ä»–åˆ—{col}: æ£€æµ‹åˆ° {col_extreme_mask.sum()} ä¸ªæç«¯å¼‚å¸¸å€¼")
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
            
    elif feature_type == 'current':
        # ç”µæµç‰©ç†çº¦æŸï¼š-100Aåˆ°100A
        valid_mask = (data_np >= -100) & (data_np <= 100)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºç”µæµèŒƒå›´[-100,100]Açš„å¼‚å¸¸å€¼")
            data_np[data_np < -100] = -100
            data_np[data_np > 100] = 100
            
    elif feature_type == 'temperature':
        # æ¸©åº¦ç‰©ç†çº¦æŸï¼š-40Â°Cåˆ°80Â°C
        valid_mask = (data_np >= -40) & (data_np <= 80)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"     æ£€æµ‹åˆ° {invalid_count} ä¸ªè¶…å‡ºæ¸©åº¦èŒƒå›´[-40,80]Â°Cçš„å¼‚å¸¸å€¼")
            data_np[data_np < -40] = -40
            data_np[data_np > 80] = 80
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    print("   æ­¥éª¤3: å¤„ç†é‡‡æ ·æ•…éšœ...")
    
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        print(f"     æ£€æµ‹åˆ° {fault_mask.sum()} ä¸ªé‡‡æ ·æ•…éšœç‚¹")
        print(f"     ç”¨ä¸­ä½æ•°æ›¿æ¢æ•…éšœç‚¹ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡ä¸å˜")
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                print(f"       ç‰¹å¾{col}: ç‰¹æ®Šä¿ç•™åˆ—ï¼Œè·³è¿‡é‡‡æ ·æ•…éšœå¤„ç†")
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    print(f"       ç‰¹å¾{col}: ç”¨ä¸­ä½æ•° {median_val:.4f} æ›¿æ¢ {col_fault_mask.sum()} ä¸ªæ•…éšœå€¼")
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    print(f"       ç‰¹å¾{col}: å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢")
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    print("   æ­¥éª¤4: æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        print(f"     âš ï¸  ä»æœ‰ {final_nan_count} ä¸ªNaNå’Œ {final_inf_count} ä¸ªInfå€¼")
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    else:
        print("     âœ… æ‰€æœ‰å¼‚å¸¸å€¼å·²å¤„ç†å®Œæˆ")
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    # æ£€æŸ¥æ•°æ®ç‚¹æ•°é‡æ˜¯å¦ä¿æŒä¸€è‡´
    final_data_points = data_tensor.shape[0]
    if final_data_points == original_data_points:
        print(f"   å¤„ç†å®Œæˆ: {data_tensor.shape}, dtype={data_tensor.dtype}")
        print(f"   âœ… æ•°æ®ç‚¹æ•°é‡ä¿æŒä¸€è‡´: {original_data_points} -> {final_data_points}")
    else:
        print(f"   âš ï¸  æ•°æ®ç‚¹æ•°é‡å‘ç”Ÿå˜åŒ–: {original_data_points} -> {final_data_points}")
    
    return data_tensor

def physics_based_data_processing_silent(data, feature_type='general'):
    """åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼Œåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼‰"""
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œé¢„å¤„ç†
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    else:
        data_np = np.array(data)
    
    # è®°å½•åŸå§‹æ•°æ®ç‚¹æ•°é‡
    original_data_points = data_np.shape[0]
    
    # 1. å¤„ç†ç¼ºå¤±æ•°æ® (Missing Data) - ç”¨ä¸­ä½æ•°æ›¿æ¢å…¨NaNè¡Œï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    complete_nan_rows = np.isnan(data_np).all(axis=1)
    if complete_nan_rows.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—ä¸­ä½æ•°
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            valid_values = data_np[~np.isnan(data_np[:, col]), col]
            if len(valid_values) > 0:
                median_val = np.median(valid_values)
                # æ›¿æ¢å…¨NaNè¡Œä¸­è¯¥ç‰¹å¾çš„å€¼
                data_np[complete_nan_rows, col] = median_val
            else:
                # å¦‚æœè¯¥ç‰¹å¾å…¨éƒ¨ä¸ºNaNï¼Œç”¨0æ›¿æ¢
                data_np[complete_nan_rows, col] = 0.0
    
    # 2. å¤„ç†å¼‚å¸¸æ•°æ® (Abnormal Data) - åŸºäºç‰©ç†çº¦æŸè¿‡æ»¤
    if feature_type == 'vin2':
        # vin_2æ•°æ®å¤„ç†ï¼ˆ225åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPackç”µå‹é¢„æµ‹å€¼ - é™åˆ¶åœ¨[0,5]V
        voltage_pred_columns = [0, 1]
        for col in voltage_pred_columns:
            col_valid_mask = (data_np[:, col] >= 0) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < 0, col] = 0
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•2-221ï¼š220ä¸ªç‰¹å¾å€¼ - é™åˆ¶åœ¨[-5,5]èŒƒå›´å†…
        voltage_columns = list(range(2, 222))
        for col in voltage_columns:
            col_valid_mask = (data_np[:, col] >= -5) & (data_np[:, col] <= 5)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -5, col] = -5
                data_np[data_np[:, col] > 5, col] = 5
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 224
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            if col < data_np.shape[1]:
                col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
                if col_extreme_mask.any():
                    valid_values = data_np[~col_extreme_mask, col]
                    if len(valid_values) > 0:
                        median_val = np.median(valid_values)
                        data_np[col_extreme_mask, col] = median_val
    
    elif feature_type == 'vin3':
        # vin_3æ•°æ®å¤„ç†ï¼ˆ226åˆ—ï¼‰
        
        # ç´¢å¼•0,1ï¼šBiLSTMå’ŒPack SOCé¢„æµ‹å€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        soc_pred_columns = [0, 1]
        for col in soc_pred_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•2-111ï¼š110ä¸ªå•ä½“ç”µæ± çœŸå®SOCå€¼ - é™åˆ¶åœ¨[-0.2,2.0]
        cell_soc_columns = list(range(2, 112))
        for col in cell_soc_columns:
            col_valid_mask = (data_np[:, col] >= -0.2) & (data_np[:, col] <= 2.0)
            col_invalid_count = (~col_valid_mask).sum()
            if col_invalid_count > 0:
                data_np[data_np[:, col] < -0.2, col] = -0.2
                data_np[data_np[:, col] > 2.0, col] = 2.0
        
        # ç´¢å¼•112-221ï¼š110ä¸ªå•ä½“ç”µæ± SOCåå·®å€¼ - ä¸é™åˆ¶èŒƒå›´ï¼Œåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        soc_dev_columns = list(range(112, 222))
        for col in soc_dev_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
        
        # ç´¢å¼•222ï¼šç”µæ± æ¸©åº¦ - é™åˆ¶åœ¨åˆç†æ¸©åº¦èŒƒå›´[-40,80]Â°C
        temp_col = 222
        temp_valid_mask = (data_np[:, temp_col] >= -40) & (data_np[:, temp_col] <= 80)
        temp_invalid_count = (~temp_valid_mask).sum()
        if temp_invalid_count > 0:
            data_np[data_np[:, temp_col] < -40, temp_col] = -40
            data_np[data_np[:, temp_col] > 80, temp_col] = 80
        
        # ç´¢å¼•224ï¼šç‰¹æ®Šä¿ç•™åˆ— - ä¿æŒåŸå€¼ä¸å˜
        # ä¸éœ€è¦å¤„ç†ï¼Œä¿æŒåŸå€¼
        
        # ç´¢å¼•225ï¼šç”µæµæ•°æ® - é™åˆ¶åœ¨[-1004,162]A
        current_col = 225
        current_valid_mask = (data_np[:, current_col] >= -1004) & (data_np[:, current_col] <= 162)
        current_invalid_count = (~current_valid_mask).sum()
        if current_invalid_count > 0:
            data_np[data_np[:, current_col] < -1004, current_col] = -1004
            data_np[data_np[:, current_col] > 162, current_col] = 162
        
        # å…¶ä»–åˆ—ï¼ˆç´¢å¼•223ï¼‰ï¼šåªå¤„ç†æç«¯å¼‚å¸¸å€¼
        other_columns = [223]
        for col in other_columns:
            col_extreme_mask = np.isnan(data_np[:, col]) | np.isinf(data_np[:, col])
            if col_extreme_mask.any():
                valid_values = data_np[~col_extreme_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_extreme_mask, col] = median_val
    
    # 3. å¤„ç†é‡‡æ ·æ•…éšœ (Sampling Faults) - ç”¨ä¸­ä½æ•°æ›¿æ¢ï¼Œä¿æŒæ•°æ®ç‚¹æ•°é‡
    # æ£€æµ‹NaNå’ŒInfå€¼ï¼ˆå¯èƒ½æ˜¯é‡‡æ ·æ•…éšœï¼‰
    nan_mask = np.isnan(data_np)
    inf_mask = np.isinf(data_np)
    fault_mask = nan_mask | inf_mask
    
    if fault_mask.any():
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
        for col in range(data_np.shape[1]):
            # å¯¹äºvin_3æ•°æ®çš„ç¬¬224åˆ—ï¼Œè·³è¿‡å¤„ç†
            if data_np.shape[1] == 226 and col == 224:
                continue
                
            col_fault_mask = fault_mask[:, col]
            if col_fault_mask.any():
                # è®¡ç®—è¯¥åˆ—çš„ä¸­ä½æ•°ï¼ˆæ’é™¤æ•…éšœå€¼ï¼‰
                valid_values = data_np[~col_fault_mask, col]
                if len(valid_values) > 0:
                    median_val = np.median(valid_values)
                    data_np[col_fault_mask, col] = median_val
                else:
                    # å¦‚æœè¯¥åˆ—å…¨éƒ¨ä¸ºæ•…éšœå€¼ï¼Œç”¨0æ›¿æ¢
                    data_np[col_fault_mask, col] = 0.0
    
    # 4. æœ€ç»ˆæ£€æŸ¥
    final_nan_count = np.isnan(data_np).sum()
    final_inf_count = np.isinf(data_np).sum()
    
    if final_nan_count > 0 or final_inf_count > 0:
        # æœ€åçš„å®‰å…¨å¤„ç†
        data_np[np.isnan(data_np)] = 0.0
        data_np[np.isinf(data_np)] = 0.0
    
    # è½¬æ¢ä¸ºtensor
    data_tensor = torch.tensor(data_np, dtype=torch.float32)
    
    return data_tensor

# ä¸­æ–‡æ³¨é‡Šï¼šåŠ è½½MC-AEæ¨¡å‹è¾“å…¥ç‰¹å¾ï¼ˆvin_2.pklå’Œvin_3.pklï¼‰
# åˆå¹¶æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„vin_2å’Œvin_3æ•°æ®
all_vin2_data = []
all_vin3_data = []

# æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
sample_summary = {
    'total_samples': len(train_samples),
    'processed_samples': 0,
    'error_samples': 0,
    'total_vin2_issues_fixed': 0,
    'total_vin3_issues_fixed': 0
}

print("="*60)
print("ğŸ“¥ å¼€å§‹æ•°æ®åŠ è½½å’Œè´¨é‡æ£€æŸ¥")
print("="*60)

for sample_id in train_samples:
    vin2_path = f'../QAS/{sample_id}/vin_2.pkl'
    vin3_path = f'../QAS/{sample_id}/vin_3.pkl'
    
    # åŠ è½½åŸå§‹vin_2æ•°æ®
    try:
        with open(vin2_path, 'rb') as file:
            vin2_data = pickle.load(file)
        
        # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        vin2_tensor = physics_based_data_processing_silent(vin2_data, feature_type='vin2')
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ ·æœ¬ {sample_id} çš„vin_2æ•°æ®å¤±è´¥: {e}")
        sample_summary['error_samples'] += 1
        continue
    
    # åŠ è½½åŸå§‹vin_3æ•°æ®
    try:
        with open(vin3_path, 'rb') as file:
            vin3_data = pickle.load(file)
        
        # åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®å¤„ç†ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        vin3_tensor = physics_based_data_processing_silent(vin3_data, feature_type='vin3')
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ ·æœ¬ {sample_id} çš„vin_3æ•°æ®å¤±è´¥: {e}")
        sample_summary['error_samples'] += 1
        continue
    
    # æ·»åŠ åˆ°åˆ—è¡¨
    all_vin2_data.append(vin2_tensor)
    all_vin3_data.append(vin3_tensor)
    sample_summary['processed_samples'] += 1
    
    # æ¯å¤„ç†10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
    if sample_summary['processed_samples'] % 10 == 0:
        print(f"ğŸ“Š å·²å¤„ç† {sample_summary['processed_samples']}/{sample_summary['total_samples']} ä¸ªæ ·æœ¬")

# è¾“å‡ºå¤„ç†æ±‡æ€»ä¿¡æ¯
print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
print(f"   æ€»æ ·æœ¬æ•°: {sample_summary['total_samples']}")
print(f"   æˆåŠŸå¤„ç†: {sample_summary['processed_samples']}")
print(f"   å¤„ç†å¤±è´¥: {sample_summary['error_samples']}")
print(f"   æˆåŠŸç‡: {sample_summary['processed_samples']/sample_summary['total_samples']*100:.1f}%")

# åˆå¹¶æ•°æ®
print("\n" + "="*60)
print("ğŸ”— åˆå¹¶æ‰€æœ‰æ ·æœ¬æ•°æ®")
print("="*60)

combined_tensor = torch.cat(all_vin2_data, dim=0)
combined_tensorx = torch.cat(all_vin3_data, dim=0)

print(f"åˆå¹¶åvin_2æ•°æ®å½¢çŠ¶: {combined_tensor.shape}")
print(f"åˆå¹¶åvin_3æ•°æ®å½¢çŠ¶: {combined_tensorx.shape}")

# ç®€è¦æ£€æŸ¥åˆå¹¶åçš„æ•°æ®è´¨é‡
print("\nğŸ” åˆå¹¶åæ•°æ®è´¨é‡ç®€è¦æ£€æŸ¥:")
vin2_nan_count = torch.isnan(combined_tensor).sum().item()
vin2_inf_count = torch.isinf(combined_tensor).sum().item()
vin3_nan_count = torch.isnan(combined_tensorx).sum().item()
vin3_inf_count = torch.isinf(combined_tensorx).sum().item()

print(f"   vin_2: NaN={vin2_nan_count}, Inf={vin2_inf_count}")
print(f"   vin_3: NaN={vin3_nan_count}, Inf={vin3_inf_count}")

# æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼éœ€è¦å¤„ç†
vin2_has_issues = (vin2_nan_count > 0 or vin2_inf_count > 0)
vin3_has_issues = (vin3_nan_count > 0 or vin3_inf_count > 0)

if vin2_has_issues or vin3_has_issues:
    print("âš ï¸  æ£€æµ‹åˆ°æ•°æ®é—®é¢˜ï¼Œè¿›è¡Œä¿®å¤...")
    
    # ä¿®å¤NaNå’ŒInfå€¼
    if vin2_has_issues:
        combined_tensor = torch.where(torch.isnan(combined_tensor) | torch.isinf(combined_tensor), 
                                     torch.zeros_like(combined_tensor), combined_tensor)
        print("   âœ… vin_2æ•°æ®ä¿®å¤å®Œæˆ")
    
    if vin3_has_issues:
        combined_tensorx = torch.where(torch.isnan(combined_tensorx) | torch.isinf(combined_tensorx), 
                                      torch.zeros_like(combined_tensorx), combined_tensorx)
        print("   âœ… vin_3æ•°æ®ä¿®å¤å®Œæˆ")
else:
    print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®å¤")

#----------------------------------------MC-AEå¤šé€šé“è‡ªç¼–ç å™¨è®­ç»ƒ--------------------------
print("="*50)
print("é˜¶æ®µ2: è®­ç»ƒMC-AEå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹BiLSTMæ•°æ®ï¼‰")
print("="*50)

# ä¸­æ–‡æ³¨é‡Šï¼šå®šä¹‰ç‰¹å¾åˆ‡ç‰‡ç»´åº¦
# vin_2.pkl
dim_x = 2
dim_y = 110
dim_z = 110
dim_q = 3

# ä¸­æ–‡æ³¨é‡Šï¼šåˆ†å‰²ç‰¹å¾å¼ é‡
x_recovered = combined_tensor[:, :dim_x]
y_recovered = combined_tensor[:, dim_x:dim_x + dim_y]
z_recovered = combined_tensor[:, dim_x + dim_y: dim_x + dim_y + dim_z]
q_recovered = combined_tensor[:, dim_x + dim_y + dim_z:]

# vin_3.pkl
dim_x2 = 2
dim_y2 = 110
dim_z2 = 110
dim_q2= 4

x_recovered2 = combined_tensorx[:, :dim_x2]
y_recovered2 = combined_tensorx[:, dim_x2:dim_x2 + dim_y2]
z_recovered2 = combined_tensorx[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
q_recovered2 = combined_tensorx[:, dim_x2 + dim_y2 + dim_z2:]

# è®­ç»ƒè¶…å‚æ•°é…ç½®ï¼ˆå·²åœ¨å‰é¢å®šä¹‰ï¼‰

# ç”¨äºè®°å½•è®­ç»ƒæŸå¤±
train_losses_mcae1 = []
train_losses_mcae2 = []

# ä¸­æ–‡æ³¨é‡Šï¼šè‡ªå®šä¹‰å¤šè¾“å…¥æ•°æ®é›†ç±»ï¼ˆæœ¬åœ°å®šä¹‰ï¼ŒéClass_.pyä¸­çš„Datasetï¼‰
class MultiInputDataset(Dataset):
    def __init__(self, x, y, z, q):
        self.x = x.to(torch.float32)
        self.y = y.to(torch.float32)
        self.z = z.to(torch.float32)
        self.q = q.to(torch.float32)
    def __len__(self):
        return len(self.x)
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx], self.z[idx], self.q[idx]

# ä¸­æ–‡æ³¨é‡Šï¼šç”¨DataLoaderæ‰¹é‡åŠ è½½å¤šé€šé“ç‰¹å¾æ•°æ®
train_loader_u = DataLoader(MultiInputDataset(x_recovered, y_recovered, z_recovered, q_recovered), batch_size=BATCHSIZE, shuffle=False)

# ä¸­æ–‡æ³¨é‡Šï¼šåˆå§‹åŒ–MC-AEæ¨¡å‹ï¼ˆä½¿ç”¨float32ï¼‰
net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110, activation_fn=custom_activation, use_dx_in_forward=True).to(device).to(torch.float32)

netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device).to(torch.float32)

# ä½¿ç”¨æ›´ç¨³å®šçš„æƒé‡åˆå§‹åŒ–
def stable_weight_init(model):
    """ä½¿ç”¨æ›´ç¨³å®šçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
    for module in model.modules():
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œä½†é™åˆ¶æƒé‡èŒƒå›´
            nn.init.xavier_uniform_(module.weight, gain=0.3)  # é™ä½gainå€¼é¿å…æ¢¯åº¦çˆ†ç‚¸
            if module.bias is not None:
                nn.init.zeros_(module.bias)

# åº”ç”¨ç¨³å®šçš„æƒé‡åˆå§‹åŒ–
stable_weight_init(net)
stable_weight_init(netx)
print("âœ… åº”ç”¨ç¨³å®šçš„æƒé‡åˆå§‹åŒ–")

# å•GPUä¼˜åŒ–æ¨¡å¼ï¼ˆå°æ ·æœ¬è®­ç»ƒï¼Œé¿å…è·¨å¡é€šä¿¡å¼€é”€ï¼‰
print("ğŸ”§ å•GPUä¼˜åŒ–æ¨¡å¼ï¼šé¿å…æ•°æ®å¹¶è¡Œå¼€é”€ï¼Œä¸“æ³¨äºå°æ ·æœ¬è®­ç»ƒ")

optimizer = torch.optim.Adam(net.parameters(), lr=INIT_LR)
l1_lambda = 0.01
loss_f = nn.MSELoss()

# ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå‚è€ƒæºä»£ç ï¼‰
# scaler = torch.cuda.amp.GradScaler()
# print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

# å¯ç”¨CUDAæ€§èƒ½ä¼˜åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
print("âœ… å¯ç”¨CUDAæ€§èƒ½ä¼˜åŒ– (cudnn.benchmark)")
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    grad_too_small_count = 0  # åˆå§‹åŒ–æ¢¯åº¦è¿‡å°ç»Ÿè®¡
    grad_norms = []  # æ”¶é›†æ¢¯åº¦èŒƒæ•°ç”¨äºç›‘æ§
    
    # æ›´æ–°å­¦ä¹ ç‡
    current_lr = get_lr(epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr
    
    for iteration, (x, y, z, q) in enumerate(train_loader_u):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰
        if torch.isnan(x).any() or torch.isinf(x).any() or torch.isnan(y).any() or torch.isinf(y).any():
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®åŒ…å«NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†ï¼ˆæ›´ä¸¥æ ¼çš„é™åˆ¶ï¼‰
        if x.abs().max() > 100 or y.abs().max() > 100:
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
            continue
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ŒæŒ‰ç…§æºä»£ç çš„æ–¹å¼å¤„ç†
        net = net.float()  # ç¡®ä¿æ¨¡å‹ä½¿ç”¨float32
        recon_im, recon_p = net(x, z, q)
        loss_u = loss_f(y, recon_im)
            
                    # ç®€åŒ–æŸå¤±æ£€æŸ¥ï¼ˆå‚è€ƒæºä»£ç ï¼‰
        if torch.isnan(loss_u) or torch.isinf(loss_u):
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†
        if x.abs().max() > 1000 or y.abs().max() > 1000:
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
            continue
        
        total_loss += loss_u.item()
        num_batches += 1
        optimizer.zero_grad()
        loss_u.backward()
        
        # ç®€åŒ–çš„æ¢¯åº¦å¤„ç†ï¼ˆå‚è€ƒæºä»£ç ï¼‰
        grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), MAX_GRAD_NORM)
        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ¢¯åº¦å¼‚å¸¸ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ”¶é›†æ¢¯åº¦èŒƒæ•°ç”¨äºç›‘æ§
        grad_norms.append(grad_norm.item())
            
        optimizer.step()
    
    avg_loss = total_loss / num_batches
    train_losses_mcae1.append(avg_loss)
    
    # æ¢¯åº¦ç»Ÿè®¡æ€»ç»“
    avg_grad_norm = np.mean(grad_norms) if grad_norms else 0
    if grad_too_small_count > 0:
        grad_percentage = (grad_too_small_count / num_batches) * 100
        print(f'MC-AE1 Epoch: {epoch:2d} | Average Loss: {avg_loss:.6f} | æ¢¯åº¦è¿‡å°: {grad_too_small_count}/{num_batches} ({grad_percentage:.1f}%) | å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.4f}')
    else:
        if epoch % 50 == 0:
            print('MC-AE1 Epoch: {:2d} | Average Loss: {:.6f} | å¹³å‡æ¢¯åº¦èŒƒæ•°: {:.4f}'.format(epoch, avg_loss, avg_grad_norm))
        elif epoch % 10 == 0:  # æ¯10ä¸ªepochè¾“å‡ºä¸€æ¬¡è¿›åº¦
            print('MC-AE1 Epoch: {:2d} | Average Loss: {:.6f} | å¹³å‡æ¢¯åº¦èŒƒæ•°: {:.4f}'.format(epoch, avg_loss, avg_grad_norm))
            # GPUåˆ©ç”¨ç‡ç›‘æ§
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"   GPUæ˜¾å­˜: {gpu_memory_used:.1f}GB / {gpu_memory_total:.1f}GB ({gpu_memory_used/gpu_memory_total*100:.1f}%)")

# ä¸­æ–‡æ³¨é‡Šï¼šå…¨é‡æ¨ç†ï¼Œè·å¾—é‡æ„è¯¯å·®
train_loader2 = DataLoader(MultiInputDataset(x_recovered, y_recovered, z_recovered, q_recovered), batch_size=len(x_recovered), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loader2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    net = net.float()
    recon_imtest, recon = net(x, z, q)
AA = recon_imtest.cpu().detach().numpy()
yTrainU = y_recovered.cpu().detach().numpy()
ERRORU = AA - yTrainU

# ä¸­æ–‡æ³¨é‡Šï¼šç¬¬äºŒç»„ç‰¹å¾çš„MC-AEè®­ç»ƒ
train_loader_soc = DataLoader(MultiInputDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), batch_size=BATCHSIZE, shuffle=False)
optimizer = torch.optim.Adam(netx.parameters(), lr=INIT_LR)
loss_f = nn.MSELoss()

# ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå‚è€ƒæºä»£ç ï¼‰
# scaler2 = torch.cuda.amp.GradScaler()

avg_loss_list_x = []
for epoch in range(EPOCH):
    total_loss = 0
    num_batches = 0
    grad_too_small_count_x = 0  # åˆå§‹åŒ–ç¬¬äºŒä¸ªæ¨¡å‹çš„æ¢¯åº¦è¿‡å°ç»Ÿè®¡
    grad_norms_x = []  # æ”¶é›†ç¬¬äºŒä¸ªæ¨¡å‹çš„æ¢¯åº¦èŒƒæ•°
    
    # æ›´æ–°å­¦ä¹ ç‡
    current_lr = get_lr(epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr
    
    for iteration, (x, y, z, q) in enumerate(train_loader_soc):
        x = x.to(device)
        y = y.to(device)
        z = z.to(device)
        q = q.to(device)
        
        # MC-AE2è¾“å…¥æ•°æ®æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if torch.isnan(x).any() or torch.isinf(x).any() or torch.isnan(y).any() or torch.isinf(y).any():
            print(f"MC-AE2è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®åŒ…å«NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†ï¼ˆæ›´ä¸¥æ ¼çš„é™åˆ¶ï¼‰
        if x.abs().max() > 100 or y.abs().max() > 100:
            print(f"MC-AE2è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
            continue
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ŒæŒ‰ç…§æºä»£ç çš„æ–¹å¼å¤„ç†
        netx = netx.float()  # ç¡®ä¿æ¨¡å‹ä½¿ç”¨float32
        recon_im, z = netx(x, z, q)
        loss_x = loss_f(y, recon_im)
            
                    # ç®€åŒ–æŸå¤±æ£€æŸ¥ï¼ˆå‚è€ƒæºä»£ç ï¼‰
        if torch.isnan(loss_x) or torch.isinf(loss_x):
            print(f"MC-AE2è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†
        if x.abs().max() > 1000 or y.abs().max() > 1000:
            print(f"è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡è¾“å…¥æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            print(f"xèŒƒå›´: [{x.min():.4f}, {x.max():.4f}]")
            print(f"yèŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
            continue
        
        total_loss += loss_x.item()
        num_batches += 1
        optimizer.zero_grad()
        loss_x.backward()
        
        # ç®€åŒ–çš„æ¢¯åº¦å¤„ç†ï¼ˆå‚è€ƒæºä»£ç ï¼‰
        grad_norm = torch.nn.utils.clip_grad_norm_(netx.parameters(), MAX_GRAD_NORM)
        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
            print(f"MC-AE2è­¦å‘Šï¼šç¬¬{epoch}è½®ç¬¬{iteration}æ‰¹æ¬¡æ¢¯åº¦å¼‚å¸¸ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
        
        # æ”¶é›†æ¢¯åº¦èŒƒæ•°ç”¨äºç›‘æ§
        grad_norms_x.append(grad_norm.item())
            
        optimizer.step()
    
    avg_loss = total_loss / num_batches
    avg_loss_list_x.append(avg_loss)
    train_losses_mcae2.append(avg_loss)
    
    # æ¢¯åº¦ç»Ÿè®¡æ€»ç»“
    avg_grad_norm_x = np.mean(grad_norms_x) if grad_norms_x else 0
    if grad_too_small_count_x > 0:
        grad_percentage_x = (grad_too_small_count_x / num_batches) * 100
        print(f'MC-AE2 Epoch: {epoch:2d} | Average Loss: {avg_loss:.6f} | æ¢¯åº¦è¿‡å°: {grad_too_small_count_x}/{num_batches} ({grad_percentage_x:.1f}%) | å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm_x:.4f}')
    else:
        if epoch % 50 == 0:
            print('MC-AE2 Epoch: {:2d} | Average Loss: {:.6f} | å¹³å‡æ¢¯åº¦èŒƒæ•°: {:.4f}'.format(epoch, avg_loss, avg_grad_norm_x))
        elif epoch % 10 == 0:  # æ¯10ä¸ªepochè¾“å‡ºä¸€æ¬¡è¿›åº¦
            print('MC-AE2 Epoch: {:2d} | Average Loss: {:.6f} | å¹³å‡æ¢¯åº¦èŒƒæ•°: {:.4f}'.format(epoch, avg_loss, avg_grad_norm_x))
            # GPUåˆ©ç”¨ç‡ç›‘æ§
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"   GPUæ˜¾å­˜: {gpu_memory_used:.1f}GB / {gpu_memory_total:.1f}GB ({gpu_memory_used/gpu_memory_total*100:.1f}%)")

train_loaderx2 = DataLoader(MultiInputDataset(x_recovered2, y_recovered2, z_recovered2, q_recovered2), batch_size=len(x_recovered2), shuffle=False)
for iteration, (x, y, z, q) in enumerate(train_loaderx2):
    x = x.to(device)
    y = y.to(device)
    z = z.to(device)
    q = q.to(device)
    netx = netx.float()
    recon_imtestx, z = netx(x, z, q)

BB = recon_imtestx.cpu().detach().numpy()
yTrainX = y_recovered2.cpu().detach().numpy()
ERRORX = BB - yTrainX

# åˆ›å»ºç»“æœç›®å½•
result_dir = '/mnt/bz25t/bzhy/datasave/BILSTM/models'
if not os.path.exists(result_dir):
    os.makedirs(result_dir)
    print(f"âœ… åˆ›å»ºç»“æœç›®å½•: {result_dir}")
else:
    print(f"âœ… ä½¿ç”¨ç°æœ‰ç»“æœç›®å½•: {result_dir}")

# ä¸­æ–‡æ³¨é‡Šï¼šè¯Šæ–­ç‰¹å¾æå–ä¸PCAåˆ†æ
df_data = DiagnosisFeature(ERRORU,ERRORX)

v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = PCA(df_data,0.95,0.95)

# è®­ç»ƒç»“æŸåè‡ªåŠ¨ä¿å­˜æ¨¡å‹å’Œåˆ†æç»“æœ
print("="*50)
print("ä¿å­˜BiLSTMåŸºå‡†è®­ç»ƒç»“æœ")
print("="*50)

# ç»˜åˆ¶è®­ç»ƒç»“æœ
print("ğŸ“ˆ ç»˜åˆ¶BiLSTMè®­ç»ƒæ›²çº¿...")

# Linuxç¯å¢ƒmatplotlibé…ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# Linuxç¯å¢ƒå­—ä½“è®¾ç½® - ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
import matplotlib.font_manager as fm
import os

# å°è¯•å¤šç§å­—ä½“æ–¹æ¡ˆ
font_options = [
    'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC',
    'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS'
]

# æ£€æŸ¥å¯ç”¨å­—ä½“
available_fonts = []
for font in font_options:
    try:
        fm.findfont(font)
        available_fonts.append(font)
    except:
        continue

# è®¾ç½®å­—ä½“
if available_fonts:
    plt.rcParams['font.sans-serif'] = available_fonts
    print(f"âœ… ä½¿ç”¨å­—ä½“: {available_fonts[0]}")
else:
    # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10

# åˆ›å»ºå›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# å­å›¾1: MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿
ax1 = axes[0, 0]
epochs = range(1, len(train_losses_mcae1) + 1)
ax1.plot(epochs, train_losses_mcae1, 'b-', linewidth=2, label='MC-AE1 Training Loss')
ax1.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
ax1.set_ylabel('MSE Loss / MSEæŸå¤±')
ax1.set_title('MC-AE1 Training Loss / MC-AE1è®­ç»ƒæŸå¤±æ›²çº¿')
ax1.grid(True, alpha=0.3)
ax1.legend()
ax1.set_yscale('log')

# å­å›¾2: MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿ 
ax2 = axes[0, 1]
ax2.plot(epochs, train_losses_mcae2, 'r-', linewidth=2, label='MC-AE2 Training Loss')
ax2.set_xlabel('Training Epochs / è®­ç»ƒè½®æ•°')
ax2.set_ylabel('MSE Loss / MSEæŸå¤±')
ax2.set_title('MC-AE2 Training Loss / MC-AE2è®­ç»ƒæŸå¤±æ›²çº¿')
ax2.grid(True, alpha=0.3)
ax2.legend()
ax2.set_yscale('log')

# å­å›¾3: MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ
ax3 = axes[1, 0]
reconstruction_errors_1 = ERRORU.flatten()
mean_error_1 = np.mean(np.abs(reconstruction_errors_1))
ax3.hist(np.abs(reconstruction_errors_1), bins=50, alpha=0.7, color='blue', 
         label=f'MC-AE1 Reconstruction Error (Mean: {mean_error_1:.4f}) / MC-AE1é‡æ„è¯¯å·® (å‡å€¼: {mean_error_1:.4f})')
ax3.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
ax3.set_ylabel('Frequency / é¢‘æ•°')
ax3.set_title('MC-AE1 Reconstruction Error Distribution / MC-AE1é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax3.legend()
ax3.grid(True, alpha=0.3)

# å­å›¾4: MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ
ax4 = axes[1, 1]
reconstruction_errors_2 = ERRORX.flatten()
mean_error_2 = np.mean(np.abs(reconstruction_errors_2))
ax4.hist(np.abs(reconstruction_errors_2), bins=50, alpha=0.7, color='red',
         label=f'MC-AE2 Reconstruction Error (Mean: {mean_error_2:.4f}) / MC-AE2é‡æ„è¯¯å·® (å‡å€¼: {mean_error_2:.4f})')
ax4.set_xlabel('Absolute Reconstruction Error / ç»å¯¹é‡æ„è¯¯å·®')
ax4.set_ylabel('Frequency / é¢‘æ•°')
ax4.set_title('MC-AE2 Reconstruction Error Distribution / MC-AE2é‡æ„è¯¯å·®åˆ†å¸ƒ')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f'{result_dir}/bilstm_training_results.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"âœ… BiLSTMè®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {result_dir}/bilstm_training_results.png")

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨ï¼ˆå·²åœ¨å‰é¢åˆ›å»ºï¼‰

# 2. ä¿å­˜è¯Šæ–­ç‰¹å¾DataFrameï¼ˆé¿å…Excelæ–‡ä»¶è¿‡å¤§ï¼‰
try:
    # æ£€æŸ¥DataFrameå¤§å°
    rows, cols = df_data.shape
    print(f"ğŸ“Š è¯Šæ–­ç‰¹å¾DataFrameå¤§å°: {rows}è¡Œ x {cols}åˆ—")
    
    if rows > 1000000:  # å¦‚æœè¶…è¿‡100ä¸‡è¡Œï¼Œåªä¿å­˜CSV
        print(f"âš ï¸  DataFrameè¿‡å¤§({rows}è¡Œ)ï¼Œè·³è¿‡Excelä¿å­˜ï¼Œåªä¿å­˜CSVæ–‡ä»¶")
        df_data.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline.csv', index=False)
        print(f"âœ“ ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline.csv")
    else:
        # å°è¯•ä¿å­˜Excelï¼Œå¦‚æœå¤±è´¥åˆ™åªä¿å­˜CSV
        try:
            df_data.to_excel(f'{result_dir}/diagnosis_feature_bilstm_baseline.xlsx', index=False)
            df_data.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline.csv', index=False)
            print(f"âœ“ ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline.xlsx/csv")
        except ValueError as e:
            print(f"âš ï¸  Excelä¿å­˜å¤±è´¥: {e}")
            print("   åªä¿å­˜CSVæ–‡ä»¶")
            df_data.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline.csv', index=False)
            print(f"âœ“ ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline.csv")
except Exception as e:
    print(f"âŒ ä¿å­˜è¯Šæ–­ç‰¹å¾å¤±è´¥: {e}")
    # å°è¯•åˆ†å—ä¿å­˜
    try:
        chunk_size = 500000  # 50ä¸‡è¡Œä¸€ä¸ªæ–‡ä»¶
        for i in range(0, len(df_data), chunk_size):
            chunk = df_data.iloc[i:i+chunk_size]
            chunk.to_csv(f'{result_dir}/diagnosis_feature_bilstm_baseline_part_{i//chunk_size+1}.csv', index=False)
        print(f"âœ“ åˆ†å—ä¿å­˜è¯Šæ–­ç‰¹å¾: {result_dir}/diagnosis_feature_bilstm_baseline_part_*.csv")
    except Exception as e2:
        print(f"âŒ åˆ†å—ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")

# 3. ä¿å­˜PCAåˆ†æä¸»è¦ç»“æœ
np.save(f'{result_dir}/v_I_bilstm_baseline.npy', v_I)
np.save(f'{result_dir}/v_bilstm_baseline.npy', v)
np.save(f'{result_dir}/v_ratio_bilstm_baseline.npy', v_ratio)
np.save(f'{result_dir}/p_k_bilstm_baseline.npy', p_k)
np.save(f'{result_dir}/data_mean_bilstm_baseline.npy', data_mean)
np.save(f'{result_dir}/data_std_bilstm_baseline.npy', data_std)
np.save(f'{result_dir}/T_95_limit_bilstm_baseline.npy', T_95_limit)
np.save(f'{result_dir}/T_99_limit_bilstm_baseline.npy', T_99_limit)
np.save(f'{result_dir}/SPE_95_limit_bilstm_baseline.npy', SPE_95_limit)
np.save(f'{result_dir}/SPE_99_limit_bilstm_baseline.npy', SPE_99_limit)
np.save(f'{result_dir}/P_bilstm_baseline.npy', P)
np.save(f'{result_dir}/k_bilstm_baseline.npy', k)
np.save(f'{result_dir}/P_t_bilstm_baseline.npy', P_t)
np.save(f'{result_dir}/X_bilstm_baseline.npy', X)
np.save(f'{result_dir}/data_nor_bilstm_baseline.npy', data_nor)
print(f"âœ“ ä¿å­˜PCAåˆ†æç»“æœ: {result_dir}/*_bilstm_baseline.npy")

# 4. ä¿å­˜CombinedAEæ¨¡å‹å‚æ•°
torch.save(net.state_dict(), f'{result_dir}/net_model_bilstm_baseline.pth')
torch.save(netx.state_dict(), f'{result_dir}/netx_model_bilstm_baseline.pth')
print(f"âœ“ ä¿å­˜MC-AEæ¨¡å‹: {result_dir}/net_model_bilstm_baseline.pth, {result_dir}/netx_model_bilstm_baseline.pth")

# 5. ä¿å­˜è®­ç»ƒå†å²
training_history = {
    'mcae1_losses': train_losses_mcae1,
    'mcae2_losses': train_losses_mcae2,
    'final_mcae1_loss': train_losses_mcae1[-1],
    'final_mcae2_loss': train_losses_mcae2[-1],
    'mcae1_reconstruction_error_mean': np.mean(np.abs(ERRORU)),
    'mcae1_reconstruction_error_std': np.std(np.abs(ERRORU)),
    'mcae2_reconstruction_error_mean': np.mean(np.abs(ERRORX)),
    'mcae2_reconstruction_error_std': np.std(np.abs(ERRORX)),
    'training_samples': len(train_samples),
    'epochs': EPOCH,
    'learning_rate': INIT_LR,
    'batch_size': BATCHSIZE
}

import pickle
with open(f'{result_dir}/bilstm_training_history.pkl', 'wb') as f:
    pickle.dump(training_history, f)
print(f"âœ“ ä¿å­˜è®­ç»ƒå†å²: {result_dir}/bilstm_training_history.pkl")

print("="*50)
print("ğŸ‰ BiLSTMåŸºçº¿è®­ç»ƒå®Œæˆï¼")
print("="*50)
print("BiLSTMåŸºçº¿è®­ç»ƒæ¨¡å¼æ€»ç»“ï¼ˆä¸æºä»£ç Train_.pyå¯¹é½ï¼‰ï¼š")
print("1. âœ… è·³è¿‡Transformerè®­ç»ƒé˜¶æ®µ")
print("2. âœ… ç›´æ¥ä½¿ç”¨åŸå§‹vin_2[x[0]]å’Œvin_3[x[0]]æ•°æ®")
print("3. âœ… ä¿æŒPack Modelingè¾“å‡ºvin_2[x[1]]å’Œvin_3[x[1]]ä¸å˜")
print("4. âœ… MC-AEä½¿ç”¨åŸå§‹BiLSTMæ•°æ®è¿›è¡Œè®­ç»ƒ")
print("5. âœ… ä½¿ç”¨0-9æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼ˆå…±{len(train_samples)}ä¸ªæ ·æœ¬ï¼‰")
print("6. âœ… æ‰€æœ‰æ¨¡å‹å’Œç»“æœæ–‡ä»¶æ·»åŠ '_bilstm_baseline'åç¼€")
print("")
print("ğŸ”§ ä¸æºä»£ç ä¸€è‡´çš„å…³é”®å‚æ•°ï¼š")
print(f"   - è®­ç»ƒè½®æ•°: {EPOCH} (æºä»£ç : 300)")
print(f"   - å­¦ä¹ ç‡: {INIT_LR} (æºä»£ç : 5e-4)")
print(f"   - æ‰¹æ¬¡å¤§å°: {BATCHSIZE} (æºä»£ç : 100)")
print("   - æ¿€æ´»å‡½æ•°: MC-AE1ç”¨custom_activation, MC-AE2ç”¨sigmoid")
print("   - æ•°æ®ç±»å‹: float32, æ¢¯åº¦å¤„ç†: ç®€åŒ–ç‰ˆæœ¬")
print("")
print(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {result_dir}")
print("   - è®­ç»ƒç»“æœå›¾: bilstm_training_results.png")
print("   - è¯Šæ–­ç‰¹å¾: diagnosis_feature_bilstm_baseline.csv")
print("   - æ¨¡å‹å‚æ•°: net_model_bilstm_baseline.pth, netx_model_bilstm_baseline.pth")
print("   - PCAåˆ†æç»“æœ: *_bilstm_baseline.npy")
print("   - è®­ç»ƒå†å²: bilstm_training_history.pkl") 