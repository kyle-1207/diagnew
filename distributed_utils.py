import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # è·å–ç¯å¢ƒå˜é‡
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    world_size = int(os.environ.get("WORLD_SIZE", -1))
    
    if local_rank != -1:
        if not dist.is_initialized():
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            dist.init_process_group(backend='nccl')
            # è®¾ç½®å½“å‰è®¾å¤‡
            torch.cuda.set_device(local_rank)
            
        print(f"ğŸ–¥ï¸ è¿›ç¨‹ {local_rank}/{world_size-1} åˆå§‹åŒ–å®Œæˆ")
        return True, local_rank, world_size
    return False, 0, 1

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()
        print("ğŸ§¹ åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå·²æ¸…ç†")

def to_distributed(model, local_rank):
    """å°†æ¨¡å‹è½¬æ¢ä¸ºåˆ†å¸ƒå¼æ¨¡å‹"""
    if dist.is_initialized():
        model = model.cuda(local_rank)
        model = DDP(model, device_ids=[local_rank])
        return model
    return model.cuda()

def create_distributed_loader(dataset, batch_size, num_workers=4):
    """åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
    if dist.is_initialized():
        sampler = DistributedSampler(dataset)
        loader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=True
        )
        return loader, sampler
    else:
        loader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True
        )
        return loader, None

def is_main_process():
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    if dist.is_initialized():
        return dist.get_rank() == 0
    return True

def barrier():
    """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
    if dist.is_initialized():
        dist.barrier()

def reduce_value(value, average=True):
    """å½’çº¦å€¼åˆ°æ‰€æœ‰è¿›ç¨‹"""
    if not dist.is_initialized():
        return value
    
    with torch.no_grad():
        dist.all_reduce(value)
        if average:
            value /= dist.get_world_size()
    return value