# ä¸‰æ¨¡å‹ç»¼åˆæ¯”è¾ƒå¯è§†åŒ–è„šæœ¬
# å¯¹æ¯” BiLSTM, Transformer_Positive, Transformer_PN ä¸‰ä¸ªæ¨¡å‹çš„æ€§èƒ½
# 
# ä½œè€…: AI Assistant
# åˆ›å»ºæ—¶é—´: 2024
# 

import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
import json
import pickle
import glob
from datetime import datetime
import seaborn as sns
from sklearn.metrics import roc_curve, auc, confusion_matrix
import platform

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
import matplotlib.font_manager as fm
from matplotlib import rcParams

def setup_chinese_fonts_strict():
    """LinuxæœåŠ¡å™¨ç¯å¢ƒä¸­æ–‡å­—ä½“é…ç½®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    import subprocess
    import os
    
    # 1. å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…ï¼ˆä»…Linuxï¼‰
    if platform.system() == "Linux":
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡ç†å‘˜æƒé™å®‰è£…å­—ä½“
            result = subprocess.run(['which', 'apt-get'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("ğŸ”§ æ­£åœ¨å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…...")
                subprocess.run(['sudo', 'apt-get', 'update'], capture_output=True, timeout=30)
                subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-noto-cjk', 'fonts-wqy-microhei', 'fonts-arphic-ukai'], capture_output=True, timeout=60)
        except Exception as e:
            print(f"âš ï¸ å­—ä½“å®‰è£…å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰: {e}")
    
    # 2. æ‰©å±•å€™é€‰å­—ä½“åˆ—è¡¨
    candidates = [
        # Linuxä¼˜å…ˆå­—ä½“
        'Noto Sans CJK SC Regular',
        'Noto Sans CJK SC',
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei',
        'Source Han Sans CN',
        'Source Han Sans SC',
        'AR PL UKai CN',
        'AR PL UMing CN',
        # é€šç”¨å­—ä½“
        'Droid Sans Fallback',
        'Liberation Sans',
        # Windowså…œåº•
        'Microsoft YaHei',
        'SimHei',
        'SimSun',
        # æœ€ç»ˆå…œåº•
        'DejaVu Sans',
        'Liberation Sans'
    ]

    chosen = None
    for name in candidates:
        try:
            font_path = fm.findfont(name, fallback_to_default=False)
            if font_path and 'DejaVu' not in font_path and os.path.exists(font_path):
                chosen = name
                print(f"ğŸ” æ‰¾åˆ°å­—ä½“: {name} -> {font_path}")
                break
        except Exception:
            continue

    # 3. å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚å­—ä½“ï¼Œå°è¯•ç³»ç»Ÿå­—ä½“æ‰«æ
    if chosen is None:
        print("ğŸ” è¿›è¡Œç³»ç»Ÿå­—ä½“æ‰«æ...")
        all_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = [f for f in all_fonts if any(keyword in f.lower() for keyword in ['cjk', 'han', 'hei', 'kai', 'ming', 'noto', 'wenquanyi'])]
        if chinese_fonts:
            chosen = chinese_fonts[0]
            print(f"ğŸ” é€šè¿‡æ‰«ææ‰¾åˆ°ä¸­æ–‡å­—ä½“: {chosen}")
        else:
            chosen = 'DejaVu Sans'
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨DejaVu Sans")

    # 4. å¢å¼ºçš„å…¨å±€æ¸²æŸ“å‚æ•°
    rcParams['font.family'] = 'sans-serif'
    rcParams['font.sans-serif'] = [chosen, 'DejaVu Sans', 'Liberation Sans']
    rcParams['axes.unicode_minus'] = False
    rcParams['pdf.fonttype'] = 42
    rcParams['ps.fonttype'] = 42
    rcParams['savefig.dpi'] = 300
    rcParams['figure.dpi'] = 100
    rcParams['figure.autolayout'] = False
    rcParams['axes.titlesize'] = 12
    rcParams['axes.labelsize'] = 10
    rcParams['legend.fontsize'] = 9
    rcParams['xtick.labelsize'] = 9
    rcParams['ytick.labelsize'] = 9
    
    # 5. å¼ºåˆ¶å­—ä½“ç¼“å­˜é‡å»º
    try:
        fm._rebuild()
        cache_dir = os.path.expanduser('~/.cache/matplotlib')
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir, ignore_errors=True)
    except Exception as e:
        print(f"âš ï¸ å­—ä½“ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    print(f"âœ… æœ€ç»ˆä½¿ç”¨å­—ä½“: {chosen}")
    
    # 6. æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
    try:
        plt.figure(figsize=(1, 1))
        plt.text(0.5, 0.5, 'Font Test', fontsize=10)
        plt.close()
        print("âœ… ä¸­æ–‡å­—ä½“æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“æµ‹è¯•å¤±è´¥: {e}")
        rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("ğŸ”„ å·²åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼‰")

# æ‰§è¡Œå­—ä½“é…ç½®
setup_chinese_fonts_strict()

print("="*80)
print("ğŸ”¬ ä¸‰æ¨¡å‹ç»¼åˆæ¯”è¾ƒå¯è§†åŒ–ç³»ç»Ÿ")
print("="*80)
print("ğŸ“Š å¯¹æ¯”æ¨¡å‹: BiLSTM vs Transformer_Positive vs Transformer_PN")
print("ğŸ“ æ•°æ®æ¥æº: Three_modelç›®å½•ä¸‹å„æ¨¡å‹çš„æµ‹è¯•ç»“æœ")
print("="*80)

#----------------------------------------é…ç½®å‚æ•°------------------------------
# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    'BILSTM': {
        'name': 'BiLSTM',
        'display_name': 'BiLSTM',
        'color': '#1f77b4',  # è“è‰²
        'base_path': '/mnt/bz25t/bzhy/datasave/Three_model/BILSTM/',
        'result_pattern': 'test_results_*'
    },
    'TRANSFORMER_POSITIVE': {
        'name': 'TRANSFORMER_POSITIVE', 
        'display_name': 'Transformer (+)',
        'color': '#ff7f0e',  # æ©™è‰²
        'base_path': '/mnt/bz25t/bzhy/datasave/Three_model/transformer_positive/',
        'result_pattern': 'test_results_*'
    },
    'TRANSFORMER_PN': {
        'name': 'TRANSFORMER_PN',
        'display_name': 'Transformer (Â±)',
        'color': '#2ca02c',  # ç»¿è‰²
        'base_path': '/mnt/bz25t/bzhy/datasave/Three_model/transformer_PN/',
        'result_pattern': 'test_results_*'
    }
}

# å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (20, 16),
    "figsize_xlarge": (24, 18),
    "figsize_medium": (16, 12), 
    "bbox_inches": "tight"
}

# æ€§èƒ½æŒ‡æ ‡æ˜ å°„
METRIC_MAPPING = {
    'accuracy': 'å‡†ç¡®ç‡',
    'precision': 'ç²¾ç¡®ç‡',
    'recall': 'å¬å›ç‡',
    'f1_score': 'F1åˆ†æ•°',
    'specificity': 'ç‰¹å¼‚æ€§',
    'tpr': 'çœŸæ­£ç‡',
    'fpr': 'å‡æ­£ç‡'
}

#----------------------------------------æ•°æ®åŠ è½½æ¨¡å—------------------------------
def find_latest_test_results(base_path, pattern='test_results_*'):
    """æŸ¥æ‰¾æœ€æ–°çš„æµ‹è¯•ç»“æœç›®å½•"""
    search_path = os.path.join(base_path, pattern)
    test_dirs = glob.glob(search_path)
    
    if not test_dirs:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    latest_dir = max(test_dirs, key=os.path.getmtime)
    return latest_dir

def safe_load_json(file_path, default=None):
    """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return default
    except Exception as e:
        print(f"   âŒ åŠ è½½JSONå¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return default

def safe_load_pickle(file_path, default=None):
    """å®‰å…¨åŠ è½½Pickleæ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return default
    except Exception as e:
        print(f"   âŒ åŠ è½½Pickleå¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return default

def load_model_results(model_name, config):
    """åŠ è½½å•ä¸ªæ¨¡å‹çš„æµ‹è¯•ç»“æœ"""
    print(f"ğŸ“ åŠ è½½ {config['display_name']} æ¨¡å‹ç»“æœ...")
    
    # æŸ¥æ‰¾æœ€æ–°çš„æµ‹è¯•ç»“æœç›®å½•
    latest_dir = find_latest_test_results(config['base_path'], config['result_pattern'])
    
    if latest_dir is None:
        print(f"   âŒ æœªæ‰¾åˆ° {model_name} çš„æµ‹è¯•ç»“æœç›®å½•")
        return None
    
    print(f"   ğŸ“‚ æ‰¾åˆ°ç»“æœç›®å½•: {latest_dir}")
    
    # åŠ è½½æ€§èƒ½æŒ‡æ ‡
    if model_name == 'BILSTM':
        performance_file = os.path.join(latest_dir, 'bilstm_performance_metrics.json')
        detailed_file = os.path.join(latest_dir, 'detailed_results', 'bilstm_detailed_results.pkl')
        metadata_file = os.path.join(latest_dir, 'detailed_results', 'bilstm_test_metadata.json')
    else:
        performance_file = os.path.join(latest_dir, 'performance_metrics.json')
        detailed_file = os.path.join(latest_dir, 'detailed_results.pkl')
        metadata_file = os.path.join(latest_dir, 'test_metadata.json')
    
    # åŠ è½½æ•°æ®
    performance_data = safe_load_json(performance_file, {})
    detailed_data = safe_load_pickle(detailed_file, [])
    metadata = safe_load_json(metadata_file, {})
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    if not performance_data:
        print(f"   âš ï¸ {model_name} æ€§èƒ½æ•°æ®ä¸ºç©º")
        return None
    
    result = {
        'model_name': model_name,
        'config': config,
        'performance_data': performance_data,
        'detailed_data': detailed_data,
        'metadata': metadata,
        'result_dir': latest_dir
    }
    
    print(f"   âœ… {config['display_name']} æ•°æ®åŠ è½½å®Œæˆ")
    return result

def standardize_metrics(raw_metrics, model_name):
    """æ ‡å‡†åŒ–ä¸åŒæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡æ ¼å¼"""
    try:
        if model_name == 'BILSTM':
            # BiLSTMä½¿ç”¨çš„æ ¼å¼: performance_data['BILSTM']
            if 'BILSTM' in raw_metrics:
                model_metrics = raw_metrics['BILSTM']
            else:
                model_metrics = raw_metrics
        else:
            # Transformerä½¿ç”¨çš„æ ¼å¼: performance_data['TRANSFORMER']
            if 'TRANSFORMER' in raw_metrics:
                model_metrics = raw_metrics['TRANSFORMER']
            else:
                model_metrics = raw_metrics
        
        # æå–åˆ†ç±»æŒ‡æ ‡
        if 'classification_metrics' in model_metrics:
            classification = model_metrics['classification_metrics']
        else:
            classification = model_metrics
        
        # æå–æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in model_metrics:
            confusion = model_metrics['confusion_matrix']
        else:
            confusion = {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0}
        
        # æå–æ ·æœ¬æŒ‡æ ‡
        if 'sample_metrics' in model_metrics:
            sample_metrics = model_metrics['sample_metrics']
        else:
            sample_metrics = {}
        
        # æ ‡å‡†åŒ–æ ¼å¼
        standardized = {
            'classification_metrics': {
                'accuracy': classification.get('accuracy', 0.0),
                'precision': classification.get('precision', 0.0),
                'recall': classification.get('recall', 0.0),
                'f1_score': classification.get('f1_score', 0.0),
                'specificity': classification.get('specificity', 0.0),
                'tpr': classification.get('tpr', 0.0),
                'fpr': classification.get('fpr', 0.0)
            },
            'confusion_matrix': confusion,
            'sample_metrics': sample_metrics
        }
        
        return standardized
        
    except Exception as e:
        print(f"   âŒ æ ‡å‡†åŒ– {model_name} æŒ‡æ ‡å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤å€¼
        return {
            'classification_metrics': {
                'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0,
                'f1_score': 0.0, 'specificity': 0.0, 'tpr': 0.0, 'fpr': 0.0
            },
            'confusion_matrix': {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0},
            'sample_metrics': {}
        }

def load_all_model_results():
    """åŠ è½½æ‰€æœ‰æ¨¡å‹çš„æµ‹è¯•ç»“æœ"""
    print("\nğŸ”„ å¼€å§‹åŠ è½½æ‰€æœ‰æ¨¡å‹çš„æµ‹è¯•ç»“æœ...")
    
    all_results = {}
    
    for model_name, config in MODEL_CONFIGS.items():
        result = load_model_results(model_name, config)
        if result is not None:
            all_results[model_name] = result
        else:
            print(f"   âš ï¸ è·³è¿‡ {model_name}ï¼Œæ•°æ®åŠ è½½å¤±è´¥")
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(all_results)} ä¸ªæ¨¡å‹çš„ç»“æœ")
    return all_results

#----------------------------------------ROCæ›²çº¿æ¯”è¾ƒ------------------------------
def create_three_model_roc_comparison(all_results, save_path):
    """ç”Ÿæˆä¸‰æ¨¡å‹ROCæ›²çº¿æ¯”è¾ƒå›¾"""
    print("   ğŸ“ˆ ç”Ÿæˆä¸‰æ¨¡å‹ROCæ›²çº¿æ¯”è¾ƒå›¾...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"], constrained_layout=True)
    
    # === å­å›¾1: ROCæ›²çº¿å¯¹æ¯” ===
    ax1.set_title('(a) Three-Model ROC Curve Comparison', fontsize=14, fontweight='bold')
    
    model_auc_scores = {}
    
    for model_name, result in all_results.items():
        config = result['config']
        performance_data = result['performance_data']
        
        # æ ‡å‡†åŒ–æŒ‡æ ‡
        std_metrics = standardize_metrics(performance_data, model_name)
        
        # è·å–ROCæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        roc_data = None
        if model_name == 'BILSTM' and 'BILSTM' in performance_data:
            roc_data = performance_data['BILSTM'].get('roc_data', None)
        elif 'TRANSFORMER' in performance_data:
            roc_data = performance_data['TRANSFORMER'].get('roc_data', None)
        
        if roc_data and 'true_labels' in roc_data and 'fai_values' in roc_data:
            # ä½¿ç”¨å­˜å‚¨çš„ROCæ•°æ®
            true_labels = np.array(roc_data['true_labels'])
            fai_values = np.array(roc_data['fai_values'])
            
            # è®¡ç®—ROCæ›²çº¿
            fpr, tpr, _ = roc_curve(true_labels, fai_values)
            auc_score = auc(fpr, tpr)
            
            ax1.plot(fpr, tpr, color=config['color'], linewidth=2.5,
                    label=f'{config["display_name"]} (AUC={auc_score:.3f})')
            
            model_auc_scores[model_name] = auc_score
        else:
            # å¦‚æœæ²¡æœ‰ROCæ•°æ®ï¼Œä½¿ç”¨å·¥ä½œç‚¹
            metrics = std_metrics['classification_metrics']
            fpr_point = metrics['fpr']
            tpr_point = metrics['tpr']
            
            ax1.scatter(fpr_point, tpr_point, s=200, color=config['color'],
                       label=f'{config["display_name"]} (å·¥ä½œç‚¹)',
                       marker='o', edgecolors='black', linewidth=2)
            
            model_auc_scores[model_name] = 0.5  # é»˜è®¤AUC
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºåˆ†ç±»å™¨')
    ax1.set_xlabel('False Positive Rate (FPR)')
    ax1.set_ylabel('True Positive Rate (TPR)')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim([0, 1])
    ax1.set_ylim([0, 1])
    
    # === å­å›¾2: å·¥ä½œç‚¹æ¯”è¾ƒ ===
    ax2.set_title('(b) Working Points Comparison', fontsize=14, fontweight='bold')
    
    for model_name, result in all_results.items():
        config = result['config']
        performance_data = result['performance_data']
        
        std_metrics = standardize_metrics(performance_data, model_name)
        metrics = std_metrics['classification_metrics']
        
        ax2.scatter(metrics['fpr'], metrics['tpr'], s=300, color=config['color'],
                   label=f'{config["display_name"]}\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
                   marker='o', edgecolors='black', linewidth=2, alpha=0.8)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate (FPR)')
    ax2.set_ylabel('True Positive Rate (TPR)')
    ax2.legend(loc='lower right')
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim([0, 1])
    ax2.set_ylim([0, 1])
    
    # === å­å›¾3: åˆ†ç±»æŒ‡æ ‡å¯¹æ¯” ===
    ax3.set_title('(c) Classification Metrics Comparison', fontsize=14, fontweight='bold')
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    metric_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity']
    
    x = np.arange(len(metrics_names))
    width = 0.25
    
    for i, (model_name, result) in enumerate(all_results.items()):
        config = result['config']
        performance_data = result['performance_data']
        
        std_metrics = standardize_metrics(performance_data, model_name)
        values = [std_metrics['classification_metrics'][key] for key in metric_keys]
        
        bars = ax3.bar(x + i*width, values, width, 
                      label=config['display_name'], 
                      color=config['color'], alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=8)
    
    ax3.set_xlabel('Metrics')
    ax3.set_ylabel('Score')
    ax3.set_xticks(x + width)
    ax3.set_xticklabels(metrics_names)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim([0, 1.1])
    
    # === å­å›¾4: AUCå¯¹æ¯” ===
    ax4.set_title('(d) AUC Scores Comparison', fontsize=14, fontweight='bold')
    
    if model_auc_scores:
        models = list(model_auc_scores.keys())
        auc_values = list(model_auc_scores.values())
        colors = [MODEL_CONFIGS[m]['color'] for m in models]
        display_names = [MODEL_CONFIGS[m]['display_name'] for m in models]
        
        bars = ax4.bar(display_names, auc_values, color=colors, alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, auc_values):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        ax4.set_ylabel('AUC Score')
        ax4.set_ylim([0, 1])
        ax4.grid(True, alpha=0.3)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model_idx = np.argmax(auc_values)
        best_auc = auc_values[best_model_idx]
        best_name = display_names[best_model_idx]
        
        ax4.text(0.5, 0.95, f'æœ€ä½³: {best_name} (AUC={best_auc:.3f})', 
                transform=ax4.transAxes, ha='center', va='top',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                fontsize=10, fontweight='bold')
    else:
        ax4.text(0.5, 0.5, 'No AUC Data Available', ha='center', va='center',
                transform=ax4.transAxes, fontsize=12)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… ROCæ¯”è¾ƒå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½é›·è¾¾å›¾æ¯”è¾ƒ------------------------------
def create_three_model_radar_comparison(all_results, save_path):
    """ç”Ÿæˆä¸‰æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾æ¯”è¾ƒ"""
    print("   ğŸ•¸ï¸ ç”Ÿæˆä¸‰æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾æ¯”è¾ƒ...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'Accuracy': 'accuracy',
        'Precision': 'precision', 
        'Recall': 'recall',
        'F1-Score': 'f1_score',
        'Specificity': 'specificity',
        'Early Warning': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'False Alarm Control': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'Detection Stability': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=PLOT_CONFIG["figsize_large"], 
                                   subplot_kw=dict(projection='polar'), constrained_layout=True)
    
    # è®¾ç½®é›·è¾¾å›¾è§’åº¦
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    # === å­å›¾1: é›·è¾¾å›¾å åŠ æ˜¾ç¤º ===
    ax1.set_title('Three-Model Performance Radar Chart', pad=20, fontsize=14, fontweight='bold')
    
    model_scores = {}
    
    for model_name, result in all_results.items():
        config = result['config']
        performance_data = result['performance_data']
        
        std_metrics = standardize_metrics(performance_data, model_name)
        
        # æå–é›·è¾¾å›¾æ•°æ®
        values = []
        for metric_name, metric_key in radar_metrics.items():
            val = std_metrics['classification_metrics'][metric_key]
            
            # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
            if metric_name == 'False Alarm Control':
                val = 1 - val
                
            values.append(val)
        
        values += values[:1]  # é—­åˆ
        model_scores[model_name] = np.mean(values[:-1])  # è®¡ç®—å¹³å‡åˆ†
        
        # ç»˜åˆ¶é›·è¾¾å›¾
        ax1.plot(angles, values, 'o-', linewidth=2.5, 
                label=config['display_name'], color=config['color'])
        ax1.fill(angles, values, alpha=0.15, color=config['color'])
    
    # è®¾ç½®é›·è¾¾å›¾æ ‡ç­¾
    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(list(radar_metrics.keys()))
    ax1.set_ylim(0, 1)
    ax1.grid(True)
    ax1.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax1.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # === å­å›¾2: ç»¼åˆè¯„åˆ†å¯¹æ¯” ===
    ax2.remove()  # ç§»é™¤æåæ ‡è½´
    ax2 = fig.add_subplot(1, 2, 2)  # æ·»åŠ ç›´è§’åæ ‡è½´
    
    ax2.set_title('Comprehensive Performance Scores', fontsize=14, fontweight='bold')
    
    if model_scores:
        models = list(model_scores.keys())
        scores = list(model_scores.values())
        colors = [MODEL_CONFIGS[m]['color'] for m in models]
        display_names = [MODEL_CONFIGS[m]['display_name'] for m in models]
        
        bars = ax2.bar(display_names, scores, color=colors, alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        ax2.set_ylabel('Comprehensive Score')
        ax2.set_ylim([0, 1])
        ax2.grid(True, alpha=0.3)
        
        # æ’åºå¹¶æ ‡æ³¨
        sorted_pairs = sorted(zip(display_names, scores), key=lambda x: x[1], reverse=True)
        ranking_text = "Ranking:\n"
        for i, (name, score) in enumerate(sorted_pairs):
            ranking_text += f"{i+1}. {name}: {score:.3f}\n"
        
        ax2.text(0.02, 0.98, ranking_text.strip(), transform=ax2.transAxes, 
                va='top', ha='left',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8),
                fontsize=10)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… é›·è¾¾å›¾æ¯”è¾ƒä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºæ¯”è¾ƒ------------------------------
def create_three_model_timeline_comparison(all_results, save_path):
    """ç”Ÿæˆä¸‰æ¨¡å‹æ•…éšœæ£€æµ‹æ—¶åºæ¯”è¾ƒå›¾"""
    print("   ğŸ“Š ç”Ÿæˆä¸‰æ¨¡å‹æ•…éšœæ£€æµ‹æ—¶åºæ¯”è¾ƒå›¾...")
    
    fig, axes = plt.subplots(3, 1, figsize=(18, 14), sharex=True, constrained_layout=True)
    
    # å°è¯•æ‰¾åˆ°ä¸€ä¸ªå…±åŒçš„æ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    target_sample = None
    sample_data = {}
    
    for model_name, result in all_results.items():
        detailed_data = result['detailed_data']
        if detailed_data and len(detailed_data) > 0:
            # å¯»æ‰¾æ•…éšœæ ·æœ¬
            fault_samples = [r for r in detailed_data if r.get('label', 0) == 1]
            if fault_samples:
                sample_result = fault_samples[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
                target_sample = sample_result.get('sample_id', 'Unknown')
                sample_data[model_name] = sample_result
                
    if not sample_data:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•…éšœæ ·æœ¬ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ ·æœ¬
        for model_name, result in all_results.items():
            detailed_data = result['detailed_data']
            if detailed_data and len(detailed_data) > 0:
                sample_result = detailed_data[0]
                target_sample = sample_result.get('sample_id', 'Unknown')
                sample_data[model_name] = sample_result
    
    if not sample_data:
        print("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ ·æœ¬æ•°æ®")
        return
    
    print(f"   ğŸ“‹ ä½¿ç”¨æ ·æœ¬ {target_sample} è¿›è¡Œæ—¶åºæ¯”è¾ƒ")
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆå­å›¾
    for i, (model_name, result) in enumerate(all_results.items()):
        ax = axes[i]
        config = result['config']
        
        if model_name in sample_data:
            sample_result = sample_data[model_name]
            
            fai_values = sample_result.get('fai', [])
            fault_labels = sample_result.get('fault_labels', [])
            thresholds = sample_result.get('thresholds', {})
            detection_info = sample_result.get('detection_info', {})
            
            if len(fai_values) > 0:
                time_axis = np.arange(len(fai_values))
                
                # ç»˜åˆ¶FAIæ—¶åº
                ax.plot(time_axis, fai_values, color=config['color'], linewidth=1.5, alpha=0.8,
                       label=f'{config["display_name"]} Ï†(FAI)')
                
                # ç»˜åˆ¶é˜ˆå€¼çº¿
                if 'threshold1' in thresholds:
                    ax.axhline(y=thresholds['threshold1'], color='red', linestyle='--', alpha=0.7,
                              label='Level 1 Threshold')
                
                # æ ‡è®°æ•…éšœåŒºåŸŸ
                if len(fault_labels) > 0:
                    fault_regions = np.where(np.array(fault_labels) > 0, 1, 0)
                    ax.fill_between(time_axis, 0, np.max(fai_values) * fault_regions, 
                                   alpha=0.3, color='red', label='Detected Faults')
                
                # æ ‡è®°è§¦å‘ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if detection_info.get('trigger_points'):
                    trigger_points = detection_info['trigger_points']
                    ax.scatter(trigger_points, [fai_values[i] for i in trigger_points if i < len(fai_values)],
                              color='orange', s=30, alpha=0.8, label='Trigger Points')
                
                ax.set_ylabel(f'{config["display_name"]}\nÏ† Index')
                ax.set_title(f'{config["display_name"]} - Sample {target_sample}')
                ax.legend(loc='upper right')
                ax.grid(True, alpha=0.3)
            else:
                ax.text(0.5, 0.5, f'No Data for {config["display_name"]}', 
                       ha='center', va='center', transform=ax.transAxes, fontsize=12)
                ax.set_title(f'{config["display_name"]} - No Data')
        else:
            ax.text(0.5, 0.5, f'No Sample Data for {config["display_name"]}', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12)
            ax.set_title(f'{config["display_name"]} - No Sample Data')
    
    axes[-1].set_xlabel('Time Step')
    plt.suptitle(f'Three-Model Fault Detection Timeline Comparison - Sample {target_sample}', 
                 fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… æ—¶åºæ¯”è¾ƒå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ··æ·†çŸ©é˜µæ¯”è¾ƒ------------------------------
def create_confusion_matrix_comparison(all_results, save_path):
    """ç”Ÿæˆä¸‰æ¨¡å‹æ··æ·†çŸ©é˜µæ¯”è¾ƒå›¾"""
    print("   ğŸ“Š ç”Ÿæˆä¸‰æ¨¡å‹æ··æ·†çŸ©é˜µæ¯”è¾ƒå›¾...")
    
    num_models = len(all_results)
    fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5), constrained_layout=True)
    
    if num_models == 1:
        axes = [axes]
    
    for i, (model_name, result) in enumerate(all_results.items()):
        ax = axes[i]
        config = result['config']
        performance_data = result['performance_data']
        
        std_metrics = standardize_metrics(performance_data, model_name)
        confusion = std_metrics['confusion_matrix']
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        cm = np.array([[confusion['TN'], confusion['FP']], 
                       [confusion['FN'], confusion['TP']]])
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['Predicted Normal', 'Predicted Fault'],
                   yticklabels=['Actual Normal', 'Actual Fault'])
        
        ax.set_title(f'{config["display_name"]}\nConfusion Matrix', fontsize=12, fontweight='bold')
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        metrics = std_metrics['classification_metrics']
        info_text = f"Acc: {metrics['accuracy']:.3f}\nF1: {metrics['f1_score']:.3f}"
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes, va='top', ha='left',
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
               fontsize=10)
    
    plt.suptitle('Three-Model Confusion Matrix Comparison', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… æ··æ·†çŸ©é˜µæ¯”è¾ƒå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»¼åˆåˆ†ææŠ¥å‘Š------------------------------
def generate_comprehensive_analysis(all_results):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("   ğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    analysis = {
        'summary': {},
        'ranking': {},
        'recommendations': {},
        'detailed_comparison': {}
    }
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡
    model_metrics = {}
    for model_name, result in all_results.items():
        config = result['config']
        performance_data = result['performance_data']
        
        std_metrics = standardize_metrics(performance_data, model_name)
        
        model_metrics[model_name] = {
            'display_name': config['display_name'],
            'metrics': std_metrics['classification_metrics'],
            'confusion': std_metrics['confusion_matrix']
        }
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    weights = {
        'accuracy': 0.2,
        'precision': 0.2,
        'recall': 0.2,
        'f1_score': 0.3,
        'specificity': 0.1
    }
    
    composite_scores = {}
    for model_name, data in model_metrics.items():
        score = sum(data['metrics'][metric] * weight for metric, weight in weights.items())
        composite_scores[model_name] = score
    
    # æ’åº
    ranked_models = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analysis['summary'] = {
        'total_models': len(all_results),
        'evaluation_metrics': list(weights.keys()),
        'best_model': ranked_models[0][0] if ranked_models else None,
        'best_score': ranked_models[0][1] if ranked_models else 0.0
    }
    
    analysis['ranking'] = {
        'composite_scores': composite_scores,
        'ranked_list': ranked_models
    }
    
    # è¯¦ç»†æ¯”è¾ƒ
    metric_winners = {}
    for metric in weights.keys():
        best_model = max(model_metrics.items(), key=lambda x: x[1]['metrics'][metric])
        metric_winners[metric] = {
            'model': best_model[0],
            'display_name': best_model[1]['display_name'],
            'score': best_model[1]['metrics'][metric]
        }
    
    analysis['detailed_comparison'] = {
        'metric_winners': metric_winners,
        'model_metrics': model_metrics
    }
    
    # ç”Ÿæˆæ¨è
    if ranked_models:
        best_model_name = ranked_models[0][0]
        best_display_name = model_metrics[best_model_name]['display_name']
        
        recommendations = [
            f"ç»¼åˆæ€§èƒ½æœ€ä½³æ¨¡å‹: {best_display_name}",
            f"ç»¼åˆå¾—åˆ†: {ranked_models[0][1]:.3f}",
            ""
        ]
        
        # å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹
        recommendations.append("å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹:")
        for metric, winner in metric_winners.items():
            recommendations.append(f"  {METRIC_MAPPING.get(metric, metric)}: {winner['display_name']} ({winner['score']:.3f})")
        
        analysis['recommendations'] = recommendations
    
    return analysis

#----------------------------------------ä¸»å‡½æ•°------------------------------
def create_comprehensive_comparison_report(all_results, save_dir):
    """åˆ›å»ºç»¼åˆæ¯”è¾ƒæŠ¥å‘Š"""
    print("\nğŸ¨ ç”Ÿæˆç»¼åˆæ¯”è¾ƒå¯è§†åŒ–æŠ¥å‘Š...")
    
    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    vis_dir = os.path.join(save_dir, 'visualizations')
    os.makedirs(vis_dir, exist_ok=True)
    
    # ç”Ÿæˆå„ç§æ¯”è¾ƒå›¾è¡¨
    create_three_model_roc_comparison(all_results, 
                                     os.path.join(vis_dir, 'three_model_roc_comparison.png'))
    
    create_three_model_radar_comparison(all_results, 
                                       os.path.join(vis_dir, 'three_model_radar_comparison.png'))
    
    create_three_model_timeline_comparison(all_results, 
                                          os.path.join(vis_dir, 'three_model_timeline_comparison.png'))
    
    create_confusion_matrix_comparison(all_results, 
                                      os.path.join(vis_dir, 'three_model_confusion_matrix_comparison.png'))
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    analysis = generate_comprehensive_analysis(all_results)
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    analysis_file = os.path.join(save_dir, 'comprehensive_analysis.json')
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆExcelæŠ¥å‘Š
    create_excel_comparison_report(all_results, analysis, save_dir)
    
    print(f"   âœ… ç»¼åˆæ¯”è¾ƒæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {save_dir}")
    return analysis

def create_excel_comparison_report(all_results, analysis, save_dir):
    """åˆ›å»ºExcelæ ¼å¼çš„æ¯”è¾ƒæŠ¥å‘Š"""
    print("   ğŸ“Š ç”ŸæˆExcelæ¯”è¾ƒæŠ¥å‘Š...")
    
    excel_file = os.path.join(save_dir, 'three_model_comparison_report.xlsx')
    
    with pd.ExcelWriter(excel_file) as writer:
        # 1. ç»¼åˆæ€§èƒ½å¯¹æ¯”è¡¨
        performance_data = []
        for model_name, result in all_results.items():
            config = result['config']
            performance_data_raw = result['performance_data']
            
            std_metrics = standardize_metrics(performance_data_raw, model_name)
            metrics = std_metrics['classification_metrics']
            confusion = std_metrics['confusion_matrix']
            
            performance_data.append({
                'Model': config['display_name'],
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1_score'],
                'Specificity': metrics['specificity'],
                'TPR': metrics['tpr'],
                'FPR': metrics['fpr'],
                'TP': confusion['TP'],
                'FP': confusion['FP'],
                'TN': confusion['TN'],
                'FN': confusion['FN'],
                'Composite_Score': analysis['ranking']['composite_scores'].get(model_name, 0.0)
            })
        
        performance_df = pd.DataFrame(performance_data)
        performance_df = performance_df.sort_values('Composite_Score', ascending=False)
        performance_df.to_excel(writer, sheet_name='Performance_Comparison', index=False)
        
        # 2. æ’åè¡¨
        ranking_data = []
        for i, (model_name, score) in enumerate(analysis['ranking']['ranked_list']):
            config = MODEL_CONFIGS[model_name]
            ranking_data.append({
                'Rank': i + 1,
                'Model': config['display_name'],
                'Composite_Score': score,
                'Performance_Level': 'ä¼˜ç§€' if score >= 0.8 else 'è‰¯å¥½' if score >= 0.6 else 'ä¸€èˆ¬'
            })
        
        ranking_df = pd.DataFrame(ranking_data)
        ranking_df.to_excel(writer, sheet_name='Model_Ranking', index=False)
        
        # 3. æŒ‡æ ‡è·èƒœè€…è¡¨
        winners_data = []
        for metric, winner in analysis['detailed_comparison']['metric_winners'].items():
            winners_data.append({
                'Metric': METRIC_MAPPING.get(metric, metric),
                'Best_Model': winner['display_name'],
                'Score': winner['score']
            })
        
        winners_df = pd.DataFrame(winners_data)
        winners_df.to_excel(writer, sheet_name='Metric_Winners', index=False)
    
    print(f"   âœ… ExcelæŠ¥å‘Šä¿å­˜è‡³: {excel_file}")

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("\nğŸš€ å¼€å§‹ä¸‰æ¨¡å‹ç»¼åˆæ¯”è¾ƒåˆ†æ...")
    
    # åŠ è½½æ‰€æœ‰æ¨¡å‹ç»“æœ
    all_results = load_all_model_results()
    
    if len(all_results) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹çš„æµ‹è¯•ç»“æœï¼Œè¯·å…ˆè¿è¡Œå„æ¨¡å‹çš„æµ‹è¯•è„šæœ¬")
        return
    
    print(f"\nğŸ“Š æˆåŠŸåŠ è½½ {len(all_results)} ä¸ªæ¨¡å‹çš„ç»“æœ:")
    for model_name, result in all_results.items():
        config = result['config']
        print(f"   â€¢ {config['display_name']}: {result['result_dir']}")
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"/mnt/bz25t/bzhy/datasave/Three_model/comparison_results_{timestamp}"
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆç»¼åˆæ¯”è¾ƒæŠ¥å‘Š
    analysis = create_comprehensive_comparison_report(all_results, save_dir)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ‰ ä¸‰æ¨¡å‹ç»¼åˆæ¯”è¾ƒåˆ†æå®Œæˆï¼")
    print("="*80)
    
    print(f"\nğŸ“Š æ¯”è¾ƒç»“æœæ€»ç»“:")
    print(f"   â€¢ å‚ä¸æ¯”è¾ƒçš„æ¨¡å‹: {len(all_results)} ä¸ª")
    for model_name, result in all_results.items():
        config = result['config']
        print(f"     - {config['display_name']}")
    
    if analysis['summary']['best_model']:
        best_model_config = MODEL_CONFIGS[analysis['summary']['best_model']]
        print(f"\nğŸ† ç»¼åˆæ€§èƒ½æœ€ä½³æ¨¡å‹: {best_model_config['display_name']}")
        print(f"   ç»¼åˆå¾—åˆ†: {analysis['summary']['best_score']:.3f}")
    
    print(f"\nğŸ¯ æ€§èƒ½æ’å:")
    for i, (model_name, score) in enumerate(analysis['ranking']['ranked_list']):
        config = MODEL_CONFIGS[model_name]
        print(f"   {i+1}. {config['display_name']}: {score:.3f}")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   â€¢ ç»“æœç›®å½•: {save_dir}")
    print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {save_dir}/visualizations/")
    print(f"     - ROCæ›²çº¿æ¯”è¾ƒ: three_model_roc_comparison.png")
    print(f"     - é›·è¾¾å›¾æ¯”è¾ƒ: three_model_radar_comparison.png") 
    print(f"     - æ—¶åºå›¾æ¯”è¾ƒ: three_model_timeline_comparison.png")
    print(f"     - æ··æ·†çŸ©é˜µæ¯”è¾ƒ: three_model_confusion_matrix_comparison.png")
    print(f"   â€¢ åˆ†ææŠ¥å‘Š: comprehensive_analysis.json")
    print(f"   â€¢ ExcelæŠ¥å‘Š: three_model_comparison_report.xlsx")
    
    if 'recommendations' in analysis:
        print(f"\nğŸ’¡ æ¨èå»ºè®®:")
        for rec in analysis['recommendations']:
            if rec.strip():
                print(f"   â€¢ {rec}")
    
    print("\n" + "="*80)
    print("ä¸‰æ¨¡å‹æ¯”è¾ƒåˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
    print("="*80)

if __name__ == "__main__":
    main()
