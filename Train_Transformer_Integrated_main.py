#----------------------------------------ä¸»æ‰§è¡Œå‡½æ•°å’Œæµ‹è¯•å‡½æ•°------------------------------

def test_experiment(experiment_results):
    """æµ‹è¯•å®éªŒç»“æœ"""
    
    print(f"\nğŸ”¬ é˜¶æ®µ4: å¼€å§‹æµ‹è¯•å®éªŒ")
    start_time = time.time()
    
    config = experiment_results['config']
    save_suffix = config['save_suffix']
    save_dir = f"modelsfl{save_suffix}"
    
    try:
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        transformer = TransformerPredictor(
            input_size=7,
            d_model=config['d_model'],
            nhead=config['n_heads'],
            num_layers=config['n_layers'],
            d_ff=config['d_ff'],
            dropout=config['dropout'],
            output_size=2
        ).to(device).float()
        
        # åŠ è½½æ¨¡å‹æƒé‡
        transformer_path = f'{save_dir}/transformer_model.pth'
        state_dict = torch.load(transformer_path, map_location=device)
        # å¤„ç†DataParallelå‰ç¼€
        if any(key.startswith('module.') for key in state_dict.keys()):
            new_state_dict = {}
            for key, value in state_dict.items():
                new_key = key[7:] if key.startswith('module.') else key
                new_state_dict[new_key] = value
            state_dict = new_state_dict
        transformer.load_state_dict(state_dict)
        
        # åŠ è½½MC-AEæ¨¡å‹
        net = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                        activation_fn=custom_activation, use_dx_in_forward=True).to(device)
        netx = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                         activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
        
        net_path = f'{save_dir}/net_model.pth'
        netx_path = f'{save_dir}/netx_model.pth'
        net.load_state_dict(torch.load(net_path, map_location=device))
        netx.load_state_dict(torch.load(netx_path, map_location=device))
        
        # åŠ è½½PCAå‚æ•°
        pca_params = {}
        pca_files = ['v_I', 'v', 'v_ratio', 'p_k', 'data_mean', 'data_std', 
                     'T_95_limit', 'T_99_limit', 'SPE_95_limit', 'SPE_99_limit', 
                     'P', 'k', 'P_t', 'X', 'data_nor']
        for pca_file in pca_files:
            pca_params[pca_file] = np.load(f'{save_dir}/{pca_file}.npy')
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æµ‹è¯•æ‰€æœ‰æ ·æœ¬
        test_results = []
        
        for sample_id in ALL_TEST_SAMPLES:
            print(f"ğŸ” æµ‹è¯•æ ·æœ¬ {sample_id}...")
            
            try:
                # åŠ è½½æµ‹è¯•æ•°æ®
                vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
                
                # å¤„ç†æ•°æ®æ ¼å¼
                if len(vin1_data.shape) == 2:
                    vin1_data = vin1_data.unsqueeze(1)
                vin1_data = vin1_data.to(torch.float32).to(device)
                
                # å®šä¹‰ç»´åº¦
                dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
                dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
                
                # åˆ†ç¦»æ•°æ®
                x_recovered = vin2_data[:, :dim_x]
                y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
                z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
                q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
                
                x_recovered2 = vin3_data[:, :dim_x2]
                y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
                z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
                q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
                
                # MC-AEæ¨ç†
                net.eval()
                netx.eval()
                
                with torch.no_grad():
                    net = net.double()
                    netx = netx.double()
                    
                    recon_imtest = net(x_recovered, z_recovered, q_recovered)
                    reconx_imtest = netx(x_recovered2, z_recovered2, q_recovered2)
                
                # è®¡ç®—é‡æ„è¯¯å·®
                AA = recon_imtest[0].cpu().detach().numpy()
                yTrainU = y_recovered.cpu().detach().numpy()
                ERRORU = AA - yTrainU
            
                BB = reconx_imtest[0].cpu().detach().numpy()
                yTrainX = y_recovered2.cpu().detach().numpy()
                ERRORX = BB - yTrainX
            
                # è¯Šæ–­ç‰¹å¾æå–
                df_data = DiagnosisFeature(ERRORU, ERRORX)
                
                # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—
                time_axis = np.arange(df_data.shape[0])
                
                lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = Comprehensive_calculation(
                    df_data.values, 
                    pca_params['data_mean'], 
                    pca_params['data_std'], 
                    pca_params['v'].reshape(len(pca_params['v']), 1), 
                    pca_params['p_k'], 
                    pca_params['v_I'], 
                    pca_params['T_99_limit'], 
                    pca_params['SPE_99_limit'], 
                    pca_params['X'], 
                    time_axis
                )
                
                # è®¡ç®—é˜ˆå€¼
                threshold1, threshold2, threshold3 = calculate_thresholds(fai)
                
                # ä¸‰çª—å£æ•…éšœæ£€æµ‹
                fault_labels, detection_info = three_window_fault_detection(fai, threshold1, sample_id)
                
                # æ„å»ºç»“æœ
                sample_result = {
                    'sample_id': sample_id,
                    'config_name': config['name'],
                    'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
                    'df_data': df_data.values,
                    'fai': fai,
                    'T_squared': t_total,
                    'SPE': q_total,
                    'thresholds': {
                        'threshold1': threshold1,
                        'threshold2': threshold2, 
                        'threshold3': threshold3
                    },
                    'fault_labels': fault_labels,
                    'detection_info': detection_info,
                    'performance_metrics': {
                        'fai_mean': np.mean(fai),
                        'fai_std': np.std(fai),
                        'fai_max': np.max(fai),
                        'fai_min': np.min(fai),
                        'anomaly_count': np.sum(fai > threshold1),
                        'anomaly_ratio': np.sum(fai > threshold1) / len(fai)
                    }
                }
                
                test_results.append(sample_result)
                
                # è¾“å‡ºç®€è¦ç»“æœ
                metrics = sample_result['performance_metrics']
                window_stats = detection_info.get('window_stats', {})
                print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics['fai_mean']:.6f}, "
                      f"å¼‚å¸¸ç‡={metrics['anomaly_ratio']:.2%}, "
                      f"æ•…éšœç‡={window_stats.get('fault_ratio', 0.0):.2%}")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_metrics = calculate_performance_metrics(test_results)
        
        # è®°å½•æµ‹è¯•ç»“æœ
        experiment_results['test_results'] = {
            'sample_results': test_results,
            'performance_metrics': performance_metrics
        }
        experiment_results['timing']['test'] = time.time() - start_time
        experiment_results['stage'] = 'test_completed'
        
        print(f"âœ… æµ‹è¯•å®Œæˆï¼Œç”¨æ—¶: {experiment_results['timing']['test']:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        raise e
    
    return experiment_results

def calculate_performance_metrics(test_results):
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_true_labels = []
    all_fai_values = []
    all_fault_predictions = []
    
    for result in test_results:
        true_label = result.get('label', 0)
        fai_values = result.get('fai', [])
        fault_labels = result.get('fault_labels', [])
        thresholds = result.get('thresholds', {})
        threshold1 = thresholds.get('threshold1', 0.0)
        
        # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
        for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
            all_true_labels.append(true_label)
            all_fai_values.append(fai_val)
            
            # æ ¹æ®ROCé€»è¾‘è®¡ç®—é¢„æµ‹ç»“æœ
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                prediction = 1 if fai_val > threshold1 else 0
            else:  # æ•…éšœæ ·æœ¬
                if fai_val > threshold1 and fault_pred == 1:
                    prediction = 1  # TP
                else:
                    prediction = 0  # FN
            
            all_fault_predictions.append(prediction)
    
    # è®¡ç®—ROCæŒ‡æ ‡
    all_true_labels = np.array(all_true_labels)
    all_fai_values = np.array(all_fai_values)
    all_fault_predictions = np.array(all_fault_predictions)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
    fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
    fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
    tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    tpr = recall
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    # æ ·æœ¬çº§ç»Ÿè®¡
    sample_metrics = {
        'total_samples': len(test_results),
        'normal_samples': len([r for r in test_results if r['label'] == 0]),
        'fault_samples': len([r for r in test_results if r['label'] == 1]),
        'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                 for r in test_results if r['label'] == 0]),
        'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                for r in test_results if r['label'] == 1]),
        'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                           for r in test_results if r['label'] == 0]),
        'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                          for r in test_results if r['label'] == 1])
    }
    
    return {
        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
        'classification_metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1_score),
            'specificity': float(specificity),
            'tpr': float(tpr),
            'fpr': float(fpr)
        },
        'sample_metrics': sample_metrics,
        'roc_data': {
            'true_labels': all_true_labels.tolist(),
            'fai_values': all_fai_values.tolist(),
            'predictions': all_fault_predictions.tolist()
        }
    }

def create_comparison_visualization(results_original, results_enhanced, save_dir):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
    
    print(f"ğŸ¨ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    viz_dir = f"{save_dir}/visualizations"
    os.makedirs(viz_dir, exist_ok=True)
    
    # å›¾1: ROCæ›²çº¿å¯¹æ¯”
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # å­å›¾1: ROCæ›²çº¿å¯¹æ¯”
    perf_orig = results_original['test_results']['performance_metrics']
    perf_enh = results_enhanced['test_results']['performance_metrics']
    
    # ç»˜åˆ¶å·¥ä½œç‚¹
    ax1.scatter(perf_orig['classification_metrics']['fpr'], 
               perf_orig['classification_metrics']['tpr'], 
               s=200, color='blue', marker='o', 
               label=f"åŸå§‹å‚æ•° (AUCä¼°ç®—)", alpha=0.8)
    ax1.scatter(perf_enh['classification_metrics']['fpr'], 
               perf_enh['classification_metrics']['tpr'], 
               s=200, color='red', marker='^', 
               label=f"å¢å¼ºå‚æ•° (AUCä¼°ç®—)", alpha=0.8)
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºåˆ†ç±»å™¨')
    
    if use_chinese:
        ax1.set_xlabel('å‡æ­£ä¾‹ç‡ (FPR)')
        ax1.set_ylabel('çœŸæ­£ä¾‹ç‡ (TPR)')
        ax1.set_title('(a) ROCæ›²çº¿å¯¹æ¯”')
    else:
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.set_title('(a) ROC Curve Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2: æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    orig_values = [perf_orig['classification_metrics']['accuracy'],
                   perf_orig['classification_metrics']['precision'],
                   perf_orig['classification_metrics']['recall'],
                   perf_orig['classification_metrics']['f1_score'],
                   perf_orig['classification_metrics']['specificity']]
    enh_values = [perf_enh['classification_metrics']['accuracy'],
                  perf_enh['classification_metrics']['precision'],
                  perf_enh['classification_metrics']['recall'],
                  perf_enh['classification_metrics']['f1_score'],
                  perf_enh['classification_metrics']['specificity']]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    bars1 = ax2.bar(x - width/2, orig_values, width, label='åŸå§‹å‚æ•°', color='blue', alpha=0.7)
    bars2 = ax2.bar(x + width/2, enh_values, width, label='å¢å¼ºå‚æ•°', color='red', alpha=0.7)
    
    if use_chinese:
        ax2.set_xlabel('æ€§èƒ½æŒ‡æ ‡')
        ax2.set_ylabel('åˆ†æ•°')
        ax2.set_title('(b) æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
    else:
        ax2.set_xlabel('Performance Metrics')
        ax2.set_ylabel('Score')
        ax2.set_title('(b) Performance Metrics Comparison')
    ax2.set_xticks(x)
    ax2.set_xticklabels(metrics_names, rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # å­å›¾3: è®­ç»ƒæŸå¤±å¯¹æ¯”
    train_losses_orig = results_original['transformer_results']['train_losses']
    train_losses_enh = results_enhanced['transformer_results']['train_losses']
    
    ax3.plot(train_losses_orig, 'b-', label='åŸå§‹å‚æ•°', linewidth=2)
    ax3.plot(train_losses_enh, 'r-', label='å¢å¼ºå‚æ•°', linewidth=2)
    
    if use_chinese:
        ax3.set_xlabel('è®­ç»ƒè½®æ•°')
        ax3.set_ylabel('è®­ç»ƒæŸå¤±')
        ax3.set_title('(c) è®­ç»ƒæŸå¤±å¯¹æ¯”')
    else:
        ax3.set_xlabel('Training Epochs')
        ax3.set_ylabel('Training Loss')
        ax3.set_title('(c) Training Loss Comparison')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    # å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å¯¹æ¯”
    sample_metrics_orig = perf_orig['sample_metrics']
    sample_metrics_enh = perf_enh['sample_metrics']
    
    sample_names = ['æ­£å¸¸æ ·æœ¬\nFAIå‡å€¼', 'æ•…éšœæ ·æœ¬\nFAIå‡å€¼', 'æ­£å¸¸æ ·æœ¬\nå¼‚å¸¸ç‡', 'æ•…éšœæ ·æœ¬\nå¼‚å¸¸ç‡']
    orig_sample_values = [sample_metrics_orig['avg_fai_normal'],
                         sample_metrics_orig['avg_fai_fault'],
                         sample_metrics_orig['avg_anomaly_ratio_normal'],
                         sample_metrics_orig['avg_anomaly_ratio_fault']]
    enh_sample_values = [sample_metrics_enh['avg_fai_normal'],
                        sample_metrics_enh['avg_fai_fault'],
                        sample_metrics_enh['avg_anomaly_ratio_normal'],
                        sample_metrics_enh['avg_anomaly_ratio_fault']]
    
    x = np.arange(len(sample_names))
    bars1 = ax4.bar(x - width/2, orig_sample_values, width, label='åŸå§‹å‚æ•°', color='blue', alpha=0.7)
    bars2 = ax4.bar(x + width/2, enh_sample_values, width, label='å¢å¼ºå‚æ•°', color='red', alpha=0.7)
    
    if use_chinese:
        ax4.set_xlabel('æ ·æœ¬çº§æŒ‡æ ‡')
        ax4.set_ylabel('æ•°å€¼')
        ax4.set_title('(d) æ ·æœ¬çº§æ€§èƒ½å¯¹æ¯”')
    else:
        ax4.set_xlabel('Sample-level Metrics')
        ax4.set_ylabel('Value')
        ax4.set_title('(d) Sample-level Performance Comparison')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_names, rotation=45, ha='right')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    comparison_path = f"{viz_dir}/experiment_comparison.png"
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {comparison_path}")
    
    return comparison_path

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    print("\nğŸš€ å¼€å§‹é›†æˆè®­ç»ƒæµ‹è¯•å®éªŒ...")
    
    all_results = {}
    
    # æ‰§è¡Œä¸¤ä¸ªå®éªŒé…ç½®
    for config_name, config in EXPERIMENT_CONFIGS.items():
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ å®éªŒé…ç½®: {config['name']}")
        print(f"{'='*80}")
        
        try:
            # è®­ç»ƒå®éªŒ
            experiment_results = train_experiment(config_name, config)
            
            # ç«‹å³æµ‹è¯•
            experiment_results = test_experiment(experiment_results)
            
            # æ ‡è®°å®Œæˆ
            experiment_results['stage'] = 'completed'
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            save_suffix = config['save_suffix']
            save_dir = f"modelsfl{save_suffix}"
            checkpoint_path = f"{save_dir}/checkpoint.pkl"
            with open(checkpoint_path, 'wb') as f:
                pickle.dump(experiment_results, f)
            
            # ä¿å­˜åˆ°å…¨å±€ç»“æœ
            all_results[config_name] = experiment_results
            
            # è¾“å‡ºå®éªŒæ€»ç»“
            print(f"\nğŸ“Š {config['name']} å®éªŒæ€»ç»“:")
            print(f"   è®­ç»ƒæ—¶é—´: {experiment_results['timing']['transformer']:.2f}ç§’")
            print(f"   æµ‹è¯•æ—¶é—´: {experiment_results['timing']['test']:.2f}ç§’")
            
            perf = experiment_results['test_results']['performance_metrics']
            metrics = perf['classification_metrics']
            print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
            print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
            print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
            print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")
            print(f"   TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")
            
        except Exception as e:
            print(f"âŒ å®éªŒ {config['name']} å¤±è´¥: {e}")
            continue
    
    # å¦‚æœä¸¤ä¸ªå®éªŒéƒ½å®Œæˆï¼Œç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–
    if len(all_results) == 2:
        print(f"\nğŸ¨ ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
        try:
            comparison_path = create_comparison_visualization(
                all_results['original'], 
                all_results['enhanced'],
                "modelsfl_comparison"
            )
            
            # ä¿å­˜å®Œæ•´ç»“æœ
            results_path = "modelsfl_comparison/complete_results.pkl"
            os.makedirs("modelsfl_comparison", exist_ok=True)
            with open(results_path, 'wb') as f:
                pickle.dump(all_results, f)
            
            print(f"âœ… å®Œæ•´ç»“æœå·²ä¿å­˜: {results_path}")
            
        except Exception as e:
            print(f"âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*80}")
    print(f"ğŸ‰ é›†æˆè®­ç»ƒæµ‹è¯•å®éªŒå®Œæˆï¼")
    print(f"{'='*80}")
    
    for config_name, results in all_results.items():
        config = EXPERIMENT_CONFIGS[config_name]
        print(f"\nğŸ“Š {config['name']} æœ€ç»ˆç»“æœ:")
        
        if 'test_results' in results:
            perf = results['test_results']['performance_metrics']
            metrics = perf['classification_metrics']
            print(f"   ROCå·¥ä½œç‚¹: TPR={metrics['tpr']:.3f}, FPR={metrics['fpr']:.3f}")
            print(f"   ç»¼åˆæ€§èƒ½: F1={metrics['f1_score']:.3f}")
            print(f"   ç»“æœä¿å­˜: modelsfl{config['save_suffix']}/")
        else:
            print(f"   âŒ å®éªŒæœªå®Œæˆ")
    
    print(f"\nğŸ”„ ä¸‹ä¸€æ­¥å¯ä»¥:")
    print(f"   1. æŸ¥çœ‹è¯¦ç»†çš„è®­ç»ƒå’Œæµ‹è¯•ç»“æœ")
    print(f"   2. åˆ†æä¸¤ç§å‚æ•°é…ç½®çš„æ€§èƒ½å·®å¼‚")
    print(f"   3. æ£€æŸ¥åå‘ä¼ æ’­æœºåˆ¶çš„æ•ˆæœ")
    print(f"   4. ä¼˜åŒ–æ¨¡å‹å‚æ•°å’Œè®­ç»ƒç­–ç•¥")

if __name__ == "__main__":
    main()