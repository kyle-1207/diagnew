# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import matplotlib.ticker as mtick
import os
import warnings
import matplotlib
from Function_ import *
from Class_ import *
import math
import math
from create_dataset import series_to_supervised
from sklearn import preprocessing
import torch
import torch.nn as nn
import torch.optim as optim
#from sklearn.datasets import load_boston
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import scipy.io as scio
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import nn
from torchvision import transforms as tfs
import scipy.stats as stats
import seaborn as sns
import pickle
# ç¡®ä¿ä»å½“å‰ç›®å½•å¯¼å…¥æ›´æ–°åçš„Comprehensive_calculationå‡½æ•°
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from Comprehensive_calculation import Comprehensive_calculation

# æ–°å¢å¯¼å…¥
from tqdm import tqdm
import json
import time
from datetime import datetime
from sklearn.metrics import roc_curve, auc, confusion_matrix
import glob

# æ·»åŠ æ¨¡å‹åŠ è½½è¾…åŠ©å‡½æ•°
def remove_module_prefix(state_dict):
    """ç§»é™¤state_dictä¸­çš„module.å‰ç¼€"""
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('module.'):
            new_key = key[7:]  # ç§»é™¤'module.'å‰ç¼€
        else:
            new_key = key
        new_state_dict[new_key] = value
    return new_state_dict

def safe_get_nested(dictionary, keys, default=None):
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸çš„å€¼"""
    try:
        for key in keys:
            dictionary = dictionary[key]
        return dictionary
    except (KeyError, TypeError, IndexError):
        return default

def safe_load_model(model, model_path, model_name):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†DataParallelå‰ç¼€é—®é¢˜"""
    try:
        print(f"   æ­£åœ¨åŠ è½½{model_name}æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"   âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        state_dict = torch.load(model_path, map_location=device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤moduleå‰ç¼€
        has_module_prefix = any(key.startswith('module.') for key in state_dict.keys())
        if has_module_prefix:
            print(f"   æ£€æµ‹åˆ°DataParallelå‰ç¼€ï¼Œæ­£åœ¨ç§»é™¤...")
            state_dict = remove_module_prefix(state_dict)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„åŒ¹é…
        model_state_dict = model.state_dict()
        missing_keys = []
        unexpected_keys = []
        
        for key in model_state_dict.keys():
            if key not in state_dict:
                missing_keys.append(key)
        
        for key in state_dict.keys():
            if key not in model_state_dict:
                unexpected_keys.append(key)
        
        if missing_keys:
            print(f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys[:5]}..." if len(missing_keys) > 5 else f"   âš ï¸  ç¼ºå¤±é”®: {missing_keys}")
        
        if unexpected_keys:
            print(f"   âš ï¸  å¤šä½™é”®: {unexpected_keys[:5]}..." if len(unexpected_keys) > 5 else f"   âš ï¸  å¤šä½™é”®: {unexpected_keys}")
        
        # å°è¯•åŠ è½½
        model.load_state_dict(state_dict, strict=False)
        print(f"   âœ… {model_name}æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"   âŒ {model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# GPUé…ç½®æ£€æŸ¥
print("ğŸ–¥ï¸ GPUé…ç½®æ£€æŸ¥:")
print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"   å½“å‰è®¾å¤‡: {device}")

if torch.cuda.is_available():
    print("   GPUè¯¦ç»†ä¿¡æ¯:")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"     GPU {i}: {props.name}")
        print(f"       æ˜¾å­˜: {props.total_memory/1024**3:.1f}GB")
        print(f"       è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print("   âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
import matplotlib.font_manager as fm
from matplotlib import rcParams
import platform

def setup_chinese_fonts_strict():
    """LinuxæœåŠ¡å™¨ç¯å¢ƒä¸­æ–‡å­—ä½“é…ç½®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    import subprocess
    import os
    
    # 1. å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…ï¼ˆä»…Linuxï¼‰
    if platform.system() == "Linux":
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡ç†å‘˜æƒé™å®‰è£…å­—ä½“
            result = subprocess.run(['which', 'apt-get'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("ğŸ”§ æ­£åœ¨å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…...")
                subprocess.run(['sudo', 'apt-get', 'update'], capture_output=True, timeout=30)
                subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-noto-cjk', 'fonts-wqy-microhei', 'fonts-arphic-ukai'], capture_output=True, timeout=60)
        except Exception as e:
            print(f"âš ï¸ å­—ä½“å®‰è£…å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰: {e}")
    
    # 2. æ‰©å±•å€™é€‰å­—ä½“åˆ—è¡¨
    candidates = [
        # Linuxä¼˜å…ˆå­—ä½“
        'Noto Sans CJK SC Regular',
        'Noto Sans CJK SC',
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei',
        'Source Han Sans CN',
        'Source Han Sans SC',
        'AR PL UKai CN',
        'AR PL UMing CN',
        # é€šç”¨å­—ä½“
        'Droid Sans Fallback',
        'Liberation Sans',
        # Windowså…œåº•
        'Microsoft YaHei',
        'SimHei',
        'SimSun',
        # æœ€ç»ˆå…œåº•
        'DejaVu Sans',
        'Liberation Sans'
    ]

    chosen = None
    for name in candidates:
        try:
            font_path = fm.findfont(name, fallback_to_default=False)
            if font_path and 'DejaVu' not in font_path and os.path.exists(font_path):
                chosen = name
                print(f"ğŸ” æ‰¾åˆ°å­—ä½“: {name} -> {font_path}")
                break
        except Exception:
            continue

    # 3. å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚å­—ä½“ï¼Œå°è¯•ç³»ç»Ÿå­—ä½“æ‰«æ
    if chosen is None:
        print("ğŸ” è¿›è¡Œç³»ç»Ÿå­—ä½“æ‰«æ...")
        all_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = [f for f in all_fonts if any(keyword in f.lower() for keyword in ['cjk', 'han', 'hei', 'kai', 'ming', 'noto', 'wenquanyi'])]
        if chinese_fonts:
            chosen = chinese_fonts[0]
            print(f"ğŸ” é€šè¿‡æ‰«ææ‰¾åˆ°ä¸­æ–‡å­—ä½“: {chosen}")
        else:
            chosen = 'DejaVu Sans'
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨DejaVu Sans")

    # 4. å¢å¼ºçš„å…¨å±€æ¸²æŸ“å‚æ•°
    rcParams['font.family'] = 'sans-serif'
    rcParams['font.sans-serif'] = [chosen, 'DejaVu Sans', 'Liberation Sans']
    rcParams['axes.unicode_minus'] = False
    rcParams['pdf.fonttype'] = 42
    rcParams['ps.fonttype'] = 42
    rcParams['savefig.dpi'] = 300
    rcParams['figure.dpi'] = 100  # é™ä½ä»¥æé«˜å…¼å®¹æ€§
    rcParams['figure.autolayout'] = False
    rcParams['axes.titlesize'] = 12
    rcParams['axes.labelsize'] = 10
    rcParams['legend.fontsize'] = 9
    rcParams['xtick.labelsize'] = 9
    rcParams['ytick.labelsize'] = 9
    
    # 5. å¼ºåˆ¶å­—ä½“ç¼“å­˜é‡å»º
    try:
        fm._rebuild()
        # é¢å¤–æ¸…ç†ç¼“å­˜
        cache_dir = os.path.expanduser('~/.cache/matplotlib')
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir, ignore_errors=True)
    except Exception as e:
        print(f"âš ï¸ å­—ä½“ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    print(f"âœ… æœ€ç»ˆä½¿ç”¨å­—ä½“: {chosen}")
    
    # 6. æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
    try:
        plt.figure(figsize=(1, 1))
        plt.text(0.5, 0.5, 'Font Test', fontsize=10)
        plt.close()
        print("âœ… ä¸­æ–‡å­—ä½“æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“æµ‹è¯•å¤±è´¥: {e}")
        # é™çº§åˆ°å®‰å…¨æ¨¡å¼
        rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("ğŸ”„ å·²åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼‰")

# æ‰§è¡Œå­—ä½“é…ç½®ï¼ˆæ›´ç¨³å¥ï¼‰
setup_chinese_fonts_strict()

#----------------------------------------æµ‹è¯•é…ç½®------------------------------
print("="*60)
print("ğŸ”¬ ç”µæ± æ•…éšœè¯Šæ–­ç³»ç»Ÿ - BiLSTMæ¨¡å‹æµ‹è¯•")
print("="*60)

TEST_MODE = "BILSTM_ONLY"  # å›ºå®šä¸ºBiLSTMå•æ¨¡å‹æµ‹è¯•

# æµ‹è¯•æ•°æ®é›†é…ç½® (æ ¹æ®Labels.xlsåŠ¨æ€åŠ è½½)
def load_test_samples():
    """ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬"""
    try:
        import pandas as pd
        labels_path = '/mnt/bz25t/bzhy/zhanglikang/project/QAS/Labels.xls'
        df = pd.read_excel(labels_path)
        
        # æå–æµ‹è¯•æ ·æœ¬
        all_samples = df['Num'].tolist()
        all_labels = df['Label'].tolist()
        
        # æŒ‡å®šæµ‹è¯•æ ·æœ¬ï¼šæ­£å¸¸æ ·æœ¬10-20 å’Œæ•…éšœæ ·æœ¬340-350
        test_normal_samples = [str(i) for i in range(10, 21)]  # æ­£å¸¸æ ·æœ¬ï¼š10-20
        test_fault_samples = [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        
        print(f"ğŸ“‹ ä»Labels.xlsåŠ è½½æµ‹è¯•æ ·æœ¬:")
        print(f"   æµ‹è¯•æ­£å¸¸æ ·æœ¬: {test_normal_samples}")
        print(f"   æµ‹è¯•æ•…éšœæ ·æœ¬: {test_fault_samples}")
        
        return {
            'normal': test_normal_samples,
            'fault': test_fault_samples
        }
    except Exception as e:
        print(f"âŒ åŠ è½½Labels.xlså¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨é»˜è®¤æµ‹è¯•æ ·æœ¬")
        return {
            'normal': [str(i) for i in range(10, 21)],  # æ­£å¸¸æ ·æœ¬ï¼š10-20
            'fault': [str(i) for i in range(340, 351)]  # æ•…éšœæ ·æœ¬ï¼š340-350
        }

TEST_SAMPLES = load_test_samples()
ALL_TEST_SAMPLES = TEST_SAMPLES['normal'] + TEST_SAMPLES['fault']

# æ¨¡å‹è·¯å¾„é…ç½® (ä»BiLSTMè®­ç»ƒç»“æœç›®å½•åŠ è½½)
MODEL_PATHS = {
    "BILSTM": {
        "net_model": "/mnt/bz25t/bzhy/datasave/BILSTM/models/net_model_bilstm_baseline.pth",
        "netx_model": "/mnt/bz25t/bzhy/datasave/BILSTM/models/netx_model_bilstm_baseline.pth"
    }
}

# æ£€æµ‹æ¨¡å¼é…ç½®
DETECTION_MODES = {
    "three_point": {
        "name": "3ç‚¹æ£€æµ‹æ¨¡å¼ï¼ˆåŸç‰ˆï¼‰", 
        "description": "å¯¹äºæ•…éšœæ ·æœ¬ï¼Œå¦‚æœæŸç‚¹é«˜äºé˜ˆå€¼ä¸”å‰åç›¸é‚»ç‚¹ä¹Ÿé«˜äºé˜ˆå€¼ï¼Œåˆ™æ ‡è®°è¯¥ç‚¹åŠå‰å1ä¸ªç‚¹ï¼ˆå…±3ä¸ªç‚¹ï¼‰",
        "function": "three_point_fault_detection"
    },
    "three_point_improved": {
        "name": "3ç‚¹æ£€æµ‹æ¨¡å¼ï¼ˆæ”¹è¿›ç‰ˆï¼‰",
        "description": "æ”¹è¿›çš„3ç‚¹æ£€æµ‹ï¼šä¸¥æ ¼çš„è§¦å‘æ¡ä»¶ + åˆ†çº§æ ‡è®°èŒƒå›´ + æœ‰æ•ˆé™å™ªæœºåˆ¶",
        "function": "three_point_fault_detection"
    }
}

# å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ¨¡å¼
CURRENT_DETECTION_MODE = "three_point_improved"  # ä½¿ç”¨æ”¹è¿›çš„3ç‚¹æ£€æµ‹æ¨¡å¼
# å¤‡é€‰ï¼šå¦‚æœæ”¹è¿›ç‰ˆä»ç„¶è¿‡ä¸¥æ ¼ï¼Œå¯ä»¥åˆ‡æ¢å› "three_point" åŸç‰ˆ

# 3ç‚¹æ£€æµ‹é…ç½®
# è®¾è®¡åŸç†ï¼š
# 1. è§¦å‘æ¡ä»¶ï¼šä¸­å¿ƒç‚¹+å‰åç›¸é‚»ç‚¹å…±3ä¸ªç‚¹æ»¡è¶³é˜ˆå€¼æ¡ä»¶
# 2. æ ‡è®°èŒƒå›´ï¼šæ ‡è®°è§¦å‘ç‚¹å‰åå„1ä¸ªç‚¹ï¼ˆå…±3ä¸ªç‚¹ï¼‰
# 3. åˆ†çº§æ£€æµ‹ï¼šæ ¹æ®é˜ˆå€¼ä¸¥æ ¼ç¨‹åº¦åˆ†ä¸º3ä¸ªç­‰çº§
THREE_POINT_CONFIG = {
    "marking_range": 1,          # æ ‡è®°èŒƒå›´ï¼šå‰åå„1ä¸ªç‚¹
    "neighbor_check": True,      # æ˜¯å¦æ£€æŸ¥é‚»å±…ç‚¹
    "multi_level": True,         # æ˜¯å¦å¯ç”¨å¤šçº§æ£€æµ‹
    "startup_period": 3000       # å¯åŠ¨æœŸï¼ˆè·³è¿‡å‰3000ä¸ªç‚¹ï¼‰
}

# é«˜åˆ†è¾¨ç‡å¯è§†åŒ–é…ç½®
PLOT_CONFIG = {
    "dpi": 300,
    "figsize_large": (15, 12),
    "figsize_medium": (12, 8), 
    "bbox_inches": "tight"
}

print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
print(f"   æµ‹è¯•æ ·æœ¬: {ALL_TEST_SAMPLES}")
print(f"   æ­£å¸¸æ ·æœ¬: {TEST_SAMPLES['normal']}")
print(f"   æ•…éšœæ ·æœ¬: {TEST_SAMPLES['fault']}")
print(f"   æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
print(f"   3ç‚¹æ£€æµ‹å‚æ•°: {THREE_POINT_CONFIG}")
print(f"   3ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°3ç‚¹åŒºåŸŸ")

#----------------------------------------æ¨¡å‹æ–‡ä»¶æ£€æŸ¥------------------------------
def check_model_files():
    """æ£€æŸ¥BiLSTMæ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ” æ£€æŸ¥BiLSTMæ¨¡å‹æ–‡ä»¶...")
    
    missing_files = []
    paths = MODEL_PATHS["BILSTM"]
    
    # æ£€æŸ¥ä¸»æ¨¡å‹æ–‡ä»¶
    for key, path in paths.items():
        if not os.path.exists(path):
            missing_files.append(f"BILSTM: {path}")
            print(f"   âŒ ç¼ºå¤±: {path}")
        else:
            file_size = os.path.getsize(path) / (1024 * 1024)  # MB
            print(f"   âœ… å­˜åœ¨: {path} ({file_size:.1f}MB)")
    
    # æ£€æŸ¥PCAå‚æ•°æ–‡ä»¶ (ä»BiLSTMè®­ç»ƒç»“æœåŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/BILSTM/models/pca_params_bilstm_baseline.pkl"
    if not os.path.exists(pca_params_path):
        # å°è¯•ä»npyæ–‡ä»¶é‡å»ºPCAå‚æ•°
        print(f"   âš ï¸  PCAå‚æ•°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»npyæ–‡ä»¶é‡å»º...")
        try:
            # åŠ è½½PCAç›¸å…³å‚æ•°
            data_mean = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/data_mean_bilstm_baseline.npy")
            data_std = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/data_std_bilstm_baseline.npy")
            v = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/v_bilstm_baseline.npy")
            p_k = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/p_k_bilstm_baseline.npy")
            v_I = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/v_I_bilstm_baseline.npy")
            T_99_limit = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/T_99_limit_bilstm_baseline.npy")
            SPE_99_limit = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/SPE_99_limit_bilstm_baseline.npy")
            X = np.load("/mnt/bz25t/bzhy/datasave/BILSTM/models/X_bilstm_baseline.npy")
            
            # é‡å»ºPCAå‚æ•°å­—å…¸
            pca_params = {
                'data_mean': data_mean,
                'data_std': data_std,
                'v': v,
                'p_k': p_k,
                'v_I': v_I,
                'T_99_limit': T_99_limit,
                'SPE_99_limit': SPE_99_limit,
                'X': X
            }
            
            # ä¿å­˜é‡å»ºçš„PCAå‚æ•°
            with open(pca_params_path, 'wb') as f:
                pickle.dump(pca_params, f)
            print(f"   âœ… PCAå‚æ•°é‡å»ºå¹¶ä¿å­˜: {pca_params_path}")
            
        except Exception as e:
            missing_files.append(f"PCA_PARAMS: {pca_params_path}")
            print(f"   âŒ PCAå‚æ•°é‡å»ºå¤±è´¥: {e}")
    else:
        file_size = os.path.getsize(pca_params_path) / (1024 * 1024)  # MB
        print(f"   âœ… å­˜åœ¨: {pca_params_path} ({file_size:.1f}MB)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå¤± {len(missing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
        for file in missing_files:
            print(f"   {file}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿å·²è¿è¡ŒBiLSTMè®­ç»ƒè„šæœ¬")
        print("   2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥æ–‡ä»¶æƒé™")
        raise FileNotFoundError("è¯·å…ˆè¿è¡ŒBiLSTMè®­ç»ƒè„šæœ¬ç”Ÿæˆæ‰€éœ€æ¨¡å‹æ–‡ä»¶")
    
    print("âœ… BiLSTMæ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

# æ‰§è¡Œæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
check_model_files()

#----------------------------------------3ç‚¹æ•…éšœæ£€æµ‹æœºåˆ¶------------------------------
def three_point_fault_detection(fai_values, threshold1, sample_id, config=None):
    """
    æ”¹è¿›çš„3ç‚¹æ•…éšœæ£€æµ‹æœºåˆ¶ï¼šå¢å¼ºè¿ç»­æ€§æ£€æµ‹å’Œé™å™ªèƒ½åŠ›
    
    è®¾è®¡åŸç†ï¼š
    1. ä¸¥æ ¼çš„è§¦å‘æ¡ä»¶ï¼šè¦æ±‚ä¸­å¿ƒç‚¹åŠå…¶é‚»åŸŸæ»¡è¶³æ›´ä¸¥æ ¼çš„ä¸€è‡´æ€§
    2. åˆç†çš„æ ‡è®°èŒƒå›´ï¼šæ ¹æ®æ•…éšœçº§åˆ«æ ‡è®°3ä¸ªç‚¹çš„åŒºåŸŸ
    3. æœ‰æ•ˆçš„é™å™ªæœºåˆ¶ï¼šè¿‡æ»¤å­¤ç«‹å¼‚å¸¸ç‚¹ï¼Œå…³æ³¨æŒç»­æ€§æ•…éšœ
    
    Args:
        fai_values: ç»¼åˆè¯Šæ–­æŒ‡æ ‡åºåˆ—
        threshold1: ä¸€çº§é¢„è­¦é˜ˆå€¼
        sample_id: æ ·æœ¬IDï¼ˆç”¨äºè°ƒè¯•ï¼‰
        config: é…ç½®å‚æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå¯åŒ…å«threshold2, threshold3ï¼‰
    
    Returns:
        fault_labels: æ•…éšœæ ‡ç­¾åºåˆ— (0=æ­£å¸¸, 1=è½»å¾®æ•…éšœ, 2=ä¸­ç­‰æ•…éšœ, 3=ä¸¥é‡æ•…éšœ)
        detection_info: æ£€æµ‹è¿‡ç¨‹è¯¦ç»†ä¿¡æ¯
    """
    # ä¸Transformerä¿æŒä¸€è‡´ï¼šè·³è¿‡å¯åŠ¨æœŸå¹¶é¦–é€‰å¤–éƒ¨é˜ˆå€¼
    STARTUP_PERIOD = 3000
    fault_labels = np.zeros(len(fai_values), dtype=int)
    detection_info = {
        'trigger_points': [],
        'marked_regions': [],
        'detection_stats': {},
        'fai_stats': {
            'mean': np.mean(fai_values),
            'std': np.std(fai_values),
            'max': np.max(fai_values),
            'min': np.min(fai_values)
        },
        'startup_period': STARTUP_PERIOD
    }

    # æ ·æœ¬ç±»å‹åˆ¤å®šï¼ˆå­—ç¬¦ä¸²å¯¹é½ï¼‰
    sample_id_str = str(sample_id)
    is_fault_sample = sample_id_str in TEST_SAMPLES['fault']
    is_normal_sample = sample_id_str in TEST_SAMPLES['normal']

    if not is_fault_sample and not is_normal_sample:
        print(f"   âš ï¸ æ ·æœ¬{sample_id}æœªå‡ºç°åœ¨ä¸¤ç±»åˆ—è¡¨ä¸­ï¼Œé»˜è®¤æŒ‰æ•…éšœæ ·æœ¬å¤„ç†")
        is_fault_sample = True

    # æ­£å¸¸æ ·æœ¬ï¼šä¸æ ‡æ³¨ä»»ä½•æ•…éšœï¼Œè¾“å‡ºå‡é˜³æ€§ç»Ÿè®¡ï¼ˆä¸Transformerä¸€è‡´ï¼‰
    if not is_fault_sample:
        startup_fai = fai_values[:STARTUP_PERIOD] if len(fai_values) > STARTUP_PERIOD else fai_values
        stable_fai = fai_values[STARTUP_PERIOD:] if len(fai_values) > STARTUP_PERIOD else []
        startup_fp = np.sum(startup_fai > threshold1) if len(startup_fai) > 0 else 0
        stable_fp = np.sum(stable_fai > threshold1) if len(stable_fai) > 0 else 0
        total_fp = startup_fp + stable_fp
        detection_info['detection_stats'] = {
            'total_trigger_points': 0,
            'total_marked_regions': 0,
            'total_fault_points': 0,
            'fault_ratio': 0.0,
            'detection_mode': 'normal_sample',
            'startup_false_positives': int(startup_fp),
            'stable_false_positives': int(stable_fp),
            'total_false_positives': int(total_fp),
            'startup_fp_ratio': float(startup_fp/len(startup_fai)) if len(startup_fai) > 0 else 0.0,
            'stable_fp_ratio': float(stable_fp/len(stable_fai)) if len(stable_fai) > 0 else 0.0,
            'total_fp_ratio': float(total_fp/len(fai_values)) if len(fai_values) > 0 else 0.0
        }
        fault_labels.fill(0)
        return fault_labels, detection_info

    # è·å–/è®¡ç®—å¤šçº§é˜ˆå€¼ï¼ˆä¼˜å…ˆå¤–éƒ¨é…ç½®ï¼‰
    if config and 'threshold2' in config and 'threshold3' in config:
        threshold2 = config['threshold2']
        threshold3 = config['threshold3']
        print(f"   âœ… ä½¿ç”¨å¤–éƒ¨é˜ˆå€¼: T1={threshold1:.4f}, T2={threshold2:.4f}, T3={threshold3:.4f}")
    else:
        nm = STARTUP_PERIOD
        mm = len(fai_values)
        if mm > nm:
            baseline_fai = fai_values[nm:mm]
            mean_fai = np.mean(baseline_fai)
            std_fai = np.std(baseline_fai)
        else:
            mean_fai = np.mean(fai_values)
            std_fai = np.std(fai_values)
        threshold2 = mean_fai + 4.5 * std_fai
        threshold3 = mean_fai + 6.0 * std_fai
        print(f"   â„¹ï¸ å†…éƒ¨é˜ˆå€¼è®¡ç®—: T2={threshold2:.4f}, T3={threshold3:.4f}")
    
    # æ•…éšœæ ·æœ¬ï¼šå®æ–½æ”¹è¿›çš„å¤šçº§3ç‚¹æ£€æµ‹ï¼ˆä¸Transformerä¸€è‡´ï¼Œè·³è¿‡å¯åŠ¨æœŸï¼‰
    trigger_points = []
    marked_regions = []
    
    # ç­–ç•¥4.0ï¼šä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥
    print(f"   ğŸ”§ ç­–ç•¥4.0: ä¸‰çº§åˆ†çº§æ£€æµ‹ç­–ç•¥...")
    print(f"   åˆ†æç»“æœ: æ•…éšœæ ·æœ¬æœ‰{np.sum(fai_values > threshold1)}ä¸ªå¼‚å¸¸ç‚¹({np.sum(fai_values > threshold1)/len(fai_values)*100:.2f}%)")
    print(f"   é˜ˆå€¼åˆ†å¸ƒ: >6Ïƒ({np.sum(fai_values > threshold3)}ä¸ª), >4.5Ïƒ({np.sum(fai_values > threshold2)}ä¸ª), >3Ïƒ({np.sum(fai_values > threshold1)}ä¸ª)")
    
    detection_config = {
        'mode': 'hierarchical_v2',
        'level_3': {
            'center_threshold': threshold3,      # 6Ïƒ
            'neighbor_threshold': None,          # æ— é‚»åŸŸè¦æ±‚
            'min_neighbors': 0,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level3_high_confidence'
        },
        'level_2': {
            'center_threshold': threshold2,      # 4.5Ïƒ  
            'neighbor_threshold': threshold1,    # 3Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1
            'condition': 'level2_medium_confidence'
        },
        'level_1': {
            'center_threshold': threshold1,      # 3Ïƒ
            'neighbor_threshold': threshold1 * 0.67,  # 2Ïƒ
            'min_neighbors': 1,
            'marking_range': [-1, 0, 1],        # æ ‡è®°i-1, i, i+1 (3ä¸ªç‚¹)
            'condition': 'level1_basic_confidence'
        }
    }
    
    print(f"   æ£€æµ‹å‚æ•°:")
    print(f"   Level 3 (6Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold3:.4f}, æ— é‚»åŸŸè¦æ±‚, æ ‡è®°3ç‚¹")
    print(f"   Level 2 (4.5Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold2:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    print(f"   Level 1 (3Ïƒ): ä¸­å¿ƒé˜ˆå€¼={threshold1:.4f}, é‚»åŸŸé˜ˆå€¼={threshold1*0.67:.4f}, æœ€å°‘é‚»å±…=1ä¸ª, æ ‡è®°3ç‚¹")
    
    # ä¸‰çº§åˆ†çº§æ£€æµ‹å®ç°ï¼šä»ç¨³å®šæœŸå¼€å§‹
    triggers = []
    detection_start = max(STARTUP_PERIOD + 2, 2)
    detection_end = len(fai_values) - 2
    for i in range(detection_start, detection_end):
        neighborhood = fai_values[i-2:i+3]  # 5ä¸ªç‚¹çš„é‚»åŸŸ
        neighbors = [fai_values[i-2], fai_values[i-1], fai_values[i+1], fai_values[i+2]]  # 4ä¸ªé‚»å±…
        center = fai_values[i]
        
        triggered = False
        trigger_level = None
        trigger_condition = None
        detection_details = {}
        
        # Level 3: æœ€ä¸¥æ ¼é˜ˆå€¼ï¼Œæœ€å®½æ¾æ¡ä»¶ (6Ïƒ)
        if center > detection_config['level_3']['center_threshold']:
            triggered = True
            trigger_level = 3
            trigger_condition = detection_config['level_3']['condition']
            marking_range = detection_config['level_3']['marking_range']
            detection_details = {
                'center_value': center,
                'center_threshold': detection_config['level_3']['center_threshold'],
                'neighbors_above_threshold': 'N/A (no requirement)',
                'required_neighbors': 0,
                'neighborhood_values': neighborhood.tolist(),
                'trigger_reason': '6Ïƒ high confidence detection'
            }
            
        # Level 2: ä¸­ç­‰é˜ˆå€¼ï¼Œä¸­ç­‰æ¡ä»¶ (4.5Ïƒ) 
        elif center > detection_config['level_2']['center_threshold']:
            neighbors_above_t1 = np.sum(np.array(neighbors) > detection_config['level_2']['neighbor_threshold'])
            if neighbors_above_t1 >= detection_config['level_2']['min_neighbors']:
                triggered = True
                trigger_level = 2
                trigger_condition = detection_config['level_2']['condition']
                marking_range = detection_config['level_2']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_2']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_t1,
                    'required_neighbors': detection_config['level_2']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '4.5Ïƒ medium confidence detection'
                }
                
        # Level 1: æœ€ä½é˜ˆå€¼ï¼Œç›¸å¯¹ä¸¥æ ¼æ¡ä»¶ (3Ïƒ)
        elif center > detection_config['level_1']['center_threshold']:
            neighbors_above_2sigma = np.sum(np.array(neighbors) > detection_config['level_1']['neighbor_threshold'])
            if neighbors_above_2sigma >= detection_config['level_1']['min_neighbors']:
                triggered = True
                trigger_level = 1
                trigger_condition = detection_config['level_1']['condition']
                marking_range = detection_config['level_1']['marking_range']
                detection_details = {
                    'center_value': center,
                    'center_threshold': detection_config['level_1']['center_threshold'],
                    'neighbors_above_threshold': neighbors_above_2sigma,
                    'required_neighbors': detection_config['level_1']['min_neighbors'],
                    'neighborhood_values': neighborhood.tolist(),
                    'trigger_reason': '3Ïƒ basic confidence detection'
                }
        
        if triggered:
            # è®¡ç®—æ ‡è®°èŒƒå›´
            start_mark = max(0, i + min(marking_range))
            end_mark = min(len(fai_values), i + max(marking_range) + 1)
            
            triggers.append({
                'center': i,
                'level': trigger_level,
                'range': (start_mark, end_mark),
                'trigger_condition': trigger_condition,
                'detection_details': detection_details
            })
    
    # ç¬¬äºŒè½®ï¼šå¤„ç†æ‰€æœ‰è§¦å‘ç‚¹ï¼ˆåˆ†çº§å¤„ç†ï¼‰
    processed_triggers = []
    level_counts = {1: 0, 2: 0, 3: 0}
    
    for trigger in triggers:
        start, end = trigger['range']
        center = trigger['center']
        level = trigger['level']
        
        # ç»Ÿè®¡å„çº§åˆ«è§¦å‘æ¬¡æ•°
        level_counts[level] += 1
        
        # æ ‡è®°æ•…éšœåŒºåŸŸ
        fault_labels[start:end] = level  # ä½¿ç”¨çº§åˆ«ä½œä¸ºæ ‡è®°å€¼
        trigger_points.append(center)
        
        # è®°å½•åŒºåŸŸä¿¡æ¯
        region_data = fai_values[start:end]
        region_stats = {
            'mean_fai': np.mean(region_data),
            'max_fai': np.max(region_data),
            'min_fai': np.min(region_data),
            'std_fai': np.std(region_data),
            'length': end - start
        }
        
        marked_regions.append({
            'trigger_point': center,
            'level': level,  # åˆ†çº§æ ‡è®°
            'range': (start, end),
            'length': end - start,
            'region_stats': region_stats,
            'trigger_condition': trigger['trigger_condition'],
            'trigger_values': {
                'center': fai_values[center],
                'detection_level': f"Level {level}",
                'trigger_reason': trigger['detection_details']['trigger_reason']
            }
        })
        
        processed_triggers.append(trigger)
    
    print(f"   è§¦å‘ç»Ÿè®¡: Level 3({level_counts[3]}æ¬¡), Level 2({level_counts[2]}æ¬¡), Level 1({level_counts[1]}æ¬¡)")
    
    detection_info['trigger_points'] = trigger_points
    detection_info['marked_regions'] = marked_regions
    detection_info['processed_triggers'] = processed_triggers
    
    # ä¸ºå…¼å®¹æ—§ç‰ˆæ£€æµ‹æ¨¡å¼çš„å¯è§†åŒ–ä»£ç ï¼Œæ·»åŠ ç©ºçš„å…¼å®¹å­—æ®µ
    detection_info['candidate_points'] = []  # 3ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    detection_info['verified_points'] = []   # 3ç‚¹æ£€æµ‹æ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¸ºå…¼å®¹æ€§ä¿ç•™
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ†çº§æ£€æµ‹ï¼‰
    fault_count = np.sum(fault_labels > 0)  # å…¨åºåˆ—
    effective_labels = fault_labels[STARTUP_PERIOD:] if len(fault_labels) > STARTUP_PERIOD else fault_labels
    effective_fault_count = np.sum(effective_labels > 0) if len(effective_labels) > 0 else 0
    level1_count = np.sum(fault_labels == 1)
    level2_count = np.sum(fault_labels == 2)
    level3_count = np.sum(fault_labels == 3)

    detection_info['detection_stats'] = {
        'total_trigger_points': len(trigger_points),
        'total_marked_regions': len(marked_regions),
        'total_fault_points': int(fault_count),
        'effective_fault_points': int(effective_fault_count),
        'fault_ratio': float(fault_count / len(fault_labels)) if len(fault_labels) > 0 else 0.0,
        'effective_fault_ratio': float(effective_fault_count / len(effective_labels)) if len(effective_labels) > 0 else 0.0,
        'detection_mode': 'hierarchical_three_level_with_startup_skip',
        'startup_period': STARTUP_PERIOD,
        'effective_length': len(effective_labels) if len(effective_labels) > 0 else 0,
        'level_statistics': {
            'level_1_points': int(level1_count),
            'level_2_points': int(level2_count),
            'level_3_points': int(level3_count),
            'level_1_triggers': int(level_counts[1]),
            'level_2_triggers': int(level_counts[2]),
            'level_3_triggers': int(level_counts[3])
        },
        'mean_region_length': float(np.mean([m['length'] for m in marked_regions])) if marked_regions else 0.0,
        'mean_trigger_fai': float(np.mean([m['trigger_values']['center'] for m in marked_regions])) if marked_regions else 0.0,
        'strategy_used': 'strategy_4_hierarchical_detection_startup_aware',
        'parameters': detection_config
    }
    
    print(f"   â†’ ç­–ç•¥4.0æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={fault_count}ä¸ª ({fault_count/len(fault_labels)*100:.2f}%)")
    print(f"   â†’ åˆ†çº§ç»Ÿè®¡: L1={level1_count}ç‚¹, L2={level2_count}ç‚¹, L3={level3_count}ç‚¹")
    print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(triggers)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
    
    # æ·»åŠ æ”¹è¿›æ•ˆæœå¯¹æ¯”
    original_anomaly_count = np.sum(fai_values > threshold1)
    detected_fault_count = np.sum(fault_labels > 0)
    noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count) if original_anomaly_count > 0 else 0
    
    print(f"   â†’ é™å™ªæ•ˆæœ: åŸå§‹å¼‚å¸¸ç‚¹={original_anomaly_count}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    # å¦‚æœç­–ç•¥1æ²¡æœ‰æ£€æµ‹åˆ°æ•…éšœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2
    if detected_fault_count == 0 and is_fault_sample:
        print(f"   âš ï¸  ç­–ç•¥1æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç­–ç•¥2...")
        print(f"   ğŸ”§ ç­–ç•¥2: è¿›ä¸€æ­¥æ”¾å®½é‚»åŸŸè¦æ±‚ï¼ˆé‚»åŸŸé˜ˆå€¼=0.6Ã—3Ïƒ, æ— é‚»å±…è¦æ±‚ï¼‰")
        
        # é‡ç½®æ ‡ç­¾å’Œåˆ—è¡¨
        fault_labels.fill(0)
        trigger_points.clear()
        marked_regions.clear()
        
        # ç­–ç•¥2å‚æ•°
        strategy2_config = {
            'center_threshold': threshold1,           # ä¿æŒ3Ïƒé˜ˆå€¼
            'neighbor_threshold': threshold1 * 0.6,  # è¿›ä¸€æ­¥é™ä½é‚»åŸŸè¦æ±‚åˆ°60%
            'min_neighbors': 0,                      # ä¸è¦æ±‚é‚»å±…ï¼ˆçº¯ä¸­å¿ƒç‚¹æ£€æµ‹ï¼‰
            'marking_range': 1,                      # æ ‡è®°Â±1ä¸ªç‚¹ï¼ˆ3ç‚¹æ€»å…±ï¼‰
            'condition': 'strategy2_relaxed'
        }
        
        # ç­–ç•¥2æ£€æµ‹
        strategy2_triggers = []
        for i in range(2, len(fai_values) - 2):
            center = fai_values[i]
            
            # ç­–ç•¥2æ¡ä»¶ï¼šåªæ£€æŸ¥ä¸­å¿ƒç‚¹
            if center > strategy2_config['center_threshold']:
                start_mark = max(0, i - strategy2_config['marking_range'])
                end_mark = min(len(fai_values), i + strategy2_config['marking_range'] + 1)
                
                fault_labels[start_mark:end_mark] = 1
                trigger_points.append(i)
                
                region_data = fai_values[start_mark:end_mark]
                marked_regions.append({
                    'trigger_point': i,
                    'level': 1,
                    'range': (start_mark, end_mark),
                    'length': end_mark - start_mark,
                    'region_stats': {
                        'mean_fai': np.mean(region_data),
                        'max_fai': np.max(region_data),
                        'std_fai': np.std(region_data),
                        'length': end_mark - start_mark
                    },
                    'trigger_condition': strategy2_config['condition'],
                    'trigger_values': {
                        'center': center
                    }
                })
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detected_fault_count = np.sum(fault_labels > 0)
        
        # æ›´æ–°detection_info
        detection_info['trigger_points'] = trigger_points
        detection_info['marked_regions'] = marked_regions
        detection_info['detection_stats']['total_trigger_points'] = len(trigger_points)
        detection_info['detection_stats']['total_marked_regions'] = len(marked_regions)
        detection_info['detection_stats']['total_fault_points'] = detected_fault_count
        detection_info['detection_stats']['fault_ratio'] = detected_fault_count / len(fault_labels)
        detection_info['detection_stats']['strategy_used'] = 'strategy_2_center_only'
        detection_info['detection_stats']['parameters'] = strategy2_config
        
        noise_reduction_ratio = 1 - (detected_fault_count / original_anomaly_count) if original_anomaly_count > 0 else 0
        
        print(f"   â†’ ç­–ç•¥2æ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°æ•…éšœç‚¹={detected_fault_count}ä¸ª ({detected_fault_count/len(fault_labels)*100:.2f}%)")
        print(f"   â†’ è§¦å‘ç‚¹æ•°: {len(trigger_points)}ä¸ª, æ ‡è®°åŒºåŸŸ: {len(marked_regions)}ä¸ª")
        print(f"   â†’ ç­–ç•¥2é™å™ªæ•ˆæœ: åŸå§‹å¼‚å¸¸ç‚¹={original_anomaly_count}, æ£€æµ‹æ•…éšœç‚¹={detected_fault_count}, é™å™ªç‡={noise_reduction_ratio:.2%}")
    
    elif detected_fault_count == 0:
        print(f"   âš ï¸  æ­£å¸¸æ ·æœ¬æœªæ£€æµ‹åˆ°æ•…éšœç‚¹ï¼Œç¬¦åˆé¢„æœŸ")
        print(f"   â†’ æ£€æµ‹é€»è¾‘å·¥ä½œæ­£å¸¸")
    
    return fault_labels, detection_info


#----------------------------------------æ•°æ®åŠ è½½å‡½æ•°------------------------------
def load_test_sample(sample_id):
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    base_path = f'/mnt/bz25t/bzhy/zhanglikang/project/QAS/{sample_id}'
    
    # æ£€æŸ¥æ ·æœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_path):
        raise FileNotFoundError(f"æµ‹è¯•æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_path}")
    
    # åŠ è½½vin_1, vin_2, vin_3æ•°æ®
    try:
        with open(f'{base_path}/vin_1.pkl', 'rb') as f:
            vin1_data = pickle.load(f)
        with open(f'{base_path}/vin_2.pkl', 'rb') as f:
            vin2_data = pickle.load(f) 
        with open(f'{base_path}/vin_3.pkl', 'rb') as f:
            vin3_data = pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"æ ·æœ¬ {sample_id} æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        
    return vin1_data, vin2_data, vin3_data

def load_models():
    """åŠ è½½BiLSTMæ¨¡å‹"""
    models = {}
    
    print("ğŸ”§ å¼€å§‹åŠ è½½BiLSTMæ¨¡å‹...")
    
    # åŠ è½½MC-AEæ¨¡å‹ (BiLSTMè®­ç»ƒè„šæœ¬ä½¿ç”¨çš„æ˜¯CombinedAE)
    models['net'] = CombinedAE(input_size=2, encode2_input_size=3, output_size=110,
                              activation_fn=custom_activation, use_dx_in_forward=True).to(device)
    models['netx'] = CombinedAE(input_size=2, encode2_input_size=4, output_size=110, 
                               activation_fn=torch.sigmoid, use_dx_in_forward=True).to(device)
    
    # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°
    if not safe_load_model(models['net'], 
                          MODEL_PATHS["BILSTM"]["net_model"], 
                          "MC-AE1"):
        raise RuntimeError("MC-AE1æ¨¡å‹åŠ è½½å¤±è´¥")
    
    if not safe_load_model(models['netx'], 
                          MODEL_PATHS["BILSTM"]["netx_model"], 
                          "MC-AE2"):
        raise RuntimeError("MC-AE2æ¨¡å‹åŠ è½½å¤±è´¥")
    
    # åŠ è½½PCAå‚æ•° (ä»pickleæ–‡ä»¶åŠ è½½)
    pca_params_path = "/mnt/bz25t/bzhy/datasave/BILSTM/models/pca_params_bilstm_baseline.pkl"
    try:
        with open(pca_params_path, 'rb') as f:
            models['pca_params'] = pickle.load(f)
        print(f"âœ… PCAå‚æ•°å·²åŠ è½½: {pca_params_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½PCAå‚æ•°å¤±è´¥: {e}")
        raise RuntimeError("PCAå‚æ•°åŠ è½½å¤±è´¥")
    
    return models

#----------------------------------------å•æ ·æœ¬å¤„ç†å‡½æ•°------------------------------
def process_single_sample(sample_id, models):
    """å¤„ç†å•ä¸ªæµ‹è¯•æ ·æœ¬"""
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    vin1_data, vin2_data, vin3_data = load_test_sample(sample_id)
    
    # æ•°æ®é¢„å¤„ç†
    if len(vin1_data.shape) == 2:
        vin1_data = vin1_data.unsqueeze(1)
    vin1_data = vin1_data.to(torch.float32).to(device)

    # å®šä¹‰ç»´åº¦
    dim_x, dim_y, dim_z, dim_q = 2, 110, 110, 3
    dim_x2, dim_y2, dim_z2, dim_q2 = 2, 110, 110, 4
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è€Œä¸æ˜¯é‡æ–°è®¡ç®—
    pca_params = models['pca_params']
    
    # åˆ†ç¦»æ•°æ®
    x_recovered = vin2_data[:, :dim_x]
    y_recovered = vin2_data[:, dim_x:dim_x + dim_y]
    z_recovered = vin2_data[:, dim_x + dim_y: dim_x + dim_y + dim_z]
    q_recovered = vin2_data[:, dim_x + dim_y + dim_z:]
    
    x_recovered2 = vin3_data[:, :dim_x2]
    y_recovered2 = vin3_data[:, dim_x2:dim_x2 + dim_y2]
    z_recovered2 = vin3_data[:, dim_x2 + dim_y2: dim_x2 + dim_y2 + dim_z2]
    q_recovered2 = vin3_data[:, dim_x2 + dim_y2 + dim_z2:]
    
    # MC-AEæ¨ç†
    models['net'].eval()
    models['netx'].eval()
    
    with torch.no_grad():
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        x_recovered = x_recovered.float()
        z_recovered = z_recovered.float()
        q_recovered = q_recovered.float()
        x_recovered2 = x_recovered2.float()
        z_recovered2 = z_recovered2.float()
        q_recovered2 = q_recovered2.float()
        
        recon_imtest = models['net'](x_recovered, z_recovered, q_recovered)
        reconx_imtest = models['netx'](x_recovered2, z_recovered2, q_recovered2)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    AA = recon_imtest[0].cpu().detach().numpy()
    yTrainU = y_recovered.cpu().detach().numpy()
    ERRORU = AA - yTrainU

    BB = reconx_imtest[0].cpu().detach().numpy()
    yTrainX = y_recovered2.cpu().detach().numpy()
    ERRORX = BB - yTrainX

    # è¯Šæ–­ç‰¹å¾æå–
    df_data = DiagnosisFeature(ERRORU, ERRORX)
    
    # ä½¿ç”¨é¢„è®­ç»ƒçš„PCAå‚æ•°è¿›è¡Œç»¼åˆè®¡ç®—
    time = np.arange(df_data.shape[0])
    
    # å…ˆè¿›è¡Œåˆæ­¥è®¡ç®—è·å–faiå€¼ï¼Œç”¨äºé˜ˆå€¼è®¡ç®—
    print("   è¿›è¡Œåˆæ­¥FAIè®¡ç®—...")
    temp_result = Comprehensive_calculation(
        df_data.values, 
        pca_params['data_mean'], 
        pca_params['data_std'], 
        pca_params['v'].reshape(len(pca_params['v']), 1), 
        pca_params['p_k'], 
        pca_params['v_I'], 
        pca_params['T_99_limit'], 
        pca_params['SPE_99_limit'], 
        pca_params['X'], 
        time
    )
    temp_fai = temp_result[9]  # faiæ˜¯ç¬¬10ä¸ªè¿”å›å€¼ï¼ˆç´¢å¼•9ï¼‰
    
    # æŒ‰ç…§æºä»£ç æ–¹å¼è®¡ç®—é˜ˆå€¼ï¼ˆä¸æºä»£ç ä¿æŒä¸€è‡´ï¼‰
    print("   è®¡ç®—æŠ¥è­¦é˜ˆå€¼...")
    nm = 3000  # å›ºå®šå€¼ï¼Œä¸æºä»£ç ä¸€è‡´
    mm = len(temp_fai)  # æ•°æ®æ€»é•¿åº¦
    
    # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
    if mm > nm:
        # ä½¿ç”¨ååŠæ®µæ•°æ®è®¡ç®—é˜ˆå€¼
        threshold1 = np.mean(temp_fai[nm:mm]) + 3*np.std(temp_fai[nm:mm])
        threshold2 = np.mean(temp_fai[nm:mm]) + 4.5*np.std(temp_fai[nm:mm])
        threshold3 = np.mean(temp_fai[nm:mm]) + 6*np.std(temp_fai[nm:mm])
    else:
        # æ•°æ®å¤ªçŸ­ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        print(f"   âš ï¸ æ ·æœ¬{sample_id}æ•°æ®é•¿åº¦({mm})ä¸è¶³3000ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—é˜ˆå€¼")
        threshold1 = np.mean(temp_fai) + 3*np.std(temp_fai)
        threshold2 = np.mean(temp_fai) + 4.5*np.std(temp_fai)
        threshold3 = np.mean(temp_fai) + 6*np.std(temp_fai)
    
    print(f"   å¤–éƒ¨è®¡ç®—é˜ˆå€¼: L1={threshold1:.4f}, L2={threshold2:.4f}, L3={threshold3:.4f}")
    
    # ä½¿ç”¨å¤–éƒ¨è®¡ç®—çš„é˜ˆå€¼é‡æ–°è®¡ç®—æŠ¥è­¦ç­‰çº§ï¼ˆä¸ä½¿ç”¨Comprehensive_calculationå†…éƒ¨çš„æŠ¥è­¦ç­‰çº§ï¼‰
    print("   ä½¿ç”¨å¤–éƒ¨é˜ˆå€¼é‡æ–°è®¡ç®—æŠ¥è­¦ç­‰çº§...")
    lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, old_level, old_maxlevel, contTT, contQ, X_ratio, CContn, data_mean, data_std = temp_result
    
    # ä½¿ç”¨å¤–éƒ¨è®¡ç®—çš„é˜ˆå€¼é‡æ–°è®¡ç®—æŠ¥è­¦ç­‰çº§
    level = np.zeros_like(fai, dtype=int)
    level[fai > threshold1] = 1
    level[fai > threshold2] = 2
    level[fai > threshold3] = 3
    maxlevel = np.max(level)
    
    # æ˜¾ç¤ºä¿®æ­£åçš„æŠ¥è­¦ç»Ÿè®¡
    print(f"   å¤–éƒ¨é˜ˆå€¼æŠ¥è­¦ç‚¹æ•°: L1={np.sum(level==1)}, L2={np.sum(level==2)}, L3={np.sum(level==3)}")
    print(f"   æœ€å¤§æŠ¥è­¦ç­‰çº§: {maxlevel}")
    print(f"   (å†…éƒ¨è®¡ç®—çš„æŠ¥è­¦ç‚¹æ•°è¢«å¿½ç•¥ï¼Œä½¿ç”¨å¤–éƒ¨é˜ˆå€¼é‡æ–°è®¡ç®—)")
    
    # æ ¹æ®æ£€æµ‹æ¨¡å¼é€‰æ‹©æ£€æµ‹å‡½æ•°
    threshold_config = {
        'threshold1': threshold1,
        'threshold2': threshold2,
        'threshold3': threshold3
    }
    if CURRENT_DETECTION_MODE in ("three_point", "three_point_improved"):
        fault_labels, detection_info = three_point_fault_detection(fai, threshold1, sample_id, threshold_config)
    else:
        # é»˜è®¤ä½¿ç”¨3ç‚¹æ£€æµ‹
        fault_labels, detection_info = three_point_fault_detection(fai, threshold1, sample_id, threshold_config)
    
    # æ„å»ºç»“æœ
    sample_result = {
        'sample_id': sample_id,
        'model_type': 'BILSTM',
        'label': 1 if sample_id in TEST_SAMPLES['fault'] else 0,
        'df_data': df_data.values,
        'fai': fai,
        'T_squared': t_total,
        'SPE': q_total,
        'thresholds': {
            'threshold1': threshold1,
            'threshold2': threshold2, 
            'threshold3': threshold3
        },
        'fault_labels': fault_labels,
        'detection_info': detection_info,
        'performance_metrics': {
            'fai_mean': np.mean(fai),
            'fai_std': np.std(fai),
            'fai_max': np.max(fai),
            'fai_min': np.min(fai),
            'anomaly_count': np.sum(fai > threshold1),  # L1é˜ˆå€¼å¼‚å¸¸ç‚¹æ•°
            'anomaly_ratio': np.sum(fai > threshold1) / len(fai),
            'alarm_counts': {
                'L1': int(np.sum(level==1)),
                'L2': int(np.sum(level==2)), 
                'L3': int(np.sum(level==3))
            },
            'external_thresholds_used': True  # æ ‡è®°ä½¿ç”¨äº†å¤–éƒ¨é˜ˆå€¼
        }
    }
    
    return sample_result

#----------------------------------------ä¸»æµ‹è¯•æµç¨‹------------------------------
def main_test_process():
    """ä¸»è¦æµ‹è¯•æµç¨‹"""
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    test_results = {
        "BILSTM": [],
        "metadata": {
            "test_samples": TEST_SAMPLES,
            "three_point_config": THREE_POINT_CONFIG,
            "detection_modes": DETECTION_MODES,
            "current_mode": CURRENT_DETECTION_MODE,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    # BiLSTMå•æ¨¡å‹æµ‹è¯•
    total_operations = len(ALL_TEST_SAMPLES)  # 22ä¸ªæ ·æœ¬ (11æ­£å¸¸+11æ•…éšœ)
    
    print(f"\nğŸš€ å¼€å§‹BiLSTMæ¨¡å‹æµ‹è¯•...")
    print(f"æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
    print(f"æ£€æµ‹æè¿°: {DETECTION_MODES[CURRENT_DETECTION_MODE]['description']}")
    print(f"æ€»å…±éœ€è¦å¤„ç†: {total_operations} ä¸ªæ ·æœ¬")
    
    with tqdm(total=total_operations, desc="BiLSTMæµ‹è¯•è¿›åº¦", 
              bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}]') as pbar:
        
        print(f"\n{'='*20} æµ‹è¯• BiLSTM æ¨¡å‹ {'='*20}")
        
        # åŠ è½½æ¨¡å‹
        pbar.set_description(f"åŠ è½½BiLSTMæ¨¡å‹")
        models = load_models()
        print(f"âœ… BiLSTM æ¨¡å‹åŠ è½½å®Œæˆ")
        
        for sample_id in ALL_TEST_SAMPLES:
            pbar.set_description(f"BiLSTM-æ ·æœ¬{sample_id}")
            
            try:
                # å¤„ç†å•ä¸ªæ ·æœ¬
                sample_result = process_single_sample(sample_id, models)
                test_results["BILSTM"].append(sample_result)
                
                # è¾“å‡ºç®€è¦ç»“æœ
                metrics = sample_result.get('performance_metrics', {})
                detection_info = sample_result.get('detection_info', {})
                
                # 3ç‚¹æ£€æµ‹æ¨¡å¼ - å®‰å…¨è·å–æ£€æµ‹ç»Ÿè®¡
                detection_stats = detection_info.get('detection_stats', {})
                detection_ratio = detection_stats.get('fault_ratio', 0.0)
                
                print(f"   æ ·æœ¬{sample_id}: faiå‡å€¼={metrics.get('fai_mean', 0.0):.6f}, "
                      f"å¼‚å¸¸ç‡={metrics.get('anomaly_ratio', 0.0):.2%}, "
                      f"æ£€æµ‹ç‡={detection_ratio:.2%}")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                continue
            
            pbar.update(1)
            time.sleep(0.1)  # é¿å…è¿›åº¦æ¡æ›´æ–°è¿‡å¿«
    
    print(f"\nâœ… BiLSTMæµ‹è¯•å®Œæˆ!")
    print(f"   BiLSTM: æˆåŠŸå¤„ç† {len(test_results['BILSTM'])} ä¸ªæ ·æœ¬")
    
    return test_results

# æ‰§è¡Œä¸»æµ‹è¯•æµç¨‹
test_results = main_test_process()

#----------------------------------------æ€§èƒ½åˆ†æå‡½æ•°------------------------------
def calculate_performance_metrics(test_results):
    """è®¡ç®—BiLSTMæ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ”¬ è®¡ç®—BiLSTMæ€§èƒ½æŒ‡æ ‡...")
    
    performance_metrics = {}
    model_results = test_results["BILSTM"]
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_true_labels = []
    all_fai_values = []
    all_fault_predictions = []
    
    for result in model_results:
        true_label = result.get('label', 0)
        fai_values = result.get('fai', [])
        fault_labels = result.get('fault_labels', [])
        thresholds = result.get('thresholds', {})
        threshold1 = thresholds.get('threshold1', 0.0)
        
        # å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹
        for i, (fai_val, fault_pred) in enumerate(zip(fai_values, fault_labels)):
            # æ­£ç¡®çš„ROCé€»è¾‘ï¼šä½¿ç”¨ç‚¹çº§åˆ«çš„çœŸå®æ ‡ç­¾
            if true_label == 0:  # æ­£å¸¸æ ·æœ¬
                point_true_label = 0  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
            else:  # æ•…éšœæ ·æœ¬
                point_true_label = fault_pred  # æ•…éšœæ ·æœ¬ä½¿ç”¨3ç‚¹æ£€æµ‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾
            
            all_true_labels.append(point_true_label)  # ä½¿ç”¨ç‚¹çº§åˆ«æ ‡ç­¾
            all_fai_values.append(fai_val)
            
            # é¢„æµ‹é€»è¾‘ï¼šåŸºäºfaié˜ˆå€¼åˆ¤æ–­
            prediction = 1 if fai_val > threshold1 else 0
            all_fault_predictions.append(prediction)
    
    # è®¡ç®—ROCæŒ‡æ ‡
    all_true_labels = np.array(all_true_labels)
    all_fai_values = np.array(all_fai_values)
    all_fault_predictions = np.array(all_fault_predictions)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn = np.sum((all_true_labels == 0) & (all_fault_predictions == 0))
    fp = np.sum((all_true_labels == 0) & (all_fault_predictions == 1))
    fn = np.sum((all_true_labels == 1) & (all_fault_predictions == 0))
    tp = np.sum((all_true_labels == 1) & (all_fault_predictions == 1))
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    tpr = recall  # True Positive Rate = Recall
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
    
    # æ ·æœ¬çº§ç»Ÿè®¡
    sample_metrics = {
        'total_samples': len(model_results),
        'normal_samples': len([r for r in model_results if r['label'] == 0]),
        'fault_samples': len([r for r in model_results if r['label'] == 1]),
        'avg_fai_normal': np.mean([r['performance_metrics']['fai_mean'] 
                                 for r in model_results if r['label'] == 0]),
        'avg_fai_fault': np.mean([r['performance_metrics']['fai_mean'] 
                                for r in model_results if r['label'] == 1]),
        'avg_anomaly_ratio_normal': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                           for r in model_results if r['label'] == 0]),
        'avg_anomaly_ratio_fault': np.mean([r['performance_metrics']['anomaly_ratio'] 
                                          for r in model_results if r['label'] == 1])
    }
    
    performance_metrics["BILSTM"] = {
        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},
        'classification_metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1_score),
            'specificity': float(specificity),
            'tpr': float(tpr),
            'fpr': float(fpr)
        },
        'sample_metrics': sample_metrics,
        'roc_data': {
            'true_labels': all_true_labels.tolist(),
            'fai_values': all_fai_values.tolist(),
            'predictions': all_fault_predictions.tolist()
        }
    }
    
    return performance_metrics

#----------------------------------------ROCæ›²çº¿å¯¹æ¯”------------------------------
def create_roc_analysis(test_results, performance_metrics, save_path):
    """ç”ŸæˆBiLSTM ROCæ›²çº¿åˆ†æ"""
    print("   ğŸ“ˆ ç”ŸæˆBiLSTM ROCæ›²çº¿åˆ†æ...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_large"], constrained_layout=True)
    
    # === å­å›¾1: è¿ç»­é˜ˆå€¼ROCæ›²çº¿ ===
    ax1.set_title('(a) BiLSTM ROC Curve\n(Continuous Threshold Scan)')
    
    model_results = test_results["BILSTM"]
    
    # æ”¶é›†æ‰€æœ‰faiå€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè¿ç»­é˜ˆå€¼ROC
    all_fai = []
    all_labels = []
    all_fault_labels = []
    
    for result in model_results:
        all_fai.extend(result['fai'])
        all_labels.extend([result['label']] * len(result['fai']))
        all_fault_labels.extend(result['fault_labels'])
    
    all_fai = np.array(all_fai)
    all_labels = np.array(all_labels)
    all_fault_labels = np.array(all_fault_labels)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(all_fai) == 0:
        print("   âš ï¸ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„FAIæ•°æ®ï¼Œè·³è¿‡ROCæ›²çº¿ç”Ÿæˆ")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„ROCå›¾
        ax1.text(0.5, 0.5, 'No Data', ha='center', va='center', 
                transform=ax1.transAxes, fontsize=12)
        ax1.set_title('(a) BiLSTM ROC Curve\n(No Data)')
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.grid(True, alpha=0.3)
        return
    
    # ğŸ”§ æ·»åŠ æ•°æ®ç»Ÿè®¡åˆ†æ
    print(f"   ğŸ“Š BiLSTM ROCæ›²çº¿æ•°æ®ç»Ÿè®¡:")
    print(f"      æ€»æ•°æ®ç‚¹: {len(all_fai)}")
    print(f"      æ­£å¸¸æ ·æœ¬ç‚¹: {np.sum(all_labels == 0)}")
    print(f"      æ•…éšœæ ·æœ¬ç‚¹: {np.sum(all_labels == 1)}")
    print(f"      æ•…éšœæ ‡è®°ç‚¹: {np.sum(all_fault_labels == 1)}")
    print(f"      FAIèŒƒå›´: [{np.min(all_fai):.6f}, {np.max(all_fai):.6f}]")
    
    # ğŸ”§ æ”¹è¿›é˜ˆå€¼æ‰«æç­–ç•¥ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
    # æ–¹æ³•1ï¼šä½¿ç”¨åˆ†ä½æ•°èŒƒå›´é¿å…æç«¯å€¼å½±å“
    fai_p1 = np.percentile(all_fai, 1)   # 1%åˆ†ä½æ•°
    fai_p99 = np.percentile(all_fai, 99) # 99%åˆ†ä½æ•°
    fai_median = np.median(all_fai)
    fai_mean = np.mean(all_fai)
    
    print(f"   ğŸ“Š FAIåˆ†å¸ƒåˆ†æ:")
    print(f"      1%åˆ†ä½æ•°: {fai_p1:.6f}")
    print(f"      50%åˆ†ä½æ•°(ä¸­ä½æ•°): {fai_median:.6f}")
    print(f"      å‡å€¼: {fai_mean:.6f}")
    print(f"      99%åˆ†ä½æ•°: {fai_p99:.6f}")
    print(f"      æœ€å¤§å€¼: {np.max(all_fai):.6f}")
    
    # ä½¿ç”¨æ›´æ™ºèƒ½çš„é˜ˆå€¼èŒƒå›´ï¼šä»æœ€å°å€¼åˆ°99%åˆ†ä½æ•°ï¼Œé¿å…æç«¯å€¼
    threshold_min = np.min(all_fai)
    threshold_max = fai_p99  # ä½¿ç”¨99%åˆ†ä½æ•°è€Œä¸æ˜¯æœ€å¤§å€¼
    
    # ğŸ”§ ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ï¼šçº¿æ€§+å¯¹æ•°å°ºåº¦
    if threshold_max > threshold_min * 10:  # å¦‚æœèŒƒå›´è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°å°ºåº¦
        # æ–¹æ³•1ï¼šå¯¹æ•°å°ºåº¦æ‰«æï¼ˆå¤„ç†å¤§èŒƒå›´çš„æ•°æ®ï¼‰
        log_min = np.log10(max(threshold_min, 1e-10))  # é¿å…log(0)
        log_max = np.log10(threshold_max)
        log_thresholds = np.logspace(log_min, log_max, 50)
        
        # æ–¹æ³•2ï¼šçº¿æ€§æ‰«æï¼ˆå¤„ç†å°èŒƒå›´çš„æ•°æ®ï¼‰
        linear_thresholds = np.linspace(threshold_min, min(threshold_max, fai_median*3), 50)
        
        # åˆå¹¶å¹¶å»é‡
        thresholds = np.unique(np.concatenate([linear_thresholds, log_thresholds]))
        thresholds = np.sort(thresholds)
        
        print(f"   ğŸ“Š ä½¿ç”¨æ··åˆæ‰«æç­–ç•¥ (çº¿æ€§+å¯¹æ•°): {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    else:
        # èŒƒå›´è¾ƒå°æ—¶ä½¿ç”¨çº¿æ€§æ‰«æ
        thresholds = np.linspace(threshold_min, threshold_max, 100)
        print(f"   ğŸ“Š ä½¿ç”¨çº¿æ€§æ‰«æç­–ç•¥: {len(thresholds)}ä¸ªé˜ˆå€¼ç‚¹")
    
    print(f"   ğŸ“Š é˜ˆå€¼æ‰«æèŒƒå›´: [{threshold_min:.6f}, {threshold_max:.6f}]")
    
    tpr_list = []
    fpr_list = []
    
    for threshold in thresholds:
        tp = fp = tn = fn = 0
        
        for i, (fai_val, sample_label, fault_pred) in enumerate(zip(all_fai, all_labels, all_fault_labels)):
            # ğŸ”§ ä¿®å¤ROCæ›²çº¿é€»è¾‘ï¼šä½¿ç”¨ç‚¹çº§åˆ«çš„çœŸå®æ ‡ç­¾
            if sample_label == 0:  # æ­£å¸¸æ ·æœ¬çš„æ‰€æœ‰ç‚¹éƒ½æ˜¯æ­£å¸¸çš„
                point_true_label = 0
            else:  # æ•…éšœæ ·æœ¬ï¼šä½¿ç”¨æ•…éšœæ£€æµ‹ç®—æ³•çš„ç»“æœä½œä¸ºç‚¹çº§åˆ«çœŸå®æ ‡ç­¾
                point_true_label = fault_pred
            
            # é¢„æµ‹æ ‡ç­¾ï¼šç®€å•åŸºäºFAIé˜ˆå€¼
            predicted_label = 1 if fai_val > threshold else 0
            
            # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
            if point_true_label == 0 and predicted_label == 0:
                tn += 1
            elif point_true_label == 0 and predicted_label == 1:
                fp += 1
            elif point_true_label == 1 and predicted_label == 0:
                fn += 1
            elif point_true_label == 1 and predicted_label == 1:
                tp += 1
        
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        tpr_list.append(tpr)
        fpr_list.append(fpr)
    
    # è®¡ç®—AUC - éœ€è¦ç¡®ä¿fpr_listæ˜¯å•è°ƒé€’å¢çš„
    from sklearn.metrics import auc
    
    # ğŸ”§ ä¿®å¤AUCè®¡ç®—ï¼šç¡®ä¿FPRå•è°ƒé€’å¢
    combined = list(zip(fpr_list, tpr_list))
    combined.sort(key=lambda x: x[0])  # æŒ‰FPRæ’åº
    fpr_sorted, tpr_sorted = zip(*combined)
    
    auc_score = auc(fpr_sorted, tpr_sorted)
    
    print(f"   ğŸ“Š BiLSTM ROCæ›²çº¿è®¡ç®—ç»“æœ:")
    print(f"      é˜ˆå€¼æ•°é‡: {len(thresholds)}")
    print(f"      FPRèŒƒå›´: [{min(fpr_sorted):.3f}, {max(fpr_sorted):.3f}]")
    print(f"      TPRèŒƒå›´: [{min(tpr_sorted):.3f}, {max(tpr_sorted):.3f}]")
    print(f"      AUCå¾—åˆ†: {auc_score:.6f}")
    
    # ç»˜åˆ¶ROCæ›²çº¿ - ä½¿ç”¨æ’åºåçš„æ•°æ®
    ax1.plot(fpr_sorted, tpr_sorted, color='blue', linewidth=2,
            label=f'BiLSTM (AUC={auc_score:.3f})')
    
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # === å­å›¾2: å›ºå®šé˜ˆå€¼å·¥ä½œç‚¹ ===
    ax2.set_title('(b) Working Point\n(Three-Level Alarm Threshold)')
    
    metrics = performance_metrics["BILSTM"]['classification_metrics']
    ax2.scatter(metrics['fpr'], metrics['tpr'], 
               s=200, color='blue', 
               label=f'BiLSTM\n(TPR={metrics["tpr"]:.3f}, FPR={metrics["fpr"]:.3f})',
               marker='o', edgecolors='black', linewidth=2)
    
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # === å­å›¾3: æ€§èƒ½æŒ‡æ ‡å±•ç¤º ===
    ax3.set_title('(c) BiLSTM Classification Metrics')
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']
    metric_mapping = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1_score', 'Specificity': 'specificity'}
    bilstm_values = [performance_metrics["BILSTM"]['classification_metrics'][metric_mapping[m]] 
                     for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.6
    
    bars = ax3.bar(x, bilstm_values, width, color='blue', alpha=0.7)
    ax3.set_xlabel('Metrics')
    ax3.set_ylabel('Score')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics_names, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, bilstm_values):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === å­å›¾4: æ ·æœ¬çº§æ€§èƒ½å±•ç¤º ===
    ax4.set_title('(d) BiLSTM Sample-Level Performance')
    
    sample_metrics = ['Avg Ï†(Normal)', 'Avg Ï†(Fault)', 'Anomaly Rate(Normal)', 'Anomaly Rate(Fault)']
    bilstm_sample_values = [
        performance_metrics["BILSTM"]['sample_metrics']['avg_fai_normal'],
        performance_metrics["BILSTM"]['sample_metrics']['avg_fai_fault'],
        performance_metrics["BILSTM"]['sample_metrics']['avg_anomaly_ratio_normal'],
        performance_metrics["BILSTM"]['sample_metrics']['avg_anomaly_ratio_fault']
    ]
    
    x = np.arange(len(sample_metrics))
    bars = ax4.bar(x, bilstm_sample_values, width, color='blue', alpha=0.7)
    
    ax4.set_xlabel('Sample Metrics')
    ax4.set_ylabel('Value')
    ax4.set_xticks(x)
    ax4.set_xticklabels(sample_metrics, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, bilstm_sample_values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… BiLSTM ROCåˆ†æå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ•…éšœæ£€æµ‹æ—¶åºå›¾------------------------------
def create_fault_detection_timeline(test_results, save_path):
    """ç”ŸæˆBiLSTMæ•…éšœæ£€æµ‹æ—¶åºå›¾"""
    print("   ğŸ“Š ç”ŸæˆBiLSTMæ•…éšœæ£€æµ‹æ—¶åºå›¾...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•…éšœæ ·æœ¬
    
    fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True, constrained_layout=True)
    
    # æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„ç»“æœ
    sample_result = next((r for r in test_results["BILSTM"] if r.get('sample_id') == fault_sample_id), None)
    
    if sample_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        sample_result = test_results["BILSTM"][0] if test_results["BILSTM"] else None
    
    if sample_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = sample_result.get('fai', [])
    fault_labels = sample_result.get('fault_labels', [])
    thresholds = sample_result.get('thresholds', {})
    time_axis = np.arange(len(fai_values))
    
    # å­å›¾1: ç»¼åˆè¯Šæ–­æŒ‡æ ‡æ—¶åº
    ax1 = axes[0]
    ax1.plot(time_axis, fai_values, color='blue', linewidth=1, alpha=0.8,
           label='BiLSTM FAI')
    ax1.axhline(y=thresholds['threshold1'], color='orange', linestyle='--', alpha=0.7,
              label='Level 1 Threshold')
    ax1.axhline(y=thresholds['threshold2'], color='red', linestyle='--', alpha=0.7,
              label='Level 2 Threshold')
    ax1.axhline(y=thresholds['threshold3'], color='darkred', linestyle='--', alpha=0.7,
              label='Level 3 Threshold')
    
    ax1.set_ylabel('BiLSTM\nComprehensive Diagnostic Index Ï†')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_title(f'BiLSTM - Sample {fault_sample_id} (Fault Sample)')
    
    # å­å›¾2: æ•…éšœæ£€æµ‹ç»“æœ
    ax2 = axes[1]
    
    # å°†æ•…éšœæ ‡ç­¾è½¬æ¢ä¸ºå¯è§†åŒ–åŒºåŸŸ
    fault_regions = np.where(fault_labels == 1, 0.8, 0)
    ax2.fill_between(time_axis, fault_regions, 
                    alpha=0.6, color='blue',
                    label='BiLSTM Fault Detection')
    
    ax2.set_ylabel('Fault Detection Result')
    ax2.set_ylim(0, 1)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_title('BiLSTM Fault Detection Result')
    
    # å­å›¾3: æ£€æµ‹è¿‡ç¨‹ï¼ˆå…¼å®¹ä¸¤ç§æ¨¡å¼ï¼‰
    ax3 = axes[2]
    detection_info = sample_result['detection_info']
    
    ax3.plot(time_axis, fai_values, 'b-', alpha=0.5, label='Ï† Index Value')
    
    # 3ç‚¹æ£€æµ‹æ¨¡å¼
    # æ ‡è®°è§¦å‘ç‚¹
    if detection_info.get('trigger_points'):
        ax3.scatter(detection_info['trigger_points'], 
                   [fai_values[i] for i in detection_info['trigger_points']],
                   color='orange', s=30, label='Trigger Points', alpha=0.8)
    
    ax3.set_ylabel('3-Point Detection\nProcess')
    ax3.set_title('3-Point Detection Process (BiLSTM)')
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    if detection_info.get('marked_regions'):
        for i, region in enumerate(detection_info['marked_regions']):
            start, end = region['range']
            label = 'Marked Fault Region' if i == 0 else ""
            ax3.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax3.set_xlabel('Time Step')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… BiLSTMæ—¶åºå›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾------------------------------
def create_performance_radar(performance_metrics, save_path):
    """ç”ŸæˆBiLSTMæ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    print("   ğŸ•¸ï¸ ç”ŸæˆBiLSTMæ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
    
    # å®šä¹‰é›·è¾¾å›¾æŒ‡æ ‡
    radar_metrics = {
        'Accuracy': 'accuracy',
        'Precision': 'precision', 
        'Recall': 'recall',
        'F1-Score': 'f1_score',
        'Specificity': 'specificity',
        'Early Warning': 'tpr',  # æ—©æœŸé¢„è­¦èƒ½åŠ› (TPR)
        'False Alarm Control': 'fpr',  # è¯¯æŠ¥æ§åˆ¶ (1-FPR)
        'Detection Stability': 'accuracy'  # æ£€æµ‹ç¨³å®šæ€§ (ç”¨å‡†ç¡®ç‡ä»£è¡¨)
    }
    
    # æ•°æ®é¢„å¤„ç†ï¼šFPRéœ€è¦è½¬æ¢ä¸ºæ§åˆ¶èƒ½åŠ› (1-FPR)
    bilstm_values = []
    
    for metric_name, metric_key in radar_metrics.items():
        bilstm_val = performance_metrics['BILSTM']['classification_metrics'][metric_key]
        
        # ç‰¹æ®Šå¤„ç†ï¼šè¯¯æŠ¥æ§åˆ¶ = 1 - FPR
        if metric_name == 'False Alarm Control':
            bilstm_val = 1 - bilstm_val
            
        bilstm_values.append(bilstm_val)
    
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    bilstm_values += bilstm_values[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize_medium"], subplot_kw=dict(projection='polar'), constrained_layout=True)
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles, bilstm_values, 'o-', linewidth=2, label='BiLSTM', color='blue')
    ax.fill(angles, bilstm_values, alpha=0.25, color='blue')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(list(radar_metrics.keys()))
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('BiLSTM Performance Metrics Radar Chart', 
              pad=20, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # æ·»åŠ æ€§èƒ½æ€»ç»“
    bilstm_avg = np.mean(bilstm_values[:-1])
    
    plt.figtext(0.02, 0.02, f'BiLSTM Overall Performance: {bilstm_avg:.3f}', 
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… BiLSTMé›·è¾¾å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------3ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–------------------------------
def create_three_point_visualization(test_results, save_path):
    """ç”ŸæˆBiLSTM 3ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–"""
    print("   ğŸ” ç”ŸæˆBiLSTM 3ç‚¹æ£€æµ‹è¿‡ç¨‹å¯è§†åŒ–...")
    
    # é€‰æ‹©ä¸€ä¸ªæ•…éšœæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    fault_sample_id = TEST_SAMPLES['fault'][0] if TEST_SAMPLES['fault'] else '335'
    
    fig = plt.figure(figsize=(16, 10), constrained_layout=True)
    
    # ä½¿ç”¨GridSpecè¿›è¡Œå¤æ‚å¸ƒå±€
    gs = fig.add_gridspec(3, 4, height_ratios=[2, 1, 1], width_ratios=[1, 1, 1, 1])
    
    # === ä¸»å›¾ï¼š3ç‚¹æ£€æµ‹è¿‡ç¨‹æ—¶åºå›¾ ===
    ax_main = fig.add_subplot(gs[0, :])
    
    # é€‰æ‹©BiLSTMç»“æœè¿›è¡Œå¯è§†åŒ–
    bilstm_result = next((r for r in test_results['BILSTM'] if r.get('sample_id') == fault_sample_id), None)
    
    if bilstm_result is None:
        print(f"   âš ï¸ æœªæ‰¾åˆ°æ ·æœ¬ {fault_sample_id} çš„ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç»“æœ")
        bilstm_result = test_results['BILSTM'][0] if test_results['BILSTM'] else None
    
    if bilstm_result is None:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    fai_values = bilstm_result.get('fai', [])
    detection_info = bilstm_result.get('detection_info', {})
    thresholds = bilstm_result.get('thresholds', {})
    threshold1 = thresholds.get('threshold1', 0.0)
    
    time_axis = np.arange(len(fai_values))
    
    # ç»˜åˆ¶FAIæ—¶åº
    ax_main.plot(time_axis, fai_values, 'b-', linewidth=1.5, alpha=0.8, label='Comprehensive Diagnostic Index Ï†(FAI)')
    ax_main.axhline(y=threshold1, color='red', linestyle='--', alpha=0.7, label='Level 1 Threshold')
    
    # 3ç‚¹æ£€æµ‹æ¨¡å¼
    # æ ‡è®°è§¦å‘ç‚¹
    if detection_info.get('trigger_points'):
        trigger_points = detection_info['trigger_points']
        ax_main.scatter(trigger_points, [fai_values[i] for i in trigger_points],
                       color='orange', s=40, alpha=0.8, label=f'Trigger: {len(trigger_points)} Points',
                       marker='o', zorder=5)
    
    # æ ‡è®°æ•…éšœåŒºåŸŸ
    for i, region in enumerate(detection_info['marked_regions']):
        start, end = region['range']
        label = 'Marked: Fault Region' if i == 0 else ""
        ax_main.axvspan(start, end, alpha=0.2, color='red', label=label)
    
    ax_main.set_xlabel('Time Step')
    ax_main.set_ylabel('Comprehensive Diagnostic Index Ï†')
    
    # è®¾ç½®æ ‡é¢˜
    title = f'BiLSTM Three-Point Fault Detection Process - Sample {fault_sample_id}'
    ax_main.set_title(title, fontsize=14, fontweight='bold')
    ax_main.legend(loc='upper left')
    ax_main.grid(True, alpha=0.3)
    
    # === å­å›¾1ï¼šæ£€æµ‹çª—å£ç»Ÿè®¡ ===
    ax1 = fig.add_subplot(gs[1, 0])
    
    detection_stats = detection_info.get('detection_stats', {})
    detection_data = [
        detection_stats.get('total_trigger_points', 0),
        detection_stats.get('total_marked_regions', 0), 
        detection_stats.get('total_fault_points', 0)
    ]
    detection_labels = ['Trigger Points', 'Marked Regions', 'Fault Points']
    colors1 = ['orange', 'red', 'darkred']
    
    bars1 = ax1.bar(detection_labels, detection_data, color=colors1, alpha=0.7)
    ax1.set_title('Detection Statistics')
    ax1.set_ylabel('Count')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, detection_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(value), ha='center', va='bottom')
    
    # === å­å›¾2ï¼šæ£€æµ‹æ¨¡å¼é…ç½® ===
    ax2 = fig.add_subplot(gs[1, 1])
    
    # 3ç‚¹æ£€æµ‹æ¨¡å¼
    mode_params = [3, 2, 1]  # 3ç‚¹åŒºåŸŸ, 2ä¸ªé‚»å±…, 1ä¸ªä¸­å¿ƒç‚¹
    mode_labels = ['Marked Region\n(3 Points)', 'Neighbor Check\n(2 Points)', 'Center Point\n(1 Point)']
    colors2 = ['lightblue', 'lightgreen', 'lightcoral']
    
    wedges, texts, autotexts = ax2.pie(mode_params, labels=mode_labels, colors=colors2,
                                      autopct='%1.0f', startangle=90)
    ax2.set_title('3-Point Detection\nParameter Config')
    
    # === å­å›¾3ï¼šæ£€æµ‹è¯¦æƒ… ===
    ax3 = fig.add_subplot(gs[1, 2])
    
    # 3ç‚¹æ£€æµ‹æ¨¡å¼ï¼šæ˜¾ç¤ºè§¦å‘ç‚¹çš„FAIå€¼åˆ†å¸ƒ
    if detection_info.get('trigger_points'):
        trigger_points = detection_info['trigger_points']
        trigger_fai_values = [fai_values[i] for i in trigger_points]
        
        bars3 = ax3.bar(range(len(trigger_fai_values)), trigger_fai_values, 
                       color='orange', alpha=0.7)
        ax3.axhline(y=threshold1, color='red', linestyle='--', 
                   alpha=0.7, label='Level 1 Threshold')
        ax3.set_title('Trigger Point FAI Values')
        ax3.set_xlabel('Trigger Point')
        ax3.set_ylabel('FAI Value')
        ax3.set_xticks(range(len(trigger_fai_values)))
        ax3.set_xticklabels([f'T{i+1}' for i in range(len(trigger_fai_values))])
        ax3.legend()
    else:
        ax3.text(0.5, 0.5, 'No Trigger Points', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('Trigger Point FAI Values')
    
    # === å­å›¾4ï¼šBiLSTMæ€§èƒ½ ===
    ax4 = fig.add_subplot(gs[1, 3])
    
    sample_result = next((r for r in test_results['BILSTM'] if r.get('sample_id') == fault_sample_id), None)
    if sample_result is None:
        sample_result = test_results['BILSTM'][0] if test_results['BILSTM'] else None
    
    if sample_result is None:
        fault_ratio = 0.0
    else:
        detection_info = sample_result.get('detection_info', {})
        detection_stats = detection_info.get('detection_stats', {})
        fault_ratio = detection_stats.get('fault_ratio', 0.0)
    
    bars4 = ax4.bar(['BiLSTM'], [fault_ratio], color='blue', alpha=0.7)
    ax4.set_title('BiLSTM\n(Fault Detection Ratio)')
    ax4.set_ylabel('Fault Ratio')
    
    for bar, value in zip(bars4, [fault_ratio]):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # === åº•éƒ¨ï¼šè¿‡ç¨‹è¯´æ˜ ===
    process_text = """
    BiLSTM Three-Point Detection Process:
    
    1. Time Series Scan: Check Ï†(FAI) values point by point against threshold
    2. Trigger Condition: Triggered when current point and neighbors exceed threshold
    3. Region Marking: Mark current point and Â±1 neighboring points (3 total) as fault
    
    Advantage: Precise detection with minimal false positives and efficient computation
    """
    
    fig.text(0.02, 0.02, process_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=PLOT_CONFIG["dpi"], bbox_inches=PLOT_CONFIG["bbox_inches"], facecolor='white')
    plt.close()
    
    print(f"   âœ… BiLSTM 3ç‚¹æ£€æµ‹è¿‡ç¨‹å›¾ä¿å­˜è‡³: {save_path}")

#----------------------------------------ç»“æœä¿å­˜å‡½æ•°------------------------------
def save_test_results(test_results, performance_metrics):
    """ä¿å­˜BiLSTMæµ‹è¯•ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜BiLSTMæµ‹è¯•ç»“æœ...")
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = f"/mnt/bz25t/bzhy/datasave/BILSTM/test_results/bilstm_test_results_{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(f"{result_dir}/visualizations", exist_ok=True)
    os.makedirs(f"{result_dir}/detailed_results", exist_ok=True)
    
    # 1. ä¿å­˜æ€§èƒ½æŒ‡æ ‡JSON
    performance_file = f"{result_dir}/bilstm_performance_metrics.json"
    with open(performance_file, 'w', encoding='utf-8') as f:
        json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
    print(f"   âœ… BiLSTMæ€§èƒ½æŒ‡æ ‡ä¿å­˜è‡³: {performance_file}")
    
    # 2. ä¿å­˜è¯¦ç»†ç»“æœ
    detail_file = f"{result_dir}/detailed_results/bilstm_detailed_results.pkl"
    with open(detail_file, 'wb') as f:
        pickle.dump(test_results["BILSTM"], f)
    print(f"   âœ… BiLSTMè¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
    
    # 3. ä¿å­˜å…ƒæ•°æ®
    metadata_file = f"{result_dir}/detailed_results/bilstm_test_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(test_results['metadata'], f, indent=2, ensure_ascii=False)
    print(f"   âœ… BiLSTMæµ‹è¯•å…ƒæ•°æ®ä¿å­˜è‡³: {metadata_file}")
    
    # 4. åˆ›å»ºExcelæ€»ç»“æŠ¥å‘Š
    summary_file = f"{result_dir}/detailed_results/bilstm_summary.xlsx"
    
    with pd.ExcelWriter(summary_file) as writer:
        # BiLSTMæ€§èƒ½è¡¨
        metrics = performance_metrics["BILSTM"]['classification_metrics']
        confusion = performance_metrics["BILSTM"]['confusion_matrix']
        sample_metrics = performance_metrics["BILSTM"]['sample_metrics']
        
        performance_data = [{
            'Model': 'BILSTM',
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'Specificity': metrics['specificity'],
            'TPR': metrics['tpr'],
            'FPR': metrics['fpr'],
            'TP': confusion['TP'],
            'FP': confusion['FP'],
            'TN': confusion['TN'],
            'FN': confusion['FN'],
            'Avg_FAI_Normal': sample_metrics['avg_fai_normal'],
            'Avg_FAI_Fault': sample_metrics['avg_fai_fault']
        }]
        
        performance_df = pd.DataFrame(performance_data)
        performance_df.to_excel(writer, sheet_name='BILSTM_Performance', index=False)
        
        # æ ·æœ¬è¯¦æƒ…è¡¨
        sample_details = []
        for result in test_results["BILSTM"]:
            # å®‰å…¨è·å–detection_infoå’Œdetection_stats
            detection_info = result.get('detection_info', {})
            detection_stats = detection_info.get('detection_stats', {})
            performance_metrics = result.get('performance_metrics', {})
            
            sample_details.append({
                'Sample_ID': result.get('sample_id', 'Unknown'),
                'True_Label': 'Fault' if result.get('label', 0) == 1 else 'Normal',
                'FAI_Mean': performance_metrics.get('fai_mean', 0.0),
                'FAI_Std': performance_metrics.get('fai_std', 0.0),
                'FAI_Max': performance_metrics.get('fai_max', 0.0),
                'Anomaly_Ratio': performance_metrics.get('anomaly_ratio', 0.0),
                'Fault_Detection_Ratio': detection_stats.get('fault_ratio', 0.0),
                'Trigger_Points_Found': detection_stats.get('total_trigger_points', 0),
                'Marked_Regions': detection_stats.get('total_marked_regions', 0)
            })
        
        sample_df = pd.DataFrame(sample_details)
        sample_df.to_excel(writer, sheet_name='Sample_Details', index=False)
    
    print(f"   âœ… BiLSTM Excelæ€»ç»“æŠ¥å‘Šä¿å­˜è‡³: {summary_file}")
    
    return result_dir

#----------------------------------------ä¸»æ‰§è¡Œæµç¨‹------------------------------
print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
performance_metrics = calculate_performance_metrics(test_results)

# ä¿å­˜æµ‹è¯•ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
result_dir = save_test_results(test_results, performance_metrics)

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
print("\nğŸ¨ ç”ŸæˆBiLSTMå¯è§†åŒ–åˆ†æ...")

# ç”ŸæˆROCåˆ†æå›¾
create_roc_analysis(test_results, performance_metrics, f"{result_dir}/visualizations/bilstm_roc_analysis.png")

# ç”Ÿæˆæ•…éšœæ£€æµ‹æ—¶åºå›¾
create_fault_detection_timeline(test_results, f"{result_dir}/visualizations/bilstm_fault_detection_timeline.png")

# ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
create_performance_radar(performance_metrics, f"{result_dir}/visualizations/bilstm_performance_radar.png")

# ç”Ÿæˆ3ç‚¹æ£€æµ‹è¿‡ç¨‹å›¾
create_three_point_visualization(test_results, f"{result_dir}/visualizations/bilstm_three_point_process.png")

#----------------------------------------æœ€ç»ˆæ€»ç»“------------------------------
print("\n" + "="*80)
print("ğŸ‰ BiLSTMæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(ALL_TEST_SAMPLES)} ä¸ª (æ­£å¸¸: {len(TEST_SAMPLES['normal'])}, æ•…éšœ: {len(TEST_SAMPLES['fault'])})")
print(f"   â€¢ æ¨¡å‹ç±»å‹: BiLSTM")
print(f"   â€¢ æ£€æµ‹æ¨¡å¼: {DETECTION_MODES[CURRENT_DETECTION_MODE]['name']}")
print(f"   â€¢ 3ç‚¹æ£€æµ‹å‚æ•°: {THREE_POINT_CONFIG}")
print(f"   â€¢ 3ç‚¹æ£€æµ‹æ¨¡å¼: å½“å‰ç‚¹+å‰åç›¸é‚»ç‚¹é«˜äºé˜ˆå€¼æ—¶ï¼Œæ ‡è®°3ç‚¹åŒºåŸŸ")

print(f"\nğŸ”¬ BiLSTMæ€§èƒ½:")
metrics = performance_metrics["BILSTM"]['classification_metrics']
print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")
print(f"   TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   â€¢ ç»“æœç›®å½•: {result_dir}")
print(f"   â€¢ å¯è§†åŒ–å›¾è¡¨: {result_dir}/visualizations")
print(f"     - ROCåˆ†æå›¾: bilstm_roc_analysis.png")
print(f"     - æ•…éšœæ£€æµ‹æ—¶åºå›¾: bilstm_fault_detection_timeline.png") 
print(f"     - æ€§èƒ½é›·è¾¾å›¾: bilstm_performance_radar.png")
print(f"     - 3ç‚¹æ£€æµ‹è¿‡ç¨‹å›¾: bilstm_three_point_process.png")
print(f"   â€¢ æ€§èƒ½æŒ‡æ ‡: bilstm_performance_metrics.json")
print(f"   â€¢ ExcelæŠ¥å‘Š: bilstm_summary.xlsx")

# ç»¼åˆæ€§èƒ½è¯„ä¼°
bilstm_score = np.mean(list(performance_metrics["BILSTM"]['classification_metrics'].values()))

print(f"\nğŸ† BiLSTMç»¼åˆæ€§èƒ½è¯„ä¼°:")
print(f"   ç»¼åˆå¾—åˆ†: {bilstm_score:.3f}")

print("\n" + "="*80)
print("BiLSTMæµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Šã€‚")
print("="*80)