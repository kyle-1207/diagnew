#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„å®éªŒæ‰§è¡Œè„šæœ¬
ç›´æ¥è°ƒç”¨ç°æœ‰å‡½æ•°ï¼Œå¿«é€ŸéªŒè¯æƒ³æ³•
"""

import sys
import os
import numpy as np
import pandas as pd
import torch
import pickle
from datetime import datetime

# å¯¼å…¥ç°æœ‰æ¨¡å—
try:
    from Function_ import *
    from Class_ import *
    from Comprehensive_calculation import Comprehensive_calculation
    print("âœ… æˆåŠŸå¯¼å…¥ç°æœ‰æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

def quick_test():
    """å¿«é€Ÿæµ‹è¯•ç°æœ‰å‡½æ•°"""
    
    print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•ç°æœ‰å‡½æ•°...")
    
    # 1. æµ‹è¯•æ•°æ®åŠ è½½
    print("1. æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        # å°è¯•åŠ è½½ä¸€ä¸ªQASæ ·æœ¬
        sample_path = "../QAS/0"
        if os.path.exists(sample_path):
            files = os.listdir(sample_path)
            print(f"   æ ·æœ¬0åŒ…å«æ–‡ä»¶: {files}")
        else:
            print(f"   âš ï¸  æ ·æœ¬è·¯å¾„ä¸å­˜åœ¨: {sample_path}")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
    
    # 2. æµ‹è¯•PCAå‡½æ•°
    print("2. æµ‹è¯•PCAå‡½æ•°...")
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æµ‹è¯•PCA
        test_data = np.random.rand(100, 7)  # 100ä¸ªæ ·æœ¬ï¼Œ7ä¸ªç‰¹å¾
        
        result = PCA(test_data, 0.99, 0.99)
        v_I, v, v_ratio, p_k, data_mean, data_std, T_95_limit, T_99_limit, SPE_95_limit, SPE_99_limit, P, k, P_t, X, data_nor = result
        
        print(f"   âœ… PCAæˆåŠŸï¼Œä¸»æˆåˆ†æ•°é‡: {len(k)}ï¼Œç´¯ç§¯è´¡çŒ®ç‡: {v_ratio[k[0]-1]:.3f}")
        
    except Exception as e:
        print(f"   âŒ PCAæµ‹è¯•å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•ç»¼åˆè¯Šæ–­è®¡ç®—
    print("3. æµ‹è¯•ç»¼åˆè¯Šæ–­è®¡ç®—...")
    try:
        # ä½¿ç”¨PCAç»“æœæµ‹è¯•ç»¼åˆè¯Šæ–­
        test_X = np.random.rand(50, 7)  # 50ä¸ªæµ‹è¯•æ ·æœ¬
        time_index = np.arange(50)
        
        # è°ƒç”¨ç»¼åˆè¯Šæ–­å‡½æ•°
        diagnosis_result = Comprehensive_calculation(
            X=test_X,
            data_mean=data_mean,
            data_std=data_std,
            v=v,
            p_k=p_k,
            v_I=v_I,
            T_99_limit=T_99_limit,
            SPE_99_limit=SPE_99_limit,
            P=P,
            time=time_index
        )
        
        # è§£åŒ…ç»“æœ
        lamda, CONTN, t_total, q_total, S, FAI, g, h, kesi, fai, f_time, level, maxlevel, contTT, contQ, X_ratio, CContn, data_mean_out, data_std_out = diagnosis_result
        
        print(f"   âœ… ç»¼åˆè¯Šæ–­æˆåŠŸ")
        print(f"      FAIèŒƒå›´: [{fai.min():.4f}, {fai.max():.4f}]")
        print(f"      æœ€å¤§æŠ¥è­¦ç­‰çº§: {maxlevel}")
        print(f"      æ•…éšœç‚¹æ•°é‡: {np.sum(level > 0)}")
        
        return diagnosis_result
        
    except Exception as e:
        print(f"   âŒ ç»¼åˆè¯Šæ–­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def load_real_data_sample():
    """å°è¯•åŠ è½½çœŸå®æ•°æ®æ ·æœ¬"""
    
    print("\nğŸ“Š å°è¯•åŠ è½½çœŸå®æ•°æ®...")
    
    # QASæ ·æœ¬è·¯å¾„
    qas_samples = [f"../QAS/{i}" for i in range(10)]
    
    loaded_data = {}
    
    for sample_path in qas_samples:
        sample_id = os.path.basename(sample_path)
        
        try:
            # æ£€æŸ¥æ ·æœ¬æ–‡ä»¶
            if not os.path.exists(sample_path):
                continue
                
            files = [f for f in os.listdir(sample_path) if f.endswith('.pkl')]
            
            if len(files) >= 3:  # è‡³å°‘è¦æœ‰3ä¸ªpklæ–‡ä»¶
                sample_data = {}
                
                for file in files:
                    file_path = os.path.join(sample_path, file)
                    data = pd.read_pickle(file_path)
                    sample_data[file] = data
                    
                loaded_data[sample_id] = sample_data
                print(f"   âœ… åŠ è½½æ ·æœ¬ {sample_id}ï¼Œæ–‡ä»¶æ•°: {len(files)}")
                
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®ä¿¡æ¯
                if sample_id == '0':
                    print(f"      æ ·æœ¬0æ•°æ®è¯¦æƒ…:")
                    for file, data in sample_data.items():
                        print(f"        {file}: {data.shape if hasattr(data, 'shape') else len(data)}")
                
            else:
                print(f"   âš ï¸  æ ·æœ¬ {sample_id} æ–‡ä»¶ä¸å®Œæ•´ï¼Œè·³è¿‡")
                
        except Exception as e:
            print(f"   âŒ åŠ è½½æ ·æœ¬ {sample_id} å¤±è´¥: {e}")
    
    print(f"ğŸ“ˆ æˆåŠŸåŠ è½½ {len(loaded_data)} ä¸ªæ ·æœ¬")
    return loaded_data

def simple_transformer_test(loaded_data):
    """ç®€å•çš„Transformeræµ‹è¯•"""
    
    print("\nğŸ¤– ç®€å•Transformeræµ‹è¯•...")
    
    if len(loaded_data) == 0:
        print("   âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè·³è¿‡Transformeræµ‹è¯•")
        return
    
    try:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬æ„å»ºç®€å•ç‰¹å¾
        sample_0 = loaded_data.get('0')
        if not sample_0:
            print("   âŒ æ ·æœ¬0ä¸å¯ç”¨")
            return
        
        # ç®€å•åœ°åˆå¹¶æ‰€æœ‰æ•°æ®ä½œä¸ºç‰¹å¾ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼‰
        feature_data = []
        
        for file, data in sample_0.items():
            if hasattr(data, 'values'):
                feature_data.append(data.values.flatten()[:1000])  # å–å‰1000ä¸ªæ•°æ®ç‚¹
            else:
                feature_data.append(np.array(data).flatten()[:1000])
        
        # å¯¹é½æ•°æ®é•¿åº¦
        min_length = min(len(fd) for fd in feature_data)
        feature_matrix = np.column_stack([fd[:min_length] for fd in feature_data])
        
        print(f"   ğŸ“Š æ„å»ºç‰¹å¾çŸ©é˜µ: {feature_matrix.shape}")
        
        # åˆ›å»ºç®€å•çš„æ—¶åºæ•°æ®
        seq_len = 50
        if feature_matrix.shape[0] < seq_len:
            print(f"   âš ï¸  æ•°æ®é•¿åº¦ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{seq_len}ä¸ªæ—¶é—´æ­¥")
            return
        
        # åˆ›å»ºåºåˆ—
        sequences = []
        targets = []
        
        for i in range(feature_matrix.shape[0] - seq_len):
            sequences.append(feature_matrix[i:i+seq_len])
            # ç®€å•çš„ç›®æ ‡ï¼šé¢„æµ‹ä¸‹ä¸€æ­¥çš„å‰ä¸¤ä¸ªç‰¹å¾
            targets.append(feature_matrix[i+seq_len, :2])
        
        X = np.array(sequences)
        y = np.array(targets)
        
        print(f"   ğŸ”„ æ—¶åºæ•°æ®: X={X.shape}, y={y.shape}")
        
        # è½¬æ¢ä¸ºtensor
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.FloatTensor(y).to(device)
        
        # åˆ›å»ºç®€å•çš„transformeræ¨¡å‹
        from Train_Test_Integrated import TransformerPredictor
        
        model = TransformerPredictor(
            input_size=X.shape[2],
            d_model=64,  # è¾ƒå°çš„æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•
            nhead=4,
            num_layers=2,
            d_ff=128,
            dropout=0.1,
            output_size=2
        ).to(device)
        
        print(f"   ğŸ—ï¸  åˆ›å»ºTransformeræ¨¡å‹ï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç®€å•è®­ç»ƒæµ‹è¯•
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.MSELoss()
        
        model.train()
        
        # åªè®­ç»ƒå‡ ä¸ªepochè¿›è¡Œæµ‹è¯•
        for epoch in range(5):
            optimizer.zero_grad()
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªbatch
            batch_size = min(16, len(X_tensor))
            indices = torch.randperm(len(X_tensor))[:batch_size]
            
            batch_X = X_tensor[indices]
            batch_y = y_tensor[indices]
            
            outputs = model(batch_X)
            
            # åªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            loss = criterion(outputs[:, -1, :], batch_y)
            
            loss.backward()
            optimizer.step()
            
            print(f"     Epoch {epoch+1}, Loss: {loss.item():.6f}")
        
        # æµ‹è¯•é¢„æµ‹
        model.eval()
        with torch.no_grad():
            test_output = model(X_tensor[:5])  # æµ‹è¯•å‰5ä¸ªåºåˆ—
            print(f"   ğŸ¯ æµ‹è¯•é¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {test_output.shape}")
            
        print("   âœ… Transformerç®€å•æµ‹è¯•æˆåŠŸï¼")
        
        return model, X_tensor, y_tensor
        
    except Exception as e:
        print(f"   âŒ Transformeræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_test_results(results):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"test_results_{timestamp}.pkl"
    
    try:
        with open(result_file, 'wb') as f:
            pickle.dump(results, f)
        print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹å¿«é€ŸåŠŸèƒ½æµ‹è¯•...")
    print("="*60)
    
    # 1. æµ‹è¯•ç°æœ‰å‡½æ•°
    diagnosis_result = quick_test()
    
    # 2. åŠ è½½çœŸå®æ•°æ®
    loaded_data = load_real_data_sample()
    
    # 3. ç®€å•Transformeræµ‹è¯•
    transformer_result = simple_transformer_test(loaded_data)
    
    # 4. ä¿å­˜ç»“æœ
    results = {
        'diagnosis_result': diagnosis_result,
        'loaded_data_info': {k: {file: data.shape if hasattr(data, 'shape') else len(data) 
                                for file, data in v.items()} 
                           for k, v in loaded_data.items()},
        'transformer_test': transformer_result is not None
    }
    
    save_test_results(results)
    
    print("\nğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    if diagnosis_result and loaded_data and transformer_result:
        print("âœ… æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´å®éªŒ")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. è¿è¡Œ python Train_Test_Integrated.py è¿›è¡Œå®Œæ•´å®éªŒ")
        print("   2. æ£€æŸ¥ modelsfl_* ç›®å½•ä¸­çš„ç»“æœ")
        print("   3. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨")
    else:
        print("âš ï¸  éƒ¨åˆ†åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print("   - æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (../QAS/0/*.pkl)")
        print("   - ä¾èµ–æ¨¡å—æ˜¯å¦æ­£ç¡®å¯¼å…¥")
        print("   - GPU/CPUç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®")

if __name__ == "__main__":
    main()